{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import circuits\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "print (pd.__version__)\n",
    "RUN_CONFIGURATION_LOOP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_with_errs_d3(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"Z3\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "        \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "        \n",
    "\n",
    "def graph_with_errs_d5(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "             x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"Z6\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X10\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z11\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"X19\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z20\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X21\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "            \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "\n",
    "\n",
    "def graph_with_errs_d7(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 5.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 6.5))\n",
    "        if df.loc[i].at[\"Z10\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X11\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 4.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 5.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z19\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X20\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z21\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 5.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 6.5))\n",
    "        if df.loc[i].at[\"Z24\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X25\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"Z26\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X27\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"Z28\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        if df.loc[i].at[\"X29\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 4.5))\n",
    "        if df.loc[i].at[\"Z30\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 5.5))\n",
    "        if df.loc[i].at[\"Z31\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 0.5))\n",
    "        if df.loc[i].at[\"X32\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z33\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 2.5))\n",
    "        if df.loc[i].at[\"X34\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z35\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 4.5))\n",
    "        if df.loc[i].at[\"X36\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 5.5))\n",
    "        if df.loc[i].at[\"Z37\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 6.5))\n",
    "        if df.loc[i].at[\"Z38\"] == -1:\n",
    "            z_data[i].append((0, 5.5, -0.5))\n",
    "        if df.loc[i].at[\"X39\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 0.5))\n",
    "        if df.loc[i].at[\"X40\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 1.5))\n",
    "        if df.loc[i].at[\"Z41\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 1.5))\n",
    "        if df.loc[i].at[\"X42\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 2.5))\n",
    "        if df.loc[i].at[\"X43\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 3.5))\n",
    "        if df.loc[i].at[\"Z44\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 3.5))\n",
    "        if df.loc[i].at[\"X45\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 4.5))\n",
    "        if df.loc[i].at[\"X46\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 5.5))\n",
    "        if df.loc[i].at[\"Z47\"] == -1: \n",
    "            z_data[i].append((0, 5.5, 5.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "        \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWPM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_measurement_errs(curr_syn, prob_err, x_syn, depth):\n",
    "    #x_syn is True if it is x syndrome, False if it is Z syndrome\n",
    "    total_time = 0\n",
    "    new_syn = []\n",
    "    if x_syn:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_xmeasurement_errs(depth, prob_err))\n",
    "    else:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_zmeasurement_errs(depth, prob_err))\n",
    "\n",
    "\n",
    "def do_new_decoding(data, depth, prob_err):\n",
    "    decoder = circuits.GraphDecoder(depth,1)\n",
    "    G = decoder.S['Z']\n",
    "    df = pd.DataFrame()\n",
    "    syn = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for row in data:\n",
    "        x_input = []\n",
    "        z_input = []\n",
    "        x_type = True\n",
    "        for col in row:\n",
    "            if not col == \"[]\":\n",
    "                col = eval(col)\n",
    "                for c in col:\n",
    "                    if x_type:\n",
    "                        x_input.append(c)\n",
    "                    else:\n",
    "                        z_input.append(c)\n",
    "            x_type = not x_type  \n",
    "            \n",
    "        if prob_err > 0:\n",
    "            syndromes_x = add_measurement_errs(x_input, prob_err, True, depth)\n",
    "            syndromes_z = add_measurement_errs(z_input, prob_err, False, depth)\n",
    "        else:\n",
    "            syndromes_x = x_input\n",
    "            syndromes_z = z_input\n",
    "\n",
    "        start = time.time_ns()\n",
    "        error_graph_x, paths_x = decoder.make_error_graph(syndromes_x,'X')\n",
    "        matching_graph_x = decoder.matching_graph(error_graph_x,'X')\n",
    "        matches_x = decoder.matching(matching_graph_x,'X')\n",
    "        flips_x = decoder.calculate_qubit_flips(matches_x, paths_x,'X')\n",
    "        syn_x = (translate_errors(flips_x))\n",
    "\n",
    "        error_graph_z, paths_z = decoder.make_error_graph(syndromes_z,'Z')\n",
    "        matching_graph_z = decoder.matching_graph(error_graph_z,'Z')\n",
    "        matches_z = decoder.matching(matching_graph_z,'Z')\n",
    "        flips_z = decoder.calculate_qubit_flips(matches_z, paths_z,'Z')\n",
    "        syn_z = translate_errors(flips_z)\n",
    "        df = df.append(pd.Series([syn_x, syn_z]), ignore_index=True)\n",
    "        end = time.time_ns()\n",
    "        total_time += (end - start)/ (10 ** 9)\n",
    "    return (df, total_time) \n",
    "\n",
    "\n",
    "def return_xmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, 1.5, 0.5), (0, 2.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, 1.5, 0.5), (0, 1.5, 2.5),\n",
    "                        (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 3.5, 0.5), (0, 4.5, 1.5), (0, 3.5, 2.5), (0, 4.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, -0.5, 4.5), (0, 0.5, 5.5),\n",
    "                        (0, 1.5, 0.5), (0, 1.5, 2.5), (0, 1.5, 4.5), (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 2.5, 5.5),\n",
    "                        (0, 3.5, 0.5),  (0, 3.5, 2.5), (0, 3.5, 4.5), (0, 4.5, 1.5), (0, 4.5, 3.5), (0, 4.5, 5.5),\n",
    "                       (0, 5.5, 0.5), (0, 6.5, 1.5), (0, 5.5, 2.5), (0, 6.5, 3.5), (0, 5.5, 4.5), (0, 6.5, 5.5)]\n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "            \n",
    "\n",
    "def return_zmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 1.5, -0.5), (0, 1.5, 1.5), (0, 1.5, 3.5),\n",
    "                        (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 3.5, -0.5), (0, 3.5, 1.5), (0, 3.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 0.5, 6.5), (0, 1.5, -0.5), (0, 1.5, 1.5),\n",
    "                        (0, 1.5, 3.5), (0, 1.5, 5.5), (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 2.5, 6.5),\n",
    "                        (0, 3.5, -0.5),  (0, 3.5, 1.5), (0, 3.5, 3.5), (0, 3.5, 5.5), (0, 4.5, 0.5), (0, 4.5, 2.5),\n",
    "                       (0, 4.5, 4.5), (0, 4.5, 6.5), (0, 5.5, -0.5), (0, 5.5, 1.5), (0, 5.5, 3.5), (0, 5.5, 5.5)]\n",
    "        \n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "\n",
    "\n",
    "def translate_errors (phys_errs):\n",
    "    flipX = np.array([(0, 1),(1, 0)])\n",
    "    flipZ = np.array([(1, 0), (0, -1)])\n",
    "    errs = []\n",
    "    str2 = \"\"\n",
    "    for qubit, flip in phys_errs.items():\n",
    "        row = int(qubit[1])\n",
    "        col = int(qubit[2])\n",
    "        if str(flip) == \"X\":\n",
    "            str1 = \"X\"\n",
    "        elif str(flip) == \"Z\":\n",
    "            str1 = \"Z\"\n",
    "        else:\n",
    "            str1 = \"X\"\n",
    "            str2 = \"Z\"\n",
    "        str1 += str(row) + str(col)\n",
    "        errs.append(str1)\n",
    "        if str2 != \"\":\n",
    "            str2 += str(row) +str(col)\n",
    "            errs.append(str2)\n",
    "            str2 = \"\"\n",
    "    return errs   \n",
    "\n",
    "\n",
    "def translate_to_graph(df_graph, labels, mlb):\n",
    "#go through labels given\n",
    "    indices = []\n",
    "    labels = mlb.inverse_transform(labels)\n",
    "    \n",
    "    for row in labels:\n",
    "        label_str = str(row)\n",
    "        for index, r in df_graph.iterrows():\n",
    "            if label_str == \"('',)\":\n",
    "                if str(r[\"Labels\"]) == \"[' ']\":\n",
    "                    indices.append([index])\n",
    "                    break\n",
    "            if set(row) == set(r[\"Labels\"]):\n",
    "                indices.append([index])\n",
    "                break\n",
    "\n",
    "    df_syns = df_graph.drop(['Labels'], axis=1)\n",
    "    return_df = pd.DataFrame()\n",
    "    for i in indices:\n",
    "        return_df = return_df.append(df_syns.loc[i])\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_string(err_list):\n",
    "    newstring = err_list.replace(\"'\", \"\")\n",
    "    new_err_list = newstring.strip('][').split(', ')\n",
    "    return sorted(set(new_err_list))\n",
    "\n",
    "\n",
    "def create_string_from_list(err_list):\n",
    "    return_string = \"[\"\n",
    "    if err_list[0] == \"''\":\n",
    "        return \"[' ']\"\n",
    "    else:\n",
    "        for index, i in enumerate(err_list):\n",
    "            return_string = return_string +  \"'\" + i + \"'\"\n",
    "            if index < (len(err_list)-1):\n",
    "                return_string += \", \"\n",
    "    return return_string + \"]\"\n",
    "        \n",
    "\n",
    "#take in two 2d arrays of predicted values, true values, and threshold to determine labels. \n",
    "#calculates the partial accuracy of the predicted values, averaged out for all obersvations\n",
    "def partial_accuracy(y_pred, y_true):\n",
    "    total = 0\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j in range(0, cols):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "        total += row_correct/cols\n",
    "    return total/rows\n",
    "\n",
    "\n",
    "def partial_accuracy_and_contingency(y_pred, y_true, mlb):\n",
    "    total = 0\n",
    "    a= np.zeros(shape=y_true.shape)\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    df = pd.DataFrame(a, columns = mlb.classes_)\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j, label in enumerate(mlb.classes_):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "                df.at[i, label] = 1\n",
    "            else:\n",
    "                df.at[i, label] = 0\n",
    "\n",
    "        total += row_correct/cols\n",
    "\n",
    "    return (total/rows, df)\n",
    "\n",
    "\n",
    "def contingency_table_and_t (clf1, clf2):\n",
    "    a = 0 #clf1 pos, clf2 pos\n",
    "    b = 0 #clf1 pos, clf2 neg\n",
    "    c = 0 #clf1 neg, clf2 pos\n",
    "    d = 0 #clf1 neg, clf2 neg\n",
    "    \n",
    "    for index, value in clf1.items():\n",
    "        if value == 1 and clf2[index] == 1:\n",
    "            a+=1\n",
    "        elif value == 1 and clf2[index] == 0: #classifier 1 right, classifier 2 wrong\n",
    "            b+=1\n",
    "        elif value == 0 and clf2[index] == 1: #classifier 1 wrong, classifier 2 right\n",
    "            c+=1\n",
    "        else:\n",
    "            d+=1\n",
    "    print(\"[\"+str(a)+\", \"+str(b)+\"]\")\n",
    "    print(\"[\"+str(c)+\", \"+str(d)+\"]\")\n",
    "    if b == 0 and c ==0:\n",
    "        print(\"both b and c are zero\")\n",
    "        t=0\n",
    "    else:\n",
    "        t = (((b-c)-1)**2)/(b+c)\n",
    "    return ([[a,b],[c,d]], t)\n",
    "\n",
    "\n",
    "def add_noise(val, noise_level):\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand <= noise_level:\n",
    "        if val == -1:\n",
    "            val = 1\n",
    "        elif val == 1:\n",
    "            val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup table functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAllBinaryStrings(n, arr, i, lookup):  \n",
    "    if i == n: \n",
    "        lookup.setBitStringArray(arr, n)  \n",
    "        return\n",
    "      \n",
    "    # First assign \"0\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 0\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)  \n",
    "  \n",
    "    # And then assign \"1\" at ith position  , \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 1\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)\n",
    "\n",
    "\n",
    "class lookup_decoder:\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        #self.lookupTable = defaultdict()\n",
    "        self.lookupTable = {}\n",
    "        self.distributions = {}\n",
    "        self.depth = depth\n",
    "        #generating all possible syndrome observations\n",
    "        #arr = [None] * (depth**2 - 1)\n",
    "        #generateAllBinaryStrings((depth**2 - 1), arr, 0, self)\n",
    "        \n",
    "    def setBitStringArray(self, arr, n): \n",
    "        new_str = \"\"\n",
    "        for i in range(0, n):  \n",
    "            new_str += str(arr[i])\n",
    "        self.lookupTable.update({new_str:defaultdict()})  \n",
    "        \n",
    "    def update_table (self, syndrome, phys_errs):\n",
    "        #all the keys are made in the init, so simply update the physical error combinations for the given syndrome\n",
    "        if syndrome not in self.lookupTable:\n",
    "            self.lookupTable[syndrome] = {}\n",
    "            self.lookupTable[syndrome][phys_errs] = 1\n",
    "            return\n",
    "        \n",
    "        if phys_errs not in self.lookupTable[syndrome]:\n",
    "            self.lookupTable[syndrome].update({phys_errs: 1})\n",
    "        else:\n",
    "            self.lookupTable[syndrome][phys_errs] += 1\n",
    "     \n",
    "    def get_probable_error(self, syndrome):\n",
    "        return_key = []\n",
    "\n",
    "        if syndrome not in self.lookupTable.keys():\n",
    "            for i in range(2* int(self.depth**2) + 1):\n",
    "                return_key.append(0)\n",
    "            return return_key\n",
    "        \n",
    "        key, value = max(self.lookupTable[syndrome].items(), key=lambda x:x[1])\n",
    "        \n",
    "        for character in key:\n",
    "            if character == '0' or character == '1':\n",
    "                return_key.append(int(character))\n",
    "                \n",
    "        return return_key\n",
    "        \n",
    "    def make_distribution_graph(self, syn):\n",
    "        \n",
    "        plt.bar(list(self.lookupTable[syn].keys()), self.lookupTable[syn].values(), color='g')\n",
    "        plt.show()\n",
    "        \n",
    "    def syndrome_count_graph(self):\n",
    "        graph_dict = {}\n",
    "        for syn in self.lookupTable:\n",
    "            graph_dict.update({syn:sum(self.lookupTable[syn].values())})\n",
    "        plt.ylim((0,4))\n",
    "        plt.bar(graph_dict.keys(), graph_dict.values())\n",
    "        plt.show()\n",
    "    \n",
    "    def length_of_lookup(self):\n",
    "        print(len(self.lookupTable))\n",
    "            \n",
    "    def get_entropies(self):\n",
    "        entropies = {}\n",
    "        for syn in self.lookupTable:\n",
    "            total = sum(self.lookupTable[syn].values())\n",
    "            h = 0\n",
    "            for key in self.lookupTable[syn]:\n",
    "                p = self.lookupTable[syn][key]/total\n",
    "                h += p+log2(p)\n",
    "            entropies[syn] = -h\n",
    "        print(entropies)\n",
    "            \n",
    "    def get_syndromes(self):\n",
    "        return self.lookupTable.keys()\n",
    "                     \n",
    "    def print_lookup(self):\n",
    "        for syn in self.lookupTable:\n",
    "            print(self.lookupTable[syn])\n",
    "            \n",
    "\n",
    "def train_plut(table, data_x, data_y):\n",
    "    i = 0\n",
    "    for index, x in enumerate(data_x):\n",
    "        syn = \"\".join([str(i) for i in x])\n",
    "        syn = syn.replace(\".0\",\"\")\n",
    "        labels = np.array2string(np.array(data_y[i]), precision=1, separator='',suppress_small=True).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        table.update_table(syn, labels)\n",
    "        i+=1\n",
    "    return table\n",
    "\n",
    "\n",
    "def test_plut(table, test_set):\n",
    "    predictions_lookup = []\n",
    "    for index, x in enumerate(test_set):\n",
    "        syn_x = \"\".join([str(i) for i in x])\n",
    "        syn_x = syn_x.replace(\".0\",\"\")\n",
    "        predictions_lookup.append(table.get_probable_error(syn_x))\n",
    "    return np.array(predictions_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_FFNN_model_DepthThree(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    \n",
    "    for i in range(layers):\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(18 , activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_FFNN_cv_model_DepthThree(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    \n",
    "    for i in range(layers):\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(19 , activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_FFNN_cv_model_DepthFive(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    for i in range(layers):\n",
    "        model.add(Dense(240, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(51, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_FFNN_cv_model_DepthSeven(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    \n",
    "    for i in range(layers):\n",
    "        model.add(Dense(400, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(99, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D3 Training, Testing, and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data sets from CSV\n",
    "\n",
    "#### Data sets for D3:\n",
    "* Original:\n",
    "    - \"depth3_all_combos.csv\"\n",
    "* Exhaustive:\n",
    "    - \"ex-samples-d3.csv\"\n",
    "* Randomly-sampled, with replacement (version 2):\n",
    "    - \"v2samples-d3-1000.csv\"\n",
    "    - \"v2samples-d3-10000.csv\"\n",
    "    - \"v2samples-d3-100000.csv\"\n",
    "* Randomly-sampled, without replacement (version 3):\n",
    "    - \"v3samples-d3-1000.csv\"\n",
    "    - \"v3samples-d3-10000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and formatted in 3.488793134689331 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainData_d3 = pd.read_csv(\"SAMPLES/v2samples-d3-1000.csv\")\n",
    "\n",
    "trainData_d3[\"Labels\"] = trainData_d3['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d3 = trainData_d3.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "testData_d3_MWPM = graph_with_errs_d3(trainData_d3)\n",
    "\n",
    "mlb_d3 = MultiLabelBinarizer()\n",
    "mlb_d3.fit(trainData_d3[\"Labels\"])\n",
    "df = pd.DataFrame(mlb_d3.transform(trainData_d3['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d3 = trainData_d3.drop(['Labels'], axis=1)\n",
    "trainData_d3 = pd.concat([df['Labels'], testData_d3_MWPM, trainData_d3], axis=1, ignore_index=True)\n",
    "trainData_d3.columns = [\"Labels\",\"XSyn\", \"ZSyn\", \"X0\", \"Z1\", \"X2\", \"Z3\", \"Z4\", \"X5\", \"X6\", \"Z7\"]\n",
    "\n",
    "y_d3 = trainData_d3[\"Labels\"]\n",
    "x_d3 = trainData_d3.drop([\"Labels\"], axis=1)\n",
    "\n",
    "x_d3 = x_d3.replace([-1], 0)\n",
    "print(\"Data imported and formatted in %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 5625 samples, validate on 1875 samples\n",
      "Epoch 1/200\n",
      "5625/5625 [==============================] - 0s 59us/step - loss: 0.6442 - accuracy: 0.7006 - val_loss: 0.5934 - val_accuracy: 0.7446\n",
      "Epoch 2/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5770 - accuracy: 0.7443 - val_loss: 0.5717 - val_accuracy: 0.7447\n",
      "Epoch 3/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5706 - accuracy: 0.7443 - val_loss: 0.5702 - val_accuracy: 0.7447\n",
      "Epoch 4/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5694 - accuracy: 0.7443 - val_loss: 0.5692 - val_accuracy: 0.7447\n",
      "Epoch 5/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5684 - accuracy: 0.7443 - val_loss: 0.5682 - val_accuracy: 0.7447\n",
      "Epoch 6/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5674 - accuracy: 0.7443 - val_loss: 0.5672 - val_accuracy: 0.7447\n",
      "Epoch 7/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5664 - accuracy: 0.7443 - val_loss: 0.5662 - val_accuracy: 0.7447\n",
      "Epoch 8/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5652 - accuracy: 0.7443 - val_loss: 0.5649 - val_accuracy: 0.7447\n",
      "Epoch 9/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5640 - accuracy: 0.7443 - val_loss: 0.5637 - val_accuracy: 0.7447\n",
      "Epoch 10/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5627 - accuracy: 0.7443 - val_loss: 0.5622 - val_accuracy: 0.7448\n",
      "Epoch 11/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5613 - accuracy: 0.7442 - val_loss: 0.5612 - val_accuracy: 0.7447\n",
      "Epoch 12/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5601 - accuracy: 0.7444 - val_loss: 0.5600 - val_accuracy: 0.7447\n",
      "Epoch 13/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5590 - accuracy: 0.7442 - val_loss: 0.5590 - val_accuracy: 0.7447\n",
      "Epoch 14/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5580 - accuracy: 0.7444 - val_loss: 0.5583 - val_accuracy: 0.7449\n",
      "Epoch 15/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5571 - accuracy: 0.7442 - val_loss: 0.5574 - val_accuracy: 0.7446\n",
      "Epoch 16/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5562 - accuracy: 0.7442 - val_loss: 0.5566 - val_accuracy: 0.7444\n",
      "Epoch 17/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5555 - accuracy: 0.7442 - val_loss: 0.5559 - val_accuracy: 0.7445\n",
      "Epoch 18/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5547 - accuracy: 0.7444 - val_loss: 0.5552 - val_accuracy: 0.7445\n",
      "Epoch 19/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5539 - accuracy: 0.7441 - val_loss: 0.5543 - val_accuracy: 0.7447\n",
      "Epoch 20/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5529 - accuracy: 0.7441 - val_loss: 0.5535 - val_accuracy: 0.7447\n",
      "Epoch 21/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5520 - accuracy: 0.7441 - val_loss: 0.5524 - val_accuracy: 0.7444\n",
      "Epoch 22/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5508 - accuracy: 0.7442 - val_loss: 0.5511 - val_accuracy: 0.7447\n",
      "Epoch 23/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5494 - accuracy: 0.7441 - val_loss: 0.5498 - val_accuracy: 0.7444\n",
      "Epoch 24/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5479 - accuracy: 0.7441 - val_loss: 0.5485 - val_accuracy: 0.7447\n",
      "Epoch 25/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5463 - accuracy: 0.7440 - val_loss: 0.5471 - val_accuracy: 0.7444\n",
      "Epoch 26/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5448 - accuracy: 0.7441 - val_loss: 0.5457 - val_accuracy: 0.7448\n",
      "Epoch 27/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5437 - accuracy: 0.7442 - val_loss: 0.5444 - val_accuracy: 0.7446\n",
      "Epoch 28/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5424 - accuracy: 0.7442 - val_loss: 0.5433 - val_accuracy: 0.7445\n",
      "Epoch 29/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5414 - accuracy: 0.7442 - val_loss: 0.5423 - val_accuracy: 0.7446\n",
      "Epoch 30/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5404 - accuracy: 0.7439 - val_loss: 0.5416 - val_accuracy: 0.7447\n",
      "Epoch 31/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5395 - accuracy: 0.7441 - val_loss: 0.5405 - val_accuracy: 0.7442\n",
      "Epoch 32/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5387 - accuracy: 0.7442 - val_loss: 0.5398 - val_accuracy: 0.7445\n",
      "Epoch 33/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5379 - accuracy: 0.7440 - val_loss: 0.5387 - val_accuracy: 0.7447\n",
      "Epoch 34/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5372 - accuracy: 0.7441 - val_loss: 0.5381 - val_accuracy: 0.7446\n",
      "Epoch 35/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5364 - accuracy: 0.7446 - val_loss: 0.5377 - val_accuracy: 0.7440\n",
      "Epoch 36/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5359 - accuracy: 0.7441 - val_loss: 0.5367 - val_accuracy: 0.7444\n",
      "Epoch 37/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5352 - accuracy: 0.7443 - val_loss: 0.5363 - val_accuracy: 0.7445\n",
      "Epoch 38/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5347 - accuracy: 0.7443 - val_loss: 0.5356 - val_accuracy: 0.7443\n",
      "Epoch 39/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5342 - accuracy: 0.7443 - val_loss: 0.5352 - val_accuracy: 0.7439\n",
      "Epoch 40/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5338 - accuracy: 0.7441 - val_loss: 0.5348 - val_accuracy: 0.7437\n",
      "Epoch 41/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5334 - accuracy: 0.7441 - val_loss: 0.5343 - val_accuracy: 0.7444\n",
      "Epoch 42/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5331 - accuracy: 0.7443 - val_loss: 0.5337 - val_accuracy: 0.7440\n",
      "Epoch 43/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5328 - accuracy: 0.7443 - val_loss: 0.5334 - val_accuracy: 0.7442\n",
      "Epoch 44/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5325 - accuracy: 0.7440 - val_loss: 0.5331 - val_accuracy: 0.7437\n",
      "Epoch 45/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5323 - accuracy: 0.7443 - val_loss: 0.5327 - val_accuracy: 0.7441\n",
      "Epoch 46/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5320 - accuracy: 0.7442 - val_loss: 0.5324 - val_accuracy: 0.7437\n",
      "Epoch 47/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5318 - accuracy: 0.7443 - val_loss: 0.5325 - val_accuracy: 0.7437\n",
      "Epoch 48/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5316 - accuracy: 0.7447 - val_loss: 0.5321 - val_accuracy: 0.7438\n",
      "Epoch 49/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5314 - accuracy: 0.7444 - val_loss: 0.5324 - val_accuracy: 0.7430\n",
      "Epoch 50/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5313 - accuracy: 0.7438 - val_loss: 0.5317 - val_accuracy: 0.7438\n",
      "Epoch 51/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5311 - accuracy: 0.7440 - val_loss: 0.5317 - val_accuracy: 0.7431\n",
      "Epoch 52/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5309 - accuracy: 0.7443 - val_loss: 0.5313 - val_accuracy: 0.7431\n",
      "Epoch 53/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5307 - accuracy: 0.7445 - val_loss: 0.5314 - val_accuracy: 0.7433\n",
      "Epoch 54/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5306 - accuracy: 0.7445 - val_loss: 0.5313 - val_accuracy: 0.7427\n",
      "Epoch 55/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5306 - accuracy: 0.7439 - val_loss: 0.5309 - val_accuracy: 0.7441\n",
      "Epoch 56/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5304 - accuracy: 0.7445 - val_loss: 0.5308 - val_accuracy: 0.7447\n",
      "Epoch 57/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5303 - accuracy: 0.7445 - val_loss: 0.5306 - val_accuracy: 0.7432\n",
      "Epoch 58/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5301 - accuracy: 0.7442 - val_loss: 0.5304 - val_accuracy: 0.7432\n",
      "Epoch 59/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5300 - accuracy: 0.7439 - val_loss: 0.5303 - val_accuracy: 0.7433\n",
      "Epoch 60/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5300 - accuracy: 0.7443 - val_loss: 0.5301 - val_accuracy: 0.7441\n",
      "Epoch 61/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5297 - accuracy: 0.7445 - val_loss: 0.5300 - val_accuracy: 0.7434\n",
      "Epoch 62/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5296 - accuracy: 0.7444 - val_loss: 0.5295 - val_accuracy: 0.7432\n",
      "Epoch 63/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5294 - accuracy: 0.7439 - val_loss: 0.5297 - val_accuracy: 0.7441\n",
      "Epoch 64/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5294 - accuracy: 0.7443 - val_loss: 0.5297 - val_accuracy: 0.7440\n",
      "Epoch 65/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5293 - accuracy: 0.7445 - val_loss: 0.5292 - val_accuracy: 0.7429\n",
      "Epoch 66/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5291 - accuracy: 0.7441 - val_loss: 0.5294 - val_accuracy: 0.7440\n",
      "Epoch 67/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5291 - accuracy: 0.7438 - val_loss: 0.5289 - val_accuracy: 0.7437\n",
      "Epoch 68/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5288 - accuracy: 0.7445 - val_loss: 0.5287 - val_accuracy: 0.7438\n",
      "Epoch 69/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5288 - accuracy: 0.7438 - val_loss: 0.5285 - val_accuracy: 0.7435\n",
      "Epoch 70/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5286 - accuracy: 0.7441 - val_loss: 0.5283 - val_accuracy: 0.7435\n",
      "Epoch 71/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5285 - accuracy: 0.7440 - val_loss: 0.5281 - val_accuracy: 0.7433\n",
      "Epoch 72/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5283 - accuracy: 0.7442 - val_loss: 0.5283 - val_accuracy: 0.7439\n",
      "Epoch 73/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5282 - accuracy: 0.7445 - val_loss: 0.5278 - val_accuracy: 0.7436\n",
      "Epoch 74/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5281 - accuracy: 0.7443 - val_loss: 0.5277 - val_accuracy: 0.7431\n",
      "Epoch 75/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5279 - accuracy: 0.7448 - val_loss: 0.5278 - val_accuracy: 0.7434\n",
      "Epoch 76/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5278 - accuracy: 0.7444 - val_loss: 0.5273 - val_accuracy: 0.7431\n",
      "Epoch 77/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5275 - accuracy: 0.7441 - val_loss: 0.5271 - val_accuracy: 0.7444\n",
      "Epoch 78/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5275 - accuracy: 0.7445 - val_loss: 0.5272 - val_accuracy: 0.7433\n",
      "Epoch 79/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5274 - accuracy: 0.7445 - val_loss: 0.5268 - val_accuracy: 0.7440\n",
      "Epoch 80/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5271 - accuracy: 0.7441 - val_loss: 0.5269 - val_accuracy: 0.7439\n",
      "Epoch 81/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5269 - accuracy: 0.7441 - val_loss: 0.5267 - val_accuracy: 0.7445\n",
      "Epoch 82/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5268 - accuracy: 0.7443 - val_loss: 0.5265 - val_accuracy: 0.7434\n",
      "Epoch 83/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5266 - accuracy: 0.7441 - val_loss: 0.5265 - val_accuracy: 0.7436\n",
      "Epoch 84/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5265 - accuracy: 0.7447 - val_loss: 0.5260 - val_accuracy: 0.7446\n",
      "Epoch 85/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5263 - accuracy: 0.7443 - val_loss: 0.5258 - val_accuracy: 0.7438\n",
      "Epoch 86/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5261 - accuracy: 0.7443 - val_loss: 0.5257 - val_accuracy: 0.7449\n",
      "Epoch 87/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5259 - accuracy: 0.7442 - val_loss: 0.5252 - val_accuracy: 0.7442\n",
      "Epoch 88/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5256 - accuracy: 0.7442 - val_loss: 0.5253 - val_accuracy: 0.7439\n",
      "Epoch 89/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5254 - accuracy: 0.7446 - val_loss: 0.5250 - val_accuracy: 0.7442\n",
      "Epoch 90/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5252 - accuracy: 0.7441 - val_loss: 0.5247 - val_accuracy: 0.7438\n",
      "Epoch 91/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5249 - accuracy: 0.7442 - val_loss: 0.5245 - val_accuracy: 0.7444\n",
      "Epoch 92/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5248 - accuracy: 0.7442 - val_loss: 0.5242 - val_accuracy: 0.7438\n",
      "Epoch 93/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5245 - accuracy: 0.7442 - val_loss: 0.5241 - val_accuracy: 0.7435\n",
      "Epoch 94/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5243 - accuracy: 0.7444 - val_loss: 0.5240 - val_accuracy: 0.7435\n",
      "Epoch 95/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5240 - accuracy: 0.7442 - val_loss: 0.5235 - val_accuracy: 0.7446\n",
      "Epoch 96/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5238 - accuracy: 0.7432 - val_loss: 0.5233 - val_accuracy: 0.7434\n",
      "Epoch 97/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5236 - accuracy: 0.7448 - val_loss: 0.5235 - val_accuracy: 0.7433\n",
      "Epoch 98/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5235 - accuracy: 0.7445 - val_loss: 0.5237 - val_accuracy: 0.7444\n",
      "Epoch 99/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5233 - accuracy: 0.7439 - val_loss: 0.5230 - val_accuracy: 0.7439\n",
      "Epoch 100/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5232 - accuracy: 0.7443 - val_loss: 0.5228 - val_accuracy: 0.7447\n",
      "Epoch 101/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5228 - accuracy: 0.7445 - val_loss: 0.5227 - val_accuracy: 0.7436\n",
      "Epoch 102/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5227 - accuracy: 0.7446 - val_loss: 0.5223 - val_accuracy: 0.7450\n",
      "Epoch 103/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5225 - accuracy: 0.7448 - val_loss: 0.5222 - val_accuracy: 0.7449\n",
      "Epoch 104/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5224 - accuracy: 0.7444 - val_loss: 0.5224 - val_accuracy: 0.7440\n",
      "Epoch 105/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5222 - accuracy: 0.7445 - val_loss: 0.5219 - val_accuracy: 0.7447\n",
      "Epoch 106/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5221 - accuracy: 0.7442 - val_loss: 0.5218 - val_accuracy: 0.7457\n",
      "Epoch 107/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5219 - accuracy: 0.7440 - val_loss: 0.5217 - val_accuracy: 0.7452\n",
      "Epoch 108/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5217 - accuracy: 0.7444 - val_loss: 0.5214 - val_accuracy: 0.7444\n",
      "Epoch 109/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5216 - accuracy: 0.7450 - val_loss: 0.5224 - val_accuracy: 0.7436\n",
      "Epoch 110/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5216 - accuracy: 0.7444 - val_loss: 0.5214 - val_accuracy: 0.7455\n",
      "Epoch 111/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5215 - accuracy: 0.7442 - val_loss: 0.5216 - val_accuracy: 0.7442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5213 - accuracy: 0.7449 - val_loss: 0.5213 - val_accuracy: 0.7448\n",
      "Epoch 113/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5213 - accuracy: 0.7451 - val_loss: 0.5212 - val_accuracy: 0.7444\n",
      "Epoch 114/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5211 - accuracy: 0.7451 - val_loss: 0.5213 - val_accuracy: 0.7450\n",
      "Epoch 115/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5211 - accuracy: 0.7449 - val_loss: 0.5210 - val_accuracy: 0.7443\n",
      "Epoch 116/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5210 - accuracy: 0.7449 - val_loss: 0.5218 - val_accuracy: 0.7437\n",
      "Epoch 117/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5209 - accuracy: 0.7449 - val_loss: 0.5213 - val_accuracy: 0.7445\n",
      "Epoch 118/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5209 - accuracy: 0.7451 - val_loss: 0.5213 - val_accuracy: 0.7441\n",
      "Epoch 119/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5208 - accuracy: 0.7451 - val_loss: 0.5208 - val_accuracy: 0.7448\n",
      "Epoch 120/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5208 - accuracy: 0.7449 - val_loss: 0.5209 - val_accuracy: 0.7445\n",
      "Epoch 121/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5207 - accuracy: 0.7449 - val_loss: 0.5207 - val_accuracy: 0.7441\n",
      "Epoch 122/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5207 - accuracy: 0.7449 - val_loss: 0.5211 - val_accuracy: 0.7440\n",
      "Epoch 123/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5205 - accuracy: 0.7454 - val_loss: 0.5214 - val_accuracy: 0.7439\n",
      "Epoch 124/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5207 - accuracy: 0.7455 - val_loss: 0.5208 - val_accuracy: 0.7441\n",
      "Epoch 125/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5206 - accuracy: 0.7454 - val_loss: 0.5205 - val_accuracy: 0.7444\n",
      "Epoch 126/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5206 - accuracy: 0.7452 - val_loss: 0.5209 - val_accuracy: 0.7438\n",
      "Epoch 127/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5205 - accuracy: 0.7453 - val_loss: 0.5204 - val_accuracy: 0.7450\n",
      "Epoch 128/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5205 - accuracy: 0.7458 - val_loss: 0.5216 - val_accuracy: 0.7430\n",
      "Epoch 129/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5204 - accuracy: 0.7459 - val_loss: 0.5206 - val_accuracy: 0.7446\n",
      "Epoch 130/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5204 - accuracy: 0.7449 - val_loss: 0.5203 - val_accuracy: 0.7447\n",
      "Epoch 131/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5204 - accuracy: 0.7452 - val_loss: 0.5209 - val_accuracy: 0.7446\n",
      "Epoch 132/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5204 - accuracy: 0.7456 - val_loss: 0.5215 - val_accuracy: 0.7436\n",
      "Epoch 133/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5203 - accuracy: 0.7456 - val_loss: 0.5205 - val_accuracy: 0.7436\n",
      "Epoch 134/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5203 - accuracy: 0.7454 - val_loss: 0.5204 - val_accuracy: 0.7444\n",
      "Epoch 135/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5203 - accuracy: 0.7447 - val_loss: 0.5203 - val_accuracy: 0.7447\n",
      "Epoch 136/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5202 - accuracy: 0.7454 - val_loss: 0.5201 - val_accuracy: 0.7456\n",
      "Epoch 137/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5202 - accuracy: 0.7453 - val_loss: 0.5202 - val_accuracy: 0.7453\n",
      "Epoch 138/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5202 - accuracy: 0.7456 - val_loss: 0.5205 - val_accuracy: 0.7447\n",
      "Epoch 139/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5202 - accuracy: 0.7451 - val_loss: 0.5203 - val_accuracy: 0.7457\n",
      "Epoch 140/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5202 - accuracy: 0.7458 - val_loss: 0.5205 - val_accuracy: 0.7437\n",
      "Epoch 141/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5201 - accuracy: 0.7450 - val_loss: 0.5201 - val_accuracy: 0.7461\n",
      "Epoch 142/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5202 - accuracy: 0.7453 - val_loss: 0.5204 - val_accuracy: 0.7438\n",
      "Epoch 143/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5201 - accuracy: 0.7455 - val_loss: 0.5205 - val_accuracy: 0.7444\n",
      "Epoch 144/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5201 - accuracy: 0.7454 - val_loss: 0.5201 - val_accuracy: 0.7441\n",
      "Epoch 145/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5201 - accuracy: 0.7456 - val_loss: 0.5202 - val_accuracy: 0.7453\n",
      "Epoch 146/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5200 - accuracy: 0.7454 - val_loss: 0.5204 - val_accuracy: 0.7447\n",
      "Epoch 147/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5201 - accuracy: 0.7451 - val_loss: 0.5202 - val_accuracy: 0.7453\n",
      "Epoch 148/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5199 - accuracy: 0.7454 - val_loss: 0.5209 - val_accuracy: 0.7439\n",
      "Epoch 149/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5200 - accuracy: 0.7451 - val_loss: 0.5202 - val_accuracy: 0.7442\n",
      "Epoch 150/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5199 - accuracy: 0.7456 - val_loss: 0.5203 - val_accuracy: 0.7438\n",
      "Epoch 151/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5199 - accuracy: 0.7457 - val_loss: 0.5203 - val_accuracy: 0.7455\n",
      "Epoch 152/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5199 - accuracy: 0.7461 - val_loss: 0.5204 - val_accuracy: 0.7439\n",
      "Epoch 153/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5199 - accuracy: 0.7460 - val_loss: 0.5202 - val_accuracy: 0.7452\n",
      "Epoch 154/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5199 - accuracy: 0.7462 - val_loss: 0.5199 - val_accuracy: 0.7458\n",
      "Epoch 155/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5199 - accuracy: 0.7458 - val_loss: 0.5201 - val_accuracy: 0.7446\n",
      "Epoch 156/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5199 - accuracy: 0.7455 - val_loss: 0.5207 - val_accuracy: 0.7449\n",
      "Epoch 157/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5199 - accuracy: 0.7460 - val_loss: 0.5201 - val_accuracy: 0.7448\n",
      "Epoch 158/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5199 - accuracy: 0.7454 - val_loss: 0.5200 - val_accuracy: 0.7449\n",
      "Epoch 159/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5198 - accuracy: 0.7457 - val_loss: 0.5199 - val_accuracy: 0.7443\n",
      "Epoch 160/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5198 - accuracy: 0.7453 - val_loss: 0.5206 - val_accuracy: 0.7453\n",
      "Epoch 161/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5198 - accuracy: 0.7470 - val_loss: 0.5204 - val_accuracy: 0.7449\n",
      "Epoch 162/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5198 - accuracy: 0.7459 - val_loss: 0.5200 - val_accuracy: 0.7447\n",
      "Epoch 163/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5198 - accuracy: 0.7456 - val_loss: 0.5201 - val_accuracy: 0.7442\n",
      "Epoch 164/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5198 - accuracy: 0.7461 - val_loss: 0.5201 - val_accuracy: 0.7438\n",
      "Epoch 165/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5198 - accuracy: 0.7458 - val_loss: 0.5201 - val_accuracy: 0.7446\n",
      "Epoch 166/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5197 - accuracy: 0.7453 - val_loss: 0.5199 - val_accuracy: 0.7450\n",
      "Epoch 167/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5196 - accuracy: 0.7461 - val_loss: 0.5206 - val_accuracy: 0.7445\n",
      "Epoch 168/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5197 - accuracy: 0.7461 - val_loss: 0.5202 - val_accuracy: 0.7446\n",
      "Epoch 169/200\n",
      "5625/5625 [==============================] - 0s 33us/step - loss: 0.5197 - accuracy: 0.7456 - val_loss: 0.5200 - val_accuracy: 0.7453\n",
      "Epoch 170/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5198 - accuracy: 0.7453 - val_loss: 0.5198 - val_accuracy: 0.7449\n",
      "Epoch 171/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5196 - accuracy: 0.7458 - val_loss: 0.5200 - val_accuracy: 0.7441\n",
      "Epoch 172/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5197 - accuracy: 0.7459 - val_loss: 0.5200 - val_accuracy: 0.7442\n",
      "Epoch 173/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5196 - accuracy: 0.7462 - val_loss: 0.5204 - val_accuracy: 0.7434\n",
      "Epoch 174/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5196 - accuracy: 0.7458 - val_loss: 0.5199 - val_accuracy: 0.7447\n",
      "Epoch 175/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5196 - accuracy: 0.7466 - val_loss: 0.5200 - val_accuracy: 0.7450\n",
      "Epoch 176/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5197 - accuracy: 0.7459 - val_loss: 0.5200 - val_accuracy: 0.7455\n",
      "Epoch 177/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5196 - accuracy: 0.7465 - val_loss: 0.5200 - val_accuracy: 0.7450\n",
      "Epoch 178/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5196 - accuracy: 0.7465 - val_loss: 0.5205 - val_accuracy: 0.7448\n",
      "Epoch 179/200\n",
      "5625/5625 [==============================] - 0s 36us/step - loss: 0.5196 - accuracy: 0.7459 - val_loss: 0.5202 - val_accuracy: 0.7449\n",
      "Epoch 180/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5195 - accuracy: 0.7459 - val_loss: 0.5203 - val_accuracy: 0.7444\n",
      "Epoch 181/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5195 - accuracy: 0.7457 - val_loss: 0.5199 - val_accuracy: 0.7456\n",
      "Epoch 182/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5195 - accuracy: 0.7462 - val_loss: 0.5197 - val_accuracy: 0.7450\n",
      "Epoch 183/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5195 - accuracy: 0.7467 - val_loss: 0.5200 - val_accuracy: 0.7456\n",
      "Epoch 184/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5194 - accuracy: 0.7455 - val_loss: 0.5208 - val_accuracy: 0.7450\n",
      "Epoch 185/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5195 - accuracy: 0.7459 - val_loss: 0.5198 - val_accuracy: 0.7456\n",
      "Epoch 186/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5195 - accuracy: 0.7462 - val_loss: 0.5199 - val_accuracy: 0.7457\n",
      "Epoch 187/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5194 - accuracy: 0.7457 - val_loss: 0.5202 - val_accuracy: 0.7449\n",
      "Epoch 188/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5194 - accuracy: 0.7461 - val_loss: 0.5197 - val_accuracy: 0.7458\n",
      "Epoch 189/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5194 - accuracy: 0.7459 - val_loss: 0.5198 - val_accuracy: 0.7450\n",
      "Epoch 190/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5194 - accuracy: 0.7463 - val_loss: 0.5204 - val_accuracy: 0.7454\n",
      "Epoch 191/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5194 - accuracy: 0.7466 - val_loss: 0.5203 - val_accuracy: 0.7447\n",
      "Epoch 192/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5194 - accuracy: 0.7460 - val_loss: 0.5199 - val_accuracy: 0.7451\n",
      "Epoch 193/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5193 - accuracy: 0.7462 - val_loss: 0.5198 - val_accuracy: 0.7459\n",
      "Epoch 194/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5193 - accuracy: 0.7458 - val_loss: 0.5200 - val_accuracy: 0.7449\n",
      "Epoch 195/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5192 - accuracy: 0.7465 - val_loss: 0.5203 - val_accuracy: 0.7442\n",
      "Epoch 196/200\n",
      "5625/5625 [==============================] - 0s 35us/step - loss: 0.5193 - accuracy: 0.7464 - val_loss: 0.5198 - val_accuracy: 0.7454\n",
      "Epoch 197/200\n",
      "5625/5625 [==============================] - 0s 38us/step - loss: 0.5192 - accuracy: 0.7463 - val_loss: 0.5200 - val_accuracy: 0.7444\n",
      "Epoch 198/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5193 - accuracy: 0.7455 - val_loss: 0.5202 - val_accuracy: 0.7443\n",
      "Epoch 199/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5192 - accuracy: 0.7464 - val_loss: 0.5204 - val_accuracy: 0.7444\n",
      "Epoch 200/200\n",
      "5625/5625 [==============================] - 0s 34us/step - loss: 0.5193 - accuracy: 0.7465 - val_loss: 0.5199 - val_accuracy: 0.7460\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_151 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 18)                594       \n",
      "=================================================================\n",
      "Total params: 4,122\n",
      "Trainable params: 4,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model trained in 40.09910988807678 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "inputs = x_d3.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d3.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "train_input_d3 = inputs[:,2:]\n",
    "train_output_d3 = targets[:,1:]\n",
    "\n",
    "x_train_d3, x_test_d3, Y_train_d3, Y_test_d3 = train_test_split(train_input_d3, train_output_d3, train_size=0.75, shuffle=True)\n",
    "\n",
    "model = compile_FFNN_model_DepthThree(3)\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x=x_train_d3,\n",
    "    y=Y_train_d3,\n",
    "    validation_split=.25,\n",
    "    epochs=200\n",
    ")\n",
    "model.summary()\n",
    "print(\"Model trained in %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate confusion matrix and F1 scores on test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:   1507\n",
      "False positive:  1549\n",
      "True negative:   30194\n",
      "False negative:  9250\n"
     ]
    }
   ],
   "source": [
    "predictions_d3 = model.predict(x_test_d3)\n",
    "\n",
    "y_pred = predictions_d3\n",
    "y_test = Y_test_d3\n",
    "\n",
    "y_pred[y_pred>=.5]=1 \n",
    "y_pred[y_pred<.5]=0\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(y_pred)-1):\n",
    "    for j in range(len(y_test[0])-1):\n",
    "        if y_test[i][j] == y_pred[i][j] and y_test[i][j] == 1:\n",
    "            TP += 1\n",
    "        if y_pred[i][j] == 1 and y_test[i][j] != y_pred[i][j]:\n",
    "            FP += 1\n",
    "        if y_test[i][j] == y_pred[i][j] and y_test[i][j] == 0:\n",
    "            TN += 1\n",
    "        if y_pred[i][j] == 0 and y_test[i][j] != y_pred[i][j]:\n",
    "            FN += 1\n",
    "            \n",
    "print(\"True positive:   \" + str(TP) + \"\\nFalse positive:  \" + str(FP) + \"\\nTrue negative:   \" + str(TN) + \"\\nFalse negative:  \" + str(FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Epoch vs. Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss (MSE)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 3 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Epoch vs. Accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 3 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_d3_v3-10k.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/200\n",
      "640/640 [==============================] - 0s 409us/step - loss: 0.6928 - accuracy: 0.5697 - val_loss: 0.6850 - val_accuracy: 0.6243\n",
      "Epoch 2/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.6790 - accuracy: 0.6809 - val_loss: 0.6726 - val_accuracy: 0.7329\n",
      "Epoch 3/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6672 - accuracy: 0.7730 - val_loss: 0.6614 - val_accuracy: 0.8220\n",
      "Epoch 4/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.6558 - accuracy: 0.8493 - val_loss: 0.6500 - val_accuracy: 0.8789\n",
      "Epoch 5/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6433 - accuracy: 0.8947 - val_loss: 0.6360 - val_accuracy: 0.9118\n",
      "Epoch 6/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.6271 - accuracy: 0.9155 - val_loss: 0.6184 - val_accuracy: 0.9187\n",
      "Epoch 7/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.6073 - accuracy: 0.9213 - val_loss: 0.5970 - val_accuracy: 0.9227\n",
      "Epoch 8/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5822 - accuracy: 0.9221 - val_loss: 0.5685 - val_accuracy: 0.9227\n",
      "Epoch 9/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.5474 - accuracy: 0.9219 - val_loss: 0.5273 - val_accuracy: 0.9227\n",
      "Epoch 10/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.4955 - accuracy: 0.9219 - val_loss: 0.4647 - val_accuracy: 0.9227\n",
      "Epoch 11/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.4212 - accuracy: 0.9219 - val_loss: 0.3830 - val_accuracy: 0.9227\n",
      "Epoch 12/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.3442 - accuracy: 0.9219 - val_loss: 0.3194 - val_accuracy: 0.9227\n",
      "Epoch 13/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.3008 - accuracy: 0.9219 - val_loss: 0.2933 - val_accuracy: 0.9227\n",
      "Epoch 14/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2859 - accuracy: 0.9219 - val_loss: 0.2841 - val_accuracy: 0.9227\n",
      "Epoch 15/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2803 - accuracy: 0.9219 - val_loss: 0.2794 - val_accuracy: 0.9227\n",
      "Epoch 16/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2771 - accuracy: 0.9219 - val_loss: 0.2765 - val_accuracy: 0.9227\n",
      "Epoch 17/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2749 - accuracy: 0.9219 - val_loss: 0.2743 - val_accuracy: 0.9227\n",
      "Epoch 18/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2732 - accuracy: 0.9219 - val_loss: 0.2728 - val_accuracy: 0.9227\n",
      "Epoch 19/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2720 - accuracy: 0.9219 - val_loss: 0.2716 - val_accuracy: 0.9227\n",
      "Epoch 20/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2710 - accuracy: 0.9219 - val_loss: 0.2707 - val_accuracy: 0.9227\n",
      "Epoch 21/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2704 - accuracy: 0.9219 - val_loss: 0.2700 - val_accuracy: 0.9227\n",
      "Epoch 22/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2698 - accuracy: 0.9219 - val_loss: 0.2696 - val_accuracy: 0.9227\n",
      "Epoch 23/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2694 - accuracy: 0.9219 - val_loss: 0.2691 - val_accuracy: 0.9227\n",
      "Epoch 24/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2691 - accuracy: 0.9219 - val_loss: 0.2688 - val_accuracy: 0.9227\n",
      "Epoch 25/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2688 - accuracy: 0.9219 - val_loss: 0.2686 - val_accuracy: 0.9227\n",
      "Epoch 26/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2686 - accuracy: 0.9219 - val_loss: 0.2684 - val_accuracy: 0.9227\n",
      "Epoch 27/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2685 - accuracy: 0.9219 - val_loss: 0.2682 - val_accuracy: 0.9227\n",
      "Epoch 28/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2683 - accuracy: 0.9219 - val_loss: 0.2680 - val_accuracy: 0.9227\n",
      "Epoch 29/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2682 - accuracy: 0.9219 - val_loss: 0.2679 - val_accuracy: 0.9227\n",
      "Epoch 30/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2680 - accuracy: 0.9219 - val_loss: 0.2677 - val_accuracy: 0.9227\n",
      "Epoch 31/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2679 - accuracy: 0.9219 - val_loss: 0.2675 - val_accuracy: 0.9227\n",
      "Epoch 32/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2679 - accuracy: 0.9219 - val_loss: 0.2675 - val_accuracy: 0.9227\n",
      "Epoch 33/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2677 - accuracy: 0.9219 - val_loss: 0.2674 - val_accuracy: 0.9227\n",
      "Epoch 34/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2677 - accuracy: 0.9219 - val_loss: 0.2673 - val_accuracy: 0.9227\n",
      "Epoch 35/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2676 - accuracy: 0.9219 - val_loss: 0.2672 - val_accuracy: 0.9227\n",
      "Epoch 36/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2675 - accuracy: 0.9219 - val_loss: 0.2671 - val_accuracy: 0.9227\n",
      "Epoch 37/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2674 - accuracy: 0.9219 - val_loss: 0.2670 - val_accuracy: 0.9227\n",
      "Epoch 38/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2673 - accuracy: 0.9219 - val_loss: 0.2669 - val_accuracy: 0.9227\n",
      "Epoch 39/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2673 - accuracy: 0.9219 - val_loss: 0.2668 - val_accuracy: 0.9227\n",
      "Epoch 40/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2671 - accuracy: 0.9219 - val_loss: 0.2667 - val_accuracy: 0.9227\n",
      "Epoch 41/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2671 - accuracy: 0.9219 - val_loss: 0.2667 - val_accuracy: 0.9227\n",
      "Epoch 42/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2670 - accuracy: 0.9219 - val_loss: 0.2666 - val_accuracy: 0.9227\n",
      "Epoch 43/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2670 - accuracy: 0.9219 - val_loss: 0.2665 - val_accuracy: 0.9227\n",
      "Epoch 44/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2669 - accuracy: 0.9219 - val_loss: 0.2664 - val_accuracy: 0.9227\n",
      "Epoch 45/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2668 - accuracy: 0.9219 - val_loss: 0.2663 - val_accuracy: 0.9227\n",
      "Epoch 46/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2668 - accuracy: 0.9219 - val_loss: 0.2663 - val_accuracy: 0.9227\n",
      "Epoch 47/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2667 - accuracy: 0.9219 - val_loss: 0.2662 - val_accuracy: 0.9227\n",
      "Epoch 48/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2667 - accuracy: 0.9219 - val_loss: 0.2662 - val_accuracy: 0.9227\n",
      "Epoch 49/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2666 - accuracy: 0.9219 - val_loss: 0.2661 - val_accuracy: 0.9227\n",
      "Epoch 50/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2666 - accuracy: 0.9219 - val_loss: 0.2660 - val_accuracy: 0.9227\n",
      "Epoch 51/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2665 - accuracy: 0.9219 - val_loss: 0.2660 - val_accuracy: 0.9227\n",
      "Epoch 52/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2664 - accuracy: 0.9219 - val_loss: 0.2659 - val_accuracy: 0.9227\n",
      "Epoch 53/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2664 - accuracy: 0.9219 - val_loss: 0.2658 - val_accuracy: 0.9227\n",
      "Epoch 54/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2663 - accuracy: 0.9219 - val_loss: 0.2658 - val_accuracy: 0.9227\n",
      "Epoch 55/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2663 - accuracy: 0.9219 - val_loss: 0.2657 - val_accuracy: 0.9227\n",
      "Epoch 56/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2662 - accuracy: 0.9219 - val_loss: 0.2657 - val_accuracy: 0.9227\n",
      "Epoch 57/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2661 - accuracy: 0.9219 - val_loss: 0.2656 - val_accuracy: 0.9227\n",
      "Epoch 58/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2660 - accuracy: 0.9219 - val_loss: 0.2655 - val_accuracy: 0.9227\n",
      "Epoch 59/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2660 - accuracy: 0.9219 - val_loss: 0.2654 - val_accuracy: 0.9227\n",
      "Epoch 60/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2659 - accuracy: 0.9219 - val_loss: 0.2653 - val_accuracy: 0.9227\n",
      "Epoch 61/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2659 - accuracy: 0.9219 - val_loss: 0.2652 - val_accuracy: 0.9227\n",
      "Epoch 62/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2658 - accuracy: 0.9219 - val_loss: 0.2651 - val_accuracy: 0.9227\n",
      "Epoch 63/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2658 - accuracy: 0.9219 - val_loss: 0.2650 - val_accuracy: 0.9227\n",
      "Epoch 64/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2657 - accuracy: 0.9219 - val_loss: 0.2650 - val_accuracy: 0.9227\n",
      "Epoch 65/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2657 - accuracy: 0.9219 - val_loss: 0.2649 - val_accuracy: 0.9227\n",
      "Epoch 66/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2656 - accuracy: 0.9219 - val_loss: 0.2649 - val_accuracy: 0.9227\n",
      "Epoch 67/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2655 - accuracy: 0.9219 - val_loss: 0.2649 - val_accuracy: 0.9227\n",
      "Epoch 68/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2655 - accuracy: 0.9219 - val_loss: 0.2648 - val_accuracy: 0.9227\n",
      "Epoch 69/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2654 - accuracy: 0.9219 - val_loss: 0.2648 - val_accuracy: 0.9227\n",
      "Epoch 70/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2654 - accuracy: 0.9219 - val_loss: 0.2647 - val_accuracy: 0.9227\n",
      "Epoch 71/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2653 - accuracy: 0.9219 - val_loss: 0.2646 - val_accuracy: 0.9227\n",
      "Epoch 72/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2653 - accuracy: 0.9219 - val_loss: 0.2646 - val_accuracy: 0.9227\n",
      "Epoch 73/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2652 - accuracy: 0.9219 - val_loss: 0.2644 - val_accuracy: 0.9227\n",
      "Epoch 74/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2651 - accuracy: 0.9219 - val_loss: 0.2644 - val_accuracy: 0.9227\n",
      "Epoch 75/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2651 - accuracy: 0.9219 - val_loss: 0.2643 - val_accuracy: 0.9227\n",
      "Epoch 76/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2650 - accuracy: 0.9219 - val_loss: 0.2642 - val_accuracy: 0.9227\n",
      "Epoch 77/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2649 - accuracy: 0.9219 - val_loss: 0.2642 - val_accuracy: 0.9227\n",
      "Epoch 78/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2649 - accuracy: 0.9219 - val_loss: 0.2641 - val_accuracy: 0.9227\n",
      "Epoch 79/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2648 - accuracy: 0.9219 - val_loss: 0.2640 - val_accuracy: 0.9227\n",
      "Epoch 80/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2647 - accuracy: 0.9219 - val_loss: 0.2640 - val_accuracy: 0.9227\n",
      "Epoch 81/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2647 - accuracy: 0.9219 - val_loss: 0.2639 - val_accuracy: 0.9227\n",
      "Epoch 82/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2646 - accuracy: 0.9219 - val_loss: 0.2638 - val_accuracy: 0.9227\n",
      "Epoch 83/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2646 - accuracy: 0.9219 - val_loss: 0.2638 - val_accuracy: 0.9227\n",
      "Epoch 84/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2645 - accuracy: 0.9219 - val_loss: 0.2636 - val_accuracy: 0.9227\n",
      "Epoch 85/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2643 - accuracy: 0.9219 - val_loss: 0.2636 - val_accuracy: 0.9227\n",
      "Epoch 86/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2642 - accuracy: 0.9219 - val_loss: 0.2634 - val_accuracy: 0.9227\n",
      "Epoch 87/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2642 - accuracy: 0.9219 - val_loss: 0.2633 - val_accuracy: 0.9227\n",
      "Epoch 88/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2641 - accuracy: 0.9219 - val_loss: 0.2632 - val_accuracy: 0.9227\n",
      "Epoch 89/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2640 - accuracy: 0.9219 - val_loss: 0.2632 - val_accuracy: 0.9227\n",
      "Epoch 90/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2639 - accuracy: 0.9219 - val_loss: 0.2631 - val_accuracy: 0.9227\n",
      "Epoch 91/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2638 - accuracy: 0.9219 - val_loss: 0.2630 - val_accuracy: 0.9227\n",
      "Epoch 92/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2637 - accuracy: 0.9219 - val_loss: 0.2629 - val_accuracy: 0.9227\n",
      "Epoch 93/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2636 - accuracy: 0.9219 - val_loss: 0.2628 - val_accuracy: 0.9227\n",
      "Epoch 94/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2636 - accuracy: 0.9219 - val_loss: 0.2627 - val_accuracy: 0.9227\n",
      "Epoch 95/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2634 - accuracy: 0.9219 - val_loss: 0.2626 - val_accuracy: 0.9227\n",
      "Epoch 96/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2634 - accuracy: 0.9219 - val_loss: 0.2625 - val_accuracy: 0.9227\n",
      "Epoch 97/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2633 - accuracy: 0.9219 - val_loss: 0.2624 - val_accuracy: 0.9227\n",
      "Epoch 98/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2632 - accuracy: 0.9219 - val_loss: 0.2623 - val_accuracy: 0.9227\n",
      "Epoch 99/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2631 - accuracy: 0.9219 - val_loss: 0.2622 - val_accuracy: 0.9227\n",
      "Epoch 100/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2630 - accuracy: 0.9219 - val_loss: 0.2621 - val_accuracy: 0.9227\n",
      "Epoch 101/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2629 - accuracy: 0.9219 - val_loss: 0.2621 - val_accuracy: 0.9227\n",
      "Epoch 102/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2628 - accuracy: 0.9219 - val_loss: 0.2619 - val_accuracy: 0.9227\n",
      "Epoch 103/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2627 - accuracy: 0.9219 - val_loss: 0.2618 - val_accuracy: 0.9227\n",
      "Epoch 104/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2626 - accuracy: 0.9219 - val_loss: 0.2617 - val_accuracy: 0.9227\n",
      "Epoch 105/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2625 - accuracy: 0.9219 - val_loss: 0.2616 - val_accuracy: 0.9227\n",
      "Epoch 106/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2624 - accuracy: 0.9219 - val_loss: 0.2614 - val_accuracy: 0.9227\n",
      "Epoch 107/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2622 - accuracy: 0.9219 - val_loss: 0.2613 - val_accuracy: 0.9227\n",
      "Epoch 108/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2621 - accuracy: 0.9219 - val_loss: 0.2612 - val_accuracy: 0.9227\n",
      "Epoch 109/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2620 - accuracy: 0.9219 - val_loss: 0.2610 - val_accuracy: 0.9227\n",
      "Epoch 110/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2618 - accuracy: 0.9219 - val_loss: 0.2609 - val_accuracy: 0.9227\n",
      "Epoch 111/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2617 - accuracy: 0.9219 - val_loss: 0.2608 - val_accuracy: 0.9227\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 0s 33us/step - loss: 0.2616 - accuracy: 0.9219 - val_loss: 0.2607 - val_accuracy: 0.9227\n",
      "Epoch 113/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2614 - accuracy: 0.9219 - val_loss: 0.2605 - val_accuracy: 0.9227\n",
      "Epoch 114/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2613 - accuracy: 0.9219 - val_loss: 0.2604 - val_accuracy: 0.9227\n",
      "Epoch 115/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2611 - accuracy: 0.9219 - val_loss: 0.2603 - val_accuracy: 0.9227\n",
      "Epoch 116/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2610 - accuracy: 0.9219 - val_loss: 0.2601 - val_accuracy: 0.9227\n",
      "Epoch 117/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2608 - accuracy: 0.9219 - val_loss: 0.2600 - val_accuracy: 0.9227\n",
      "Epoch 118/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2606 - accuracy: 0.9219 - val_loss: 0.2599 - val_accuracy: 0.9227\n",
      "Epoch 119/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2605 - accuracy: 0.9219 - val_loss: 0.2597 - val_accuracy: 0.9227\n",
      "Epoch 120/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2603 - accuracy: 0.9219 - val_loss: 0.2596 - val_accuracy: 0.9227\n",
      "Epoch 121/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2601 - accuracy: 0.9219 - val_loss: 0.2594 - val_accuracy: 0.9227\n",
      "Epoch 122/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2600 - accuracy: 0.9219 - val_loss: 0.2593 - val_accuracy: 0.9227\n",
      "Epoch 123/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2598 - accuracy: 0.9219 - val_loss: 0.2591 - val_accuracy: 0.9227\n",
      "Epoch 124/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2596 - accuracy: 0.9219 - val_loss: 0.2588 - val_accuracy: 0.9227\n",
      "Epoch 125/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2594 - accuracy: 0.9219 - val_loss: 0.2587 - val_accuracy: 0.9227\n",
      "Epoch 126/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2592 - accuracy: 0.9219 - val_loss: 0.2585 - val_accuracy: 0.9227\n",
      "Epoch 127/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2590 - accuracy: 0.9219 - val_loss: 0.2582 - val_accuracy: 0.9227\n",
      "Epoch 128/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2587 - accuracy: 0.9219 - val_loss: 0.2579 - val_accuracy: 0.9227\n",
      "Epoch 129/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2585 - accuracy: 0.9219 - val_loss: 0.2578 - val_accuracy: 0.9227\n",
      "Epoch 130/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2584 - accuracy: 0.9219 - val_loss: 0.2576 - val_accuracy: 0.9227\n",
      "Epoch 131/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2581 - accuracy: 0.9219 - val_loss: 0.2574 - val_accuracy: 0.9227\n",
      "Epoch 132/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2578 - accuracy: 0.9219 - val_loss: 0.2571 - val_accuracy: 0.9227\n",
      "Epoch 133/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2576 - accuracy: 0.9219 - val_loss: 0.2570 - val_accuracy: 0.9227\n",
      "Epoch 134/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2573 - accuracy: 0.9219 - val_loss: 0.2567 - val_accuracy: 0.9227\n",
      "Epoch 135/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2570 - accuracy: 0.9219 - val_loss: 0.2565 - val_accuracy: 0.9227\n",
      "Epoch 136/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2567 - accuracy: 0.9219 - val_loss: 0.2561 - val_accuracy: 0.9227\n",
      "Epoch 137/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2564 - accuracy: 0.9219 - val_loss: 0.2559 - val_accuracy: 0.9227\n",
      "Epoch 138/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2560 - accuracy: 0.9219 - val_loss: 0.2556 - val_accuracy: 0.9227\n",
      "Epoch 139/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2557 - accuracy: 0.9219 - val_loss: 0.2552 - val_accuracy: 0.9227\n",
      "Epoch 140/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2554 - accuracy: 0.9219 - val_loss: 0.2549 - val_accuracy: 0.9227\n",
      "Epoch 141/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2550 - accuracy: 0.9219 - val_loss: 0.2546 - val_accuracy: 0.9227\n",
      "Epoch 142/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2547 - accuracy: 0.9219 - val_loss: 0.2543 - val_accuracy: 0.9227\n",
      "Epoch 143/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2543 - accuracy: 0.9219 - val_loss: 0.2541 - val_accuracy: 0.9227\n",
      "Epoch 144/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2540 - accuracy: 0.9219 - val_loss: 0.2537 - val_accuracy: 0.9227\n",
      "Epoch 145/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2536 - accuracy: 0.9219 - val_loss: 0.2533 - val_accuracy: 0.9227\n",
      "Epoch 146/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2532 - accuracy: 0.9219 - val_loss: 0.2530 - val_accuracy: 0.9227\n",
      "Epoch 147/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2529 - accuracy: 0.9219 - val_loss: 0.2528 - val_accuracy: 0.9227\n",
      "Epoch 148/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2525 - accuracy: 0.9219 - val_loss: 0.2524 - val_accuracy: 0.9227\n",
      "Epoch 149/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2521 - accuracy: 0.9219 - val_loss: 0.2520 - val_accuracy: 0.9227\n",
      "Epoch 150/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2518 - accuracy: 0.9219 - val_loss: 0.2517 - val_accuracy: 0.9227\n",
      "Epoch 151/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2514 - accuracy: 0.9219 - val_loss: 0.2514 - val_accuracy: 0.9227\n",
      "Epoch 152/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2511 - accuracy: 0.9219 - val_loss: 0.2511 - val_accuracy: 0.9227\n",
      "Epoch 153/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2507 - accuracy: 0.9219 - val_loss: 0.2507 - val_accuracy: 0.9227\n",
      "Epoch 154/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2503 - accuracy: 0.9219 - val_loss: 0.2503 - val_accuracy: 0.9227\n",
      "Epoch 155/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2500 - accuracy: 0.9219 - val_loss: 0.2499 - val_accuracy: 0.9227\n",
      "Epoch 156/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2496 - accuracy: 0.9219 - val_loss: 0.2496 - val_accuracy: 0.9227\n",
      "Epoch 157/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2492 - accuracy: 0.9219 - val_loss: 0.2493 - val_accuracy: 0.9227\n",
      "Epoch 158/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2488 - accuracy: 0.9219 - val_loss: 0.2488 - val_accuracy: 0.9227\n",
      "Epoch 159/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2485 - accuracy: 0.9219 - val_loss: 0.2485 - val_accuracy: 0.9227\n",
      "Epoch 160/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2482 - accuracy: 0.9219 - val_loss: 0.2481 - val_accuracy: 0.9227\n",
      "Epoch 161/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2478 - accuracy: 0.9219 - val_loss: 0.2477 - val_accuracy: 0.9227\n",
      "Epoch 162/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2475 - accuracy: 0.9219 - val_loss: 0.2474 - val_accuracy: 0.9227\n",
      "Epoch 163/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2471 - accuracy: 0.9219 - val_loss: 0.2471 - val_accuracy: 0.9227\n",
      "Epoch 164/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2468 - accuracy: 0.9219 - val_loss: 0.2467 - val_accuracy: 0.9227\n",
      "Epoch 165/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2465 - accuracy: 0.9219 - val_loss: 0.2464 - val_accuracy: 0.9227\n",
      "Epoch 166/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2462 - accuracy: 0.9219 - val_loss: 0.2460 - val_accuracy: 0.9227\n",
      "Epoch 167/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2458 - accuracy: 0.9219 - val_loss: 0.2457 - val_accuracy: 0.9227\n",
      "Epoch 168/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2455 - accuracy: 0.9219 - val_loss: 0.2455 - val_accuracy: 0.9227\n",
      "Epoch 169/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2452 - accuracy: 0.9219 - val_loss: 0.2451 - val_accuracy: 0.9227\n",
      "Epoch 170/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2449 - accuracy: 0.9219 - val_loss: 0.2449 - val_accuracy: 0.9227\n",
      "Epoch 171/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2446 - accuracy: 0.9219 - val_loss: 0.2446 - val_accuracy: 0.9227\n",
      "Epoch 172/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2443 - accuracy: 0.9219 - val_loss: 0.2443 - val_accuracy: 0.9227\n",
      "Epoch 173/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2440 - accuracy: 0.9219 - val_loss: 0.2441 - val_accuracy: 0.9227\n",
      "Epoch 174/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2437 - accuracy: 0.9219 - val_loss: 0.2438 - val_accuracy: 0.9227\n",
      "Epoch 175/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2435 - accuracy: 0.9219 - val_loss: 0.2435 - val_accuracy: 0.9227\n",
      "Epoch 176/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2432 - accuracy: 0.9219 - val_loss: 0.2432 - val_accuracy: 0.9230\n",
      "Epoch 177/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2429 - accuracy: 0.9217 - val_loss: 0.2430 - val_accuracy: 0.9227\n",
      "Epoch 178/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2427 - accuracy: 0.9219 - val_loss: 0.2428 - val_accuracy: 0.9230\n",
      "Epoch 179/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2424 - accuracy: 0.9218 - val_loss: 0.2426 - val_accuracy: 0.9227\n",
      "Epoch 180/200\n",
      "640/640 [==============================] - 0s 31us/step - loss: 0.2421 - accuracy: 0.9219 - val_loss: 0.2423 - val_accuracy: 0.9227\n",
      "Epoch 181/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2419 - accuracy: 0.9220 - val_loss: 0.2420 - val_accuracy: 0.9227\n",
      "Epoch 182/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2417 - accuracy: 0.9219 - val_loss: 0.2418 - val_accuracy: 0.9230\n",
      "Epoch 183/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2414 - accuracy: 0.9218 - val_loss: 0.2415 - val_accuracy: 0.9230\n",
      "Epoch 184/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2412 - accuracy: 0.9218 - val_loss: 0.2413 - val_accuracy: 0.9230\n",
      "Epoch 185/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2409 - accuracy: 0.9220 - val_loss: 0.2412 - val_accuracy: 0.9227\n",
      "Epoch 186/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2407 - accuracy: 0.9217 - val_loss: 0.2409 - val_accuracy: 0.9227\n",
      "Epoch 187/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2405 - accuracy: 0.9218 - val_loss: 0.2407 - val_accuracy: 0.9227\n",
      "Epoch 188/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2402 - accuracy: 0.9219 - val_loss: 0.2405 - val_accuracy: 0.9227\n",
      "Epoch 189/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2400 - accuracy: 0.9219 - val_loss: 0.2403 - val_accuracy: 0.9227\n",
      "Epoch 190/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2398 - accuracy: 0.9215 - val_loss: 0.2401 - val_accuracy: 0.9230\n",
      "Epoch 191/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2395 - accuracy: 0.9218 - val_loss: 0.2398 - val_accuracy: 0.9227\n",
      "Epoch 192/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2393 - accuracy: 0.9219 - val_loss: 0.2395 - val_accuracy: 0.9224\n",
      "Epoch 193/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2391 - accuracy: 0.9220 - val_loss: 0.2394 - val_accuracy: 0.9227\n",
      "Epoch 194/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2389 - accuracy: 0.9219 - val_loss: 0.2392 - val_accuracy: 0.9227\n",
      "Epoch 195/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2387 - accuracy: 0.9220 - val_loss: 0.2390 - val_accuracy: 0.9227\n",
      "Epoch 196/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2385 - accuracy: 0.9219 - val_loss: 0.2388 - val_accuracy: 0.9227\n",
      "Epoch 197/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2383 - accuracy: 0.9216 - val_loss: 0.2386 - val_accuracy: 0.9227\n",
      "Epoch 198/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2380 - accuracy: 0.9219 - val_loss: 0.2385 - val_accuracy: 0.9227\n",
      "Epoch 199/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2379 - accuracy: 0.9219 - val_loss: 0.2383 - val_accuracy: 0.9227\n",
      "Epoch 200/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2377 - accuracy: 0.9217 - val_loss: 0.2380 - val_accuracy: 0.9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 640 samples, validate on 161 samples\n",
      "Epoch 1/200\n",
      "640/640 [==============================] - 0s 216us/step - loss: 0.6857 - accuracy: 0.7127 - val_loss: 0.6781 - val_accuracy: 0.8408\n",
      "Epoch 2/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.6700 - accuracy: 0.8676 - val_loss: 0.6608 - val_accuracy: 0.8761\n",
      "Epoch 3/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6520 - accuracy: 0.8703 - val_loss: 0.6426 - val_accuracy: 0.8715\n",
      "Epoch 4/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6334 - accuracy: 0.8752 - val_loss: 0.6235 - val_accuracy: 0.8794\n",
      "Epoch 5/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.6134 - accuracy: 0.8802 - val_loss: 0.6022 - val_accuracy: 0.8800\n",
      "Epoch 6/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.5904 - accuracy: 0.8829 - val_loss: 0.5771 - val_accuracy: 0.8862\n",
      "Epoch 7/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5624 - accuracy: 0.8926 - val_loss: 0.5458 - val_accuracy: 0.9000\n",
      "Epoch 8/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5266 - accuracy: 0.9169 - val_loss: 0.5047 - val_accuracy: 0.9232\n",
      "Epoch 9/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.4789 - accuracy: 0.9221 - val_loss: 0.4498 - val_accuracy: 0.9232\n",
      "Epoch 10/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.4175 - accuracy: 0.9221 - val_loss: 0.3842 - val_accuracy: 0.9232\n",
      "Epoch 11/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.3536 - accuracy: 0.9221 - val_loss: 0.3278 - val_accuracy: 0.9232\n",
      "Epoch 12/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.3089 - accuracy: 0.9221 - val_loss: 0.2979 - val_accuracy: 0.9232\n",
      "Epoch 13/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2887 - accuracy: 0.9221 - val_loss: 0.2863 - val_accuracy: 0.9232\n",
      "Epoch 14/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2808 - accuracy: 0.9221 - val_loss: 0.2808 - val_accuracy: 0.9232\n",
      "Epoch 15/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2767 - accuracy: 0.9221 - val_loss: 0.2775 - val_accuracy: 0.9232\n",
      "Epoch 16/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2741 - accuracy: 0.9221 - val_loss: 0.2751 - val_accuracy: 0.9232\n",
      "Epoch 17/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2723 - accuracy: 0.9221 - val_loss: 0.2732 - val_accuracy: 0.9232\n",
      "Epoch 18/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2711 - accuracy: 0.9221 - val_loss: 0.2718 - val_accuracy: 0.9232\n",
      "Epoch 19/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2701 - accuracy: 0.9221 - val_loss: 0.2707 - val_accuracy: 0.9232\n",
      "Epoch 20/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2694 - accuracy: 0.9221 - val_loss: 0.2698 - val_accuracy: 0.9232\n",
      "Epoch 21/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2689 - accuracy: 0.9221 - val_loss: 0.2691 - val_accuracy: 0.9232\n",
      "Epoch 22/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2685 - accuracy: 0.9221 - val_loss: 0.2685 - val_accuracy: 0.9232\n",
      "Epoch 23/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2682 - accuracy: 0.9221 - val_loss: 0.2679 - val_accuracy: 0.9232\n",
      "Epoch 24/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2679 - accuracy: 0.9221 - val_loss: 0.2676 - val_accuracy: 0.9232\n",
      "Epoch 25/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2677 - accuracy: 0.9221 - val_loss: 0.2672 - val_accuracy: 0.9232\n",
      "Epoch 26/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2675 - accuracy: 0.9221 - val_loss: 0.2670 - val_accuracy: 0.9232\n",
      "Epoch 27/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2674 - accuracy: 0.9221 - val_loss: 0.2667 - val_accuracy: 0.9232\n",
      "Epoch 28/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2673 - accuracy: 0.9221 - val_loss: 0.2665 - val_accuracy: 0.9232\n",
      "Epoch 29/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2671 - accuracy: 0.9221 - val_loss: 0.2663 - val_accuracy: 0.9232\n",
      "Epoch 30/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2671 - accuracy: 0.9221 - val_loss: 0.2662 - val_accuracy: 0.9232\n",
      "Epoch 31/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2670 - accuracy: 0.9221 - val_loss: 0.2660 - val_accuracy: 0.9232\n",
      "Epoch 32/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2669 - accuracy: 0.9221 - val_loss: 0.2659 - val_accuracy: 0.9232\n",
      "Epoch 33/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2668 - accuracy: 0.9221 - val_loss: 0.2657 - val_accuracy: 0.9232\n",
      "Epoch 34/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2668 - accuracy: 0.9221 - val_loss: 0.2656 - val_accuracy: 0.9232\n",
      "Epoch 35/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2667 - accuracy: 0.9221 - val_loss: 0.2655 - val_accuracy: 0.9232\n",
      "Epoch 36/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2667 - accuracy: 0.9221 - val_loss: 0.2655 - val_accuracy: 0.9232\n",
      "Epoch 37/200\n",
      "640/640 [==============================] - 0s 47us/step - loss: 0.2666 - accuracy: 0.9221 - val_loss: 0.2654 - val_accuracy: 0.9232\n",
      "Epoch 38/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2665 - accuracy: 0.9221 - val_loss: 0.2653 - val_accuracy: 0.9232\n",
      "Epoch 39/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2665 - accuracy: 0.9221 - val_loss: 0.2652 - val_accuracy: 0.9232\n",
      "Epoch 40/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2665 - accuracy: 0.9221 - val_loss: 0.2651 - val_accuracy: 0.9232\n",
      "Epoch 41/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2664 - accuracy: 0.9221 - val_loss: 0.2651 - val_accuracy: 0.9232\n",
      "Epoch 42/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2664 - accuracy: 0.9221 - val_loss: 0.2650 - val_accuracy: 0.9232\n",
      "Epoch 43/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2663 - accuracy: 0.9221 - val_loss: 0.2650 - val_accuracy: 0.9232\n",
      "Epoch 44/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2663 - accuracy: 0.9221 - val_loss: 0.2649 - val_accuracy: 0.9232\n",
      "Epoch 45/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2662 - accuracy: 0.9221 - val_loss: 0.2649 - val_accuracy: 0.9232\n",
      "Epoch 46/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2662 - accuracy: 0.9221 - val_loss: 0.2648 - val_accuracy: 0.9232\n",
      "Epoch 47/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2662 - accuracy: 0.9221 - val_loss: 0.2647 - val_accuracy: 0.9232\n",
      "Epoch 48/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2661 - accuracy: 0.9221 - val_loss: 0.2647 - val_accuracy: 0.9232\n",
      "Epoch 49/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2661 - accuracy: 0.9221 - val_loss: 0.2647 - val_accuracy: 0.9232\n",
      "Epoch 50/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2661 - accuracy: 0.9221 - val_loss: 0.2646 - val_accuracy: 0.9232\n",
      "Epoch 51/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2660 - accuracy: 0.9221 - val_loss: 0.2646 - val_accuracy: 0.9232\n",
      "Epoch 52/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2660 - accuracy: 0.9221 - val_loss: 0.2645 - val_accuracy: 0.9232\n",
      "Epoch 53/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2660 - accuracy: 0.9221 - val_loss: 0.2645 - val_accuracy: 0.9232\n",
      "Epoch 54/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2659 - accuracy: 0.9221 - val_loss: 0.2645 - val_accuracy: 0.9232\n",
      "Epoch 55/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2659 - accuracy: 0.9221 - val_loss: 0.2644 - val_accuracy: 0.9232\n",
      "Epoch 56/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2659 - accuracy: 0.9221 - val_loss: 0.2644 - val_accuracy: 0.9232\n",
      "Epoch 57/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2658 - accuracy: 0.9221 - val_loss: 0.2644 - val_accuracy: 0.9232\n",
      "Epoch 58/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2658 - accuracy: 0.9221 - val_loss: 0.2644 - val_accuracy: 0.9232\n",
      "Epoch 59/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2658 - accuracy: 0.9221 - val_loss: 0.2643 - val_accuracy: 0.9232\n",
      "Epoch 60/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2657 - accuracy: 0.9221 - val_loss: 0.2642 - val_accuracy: 0.9232\n",
      "Epoch 61/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2657 - accuracy: 0.9221 - val_loss: 0.2642 - val_accuracy: 0.9232\n",
      "Epoch 62/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2657 - accuracy: 0.9221 - val_loss: 0.2642 - val_accuracy: 0.9232\n",
      "Epoch 63/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2657 - accuracy: 0.9221 - val_loss: 0.2641 - val_accuracy: 0.9232\n",
      "Epoch 64/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2656 - accuracy: 0.9221 - val_loss: 0.2641 - val_accuracy: 0.9232\n",
      "Epoch 65/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2656 - accuracy: 0.9221 - val_loss: 0.2640 - val_accuracy: 0.9232\n",
      "Epoch 66/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2656 - accuracy: 0.9221 - val_loss: 0.2640 - val_accuracy: 0.9232\n",
      "Epoch 67/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2655 - accuracy: 0.9221 - val_loss: 0.2640 - val_accuracy: 0.9232\n",
      "Epoch 68/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2655 - accuracy: 0.9221 - val_loss: 0.2640 - val_accuracy: 0.9232\n",
      "Epoch 69/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2655 - accuracy: 0.9221 - val_loss: 0.2639 - val_accuracy: 0.9232\n",
      "Epoch 70/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2655 - accuracy: 0.9221 - val_loss: 0.2639 - val_accuracy: 0.9232\n",
      "Epoch 71/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2654 - accuracy: 0.9221 - val_loss: 0.2639 - val_accuracy: 0.9232\n",
      "Epoch 72/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2654 - accuracy: 0.9221 - val_loss: 0.2638 - val_accuracy: 0.9232\n",
      "Epoch 73/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2654 - accuracy: 0.9221 - val_loss: 0.2638 - val_accuracy: 0.9232\n",
      "Epoch 74/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2654 - accuracy: 0.9221 - val_loss: 0.2638 - val_accuracy: 0.9232\n",
      "Epoch 75/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2653 - accuracy: 0.9221 - val_loss: 0.2637 - val_accuracy: 0.9232\n",
      "Epoch 76/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2653 - accuracy: 0.9221 - val_loss: 0.2637 - val_accuracy: 0.9232\n",
      "Epoch 77/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2653 - accuracy: 0.9221 - val_loss: 0.2637 - val_accuracy: 0.9232\n",
      "Epoch 78/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2652 - accuracy: 0.9221 - val_loss: 0.2636 - val_accuracy: 0.9232\n",
      "Epoch 79/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2652 - accuracy: 0.9221 - val_loss: 0.2636 - val_accuracy: 0.9232\n",
      "Epoch 80/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2652 - accuracy: 0.9221 - val_loss: 0.2636 - val_accuracy: 0.9232\n",
      "Epoch 81/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2651 - accuracy: 0.9221 - val_loss: 0.2635 - val_accuracy: 0.9232\n",
      "Epoch 82/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2651 - accuracy: 0.9221 - val_loss: 0.2635 - val_accuracy: 0.9232\n",
      "Epoch 83/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2651 - accuracy: 0.9221 - val_loss: 0.2635 - val_accuracy: 0.9232\n",
      "Epoch 84/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2650 - accuracy: 0.9221 - val_loss: 0.2635 - val_accuracy: 0.9232\n",
      "Epoch 85/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2650 - accuracy: 0.9221 - val_loss: 0.2634 - val_accuracy: 0.9232\n",
      "Epoch 86/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2650 - accuracy: 0.9221 - val_loss: 0.2634 - val_accuracy: 0.9232\n",
      "Epoch 87/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2650 - accuracy: 0.9221 - val_loss: 0.2633 - val_accuracy: 0.9232\n",
      "Epoch 88/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2649 - accuracy: 0.9221 - val_loss: 0.2633 - val_accuracy: 0.9232\n",
      "Epoch 89/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2649 - accuracy: 0.9221 - val_loss: 0.2633 - val_accuracy: 0.9232\n",
      "Epoch 90/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2649 - accuracy: 0.9221 - val_loss: 0.2632 - val_accuracy: 0.9232\n",
      "Epoch 91/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2648 - accuracy: 0.9221 - val_loss: 0.2632 - val_accuracy: 0.9232\n",
      "Epoch 92/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2648 - accuracy: 0.9221 - val_loss: 0.2631 - val_accuracy: 0.9232\n",
      "Epoch 93/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2648 - accuracy: 0.9221 - val_loss: 0.2631 - val_accuracy: 0.9232\n",
      "Epoch 94/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2647 - accuracy: 0.9221 - val_loss: 0.2631 - val_accuracy: 0.9232\n",
      "Epoch 95/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2647 - accuracy: 0.9221 - val_loss: 0.2631 - val_accuracy: 0.9232\n",
      "Epoch 96/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2647 - accuracy: 0.9221 - val_loss: 0.2630 - val_accuracy: 0.9232\n",
      "Epoch 97/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2646 - accuracy: 0.9221 - val_loss: 0.2629 - val_accuracy: 0.9232\n",
      "Epoch 98/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2646 - accuracy: 0.9221 - val_loss: 0.2629 - val_accuracy: 0.9232\n",
      "Epoch 99/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2646 - accuracy: 0.9221 - val_loss: 0.2629 - val_accuracy: 0.9232\n",
      "Epoch 100/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2645 - accuracy: 0.9221 - val_loss: 0.2629 - val_accuracy: 0.9232\n",
      "Epoch 101/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2645 - accuracy: 0.9221 - val_loss: 0.2628 - val_accuracy: 0.9232\n",
      "Epoch 102/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2644 - accuracy: 0.9221 - val_loss: 0.2628 - val_accuracy: 0.9232\n",
      "Epoch 103/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2644 - accuracy: 0.9221 - val_loss: 0.2628 - val_accuracy: 0.9232\n",
      "Epoch 104/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2644 - accuracy: 0.9221 - val_loss: 0.2627 - val_accuracy: 0.9232\n",
      "Epoch 105/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2643 - accuracy: 0.9221 - val_loss: 0.2627 - val_accuracy: 0.9232\n",
      "Epoch 106/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2643 - accuracy: 0.9221 - val_loss: 0.2626 - val_accuracy: 0.9232\n",
      "Epoch 107/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2642 - accuracy: 0.9221 - val_loss: 0.2626 - val_accuracy: 0.9232\n",
      "Epoch 108/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2642 - accuracy: 0.9221 - val_loss: 0.2626 - val_accuracy: 0.9232\n",
      "Epoch 109/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2642 - accuracy: 0.9221 - val_loss: 0.2625 - val_accuracy: 0.9232\n",
      "Epoch 110/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2641 - accuracy: 0.9221 - val_loss: 0.2625 - val_accuracy: 0.9232\n",
      "Epoch 111/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2641 - accuracy: 0.9221 - val_loss: 0.2624 - val_accuracy: 0.9232\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 0s 38us/step - loss: 0.2640 - accuracy: 0.9221 - val_loss: 0.2623 - val_accuracy: 0.9232\n",
      "Epoch 113/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2640 - accuracy: 0.9221 - val_loss: 0.2623 - val_accuracy: 0.9232\n",
      "Epoch 114/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2639 - accuracy: 0.9221 - val_loss: 0.2622 - val_accuracy: 0.9232\n",
      "Epoch 115/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2639 - accuracy: 0.9221 - val_loss: 0.2622 - val_accuracy: 0.9232\n",
      "Epoch 116/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2638 - accuracy: 0.9221 - val_loss: 0.2622 - val_accuracy: 0.9232\n",
      "Epoch 117/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2638 - accuracy: 0.9221 - val_loss: 0.2621 - val_accuracy: 0.9232\n",
      "Epoch 118/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2637 - accuracy: 0.9221 - val_loss: 0.2621 - val_accuracy: 0.9232\n",
      "Epoch 119/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2637 - accuracy: 0.9221 - val_loss: 0.2620 - val_accuracy: 0.9232\n",
      "Epoch 120/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2636 - accuracy: 0.9221 - val_loss: 0.2620 - val_accuracy: 0.9232\n",
      "Epoch 121/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2636 - accuracy: 0.9221 - val_loss: 0.2619 - val_accuracy: 0.9232\n",
      "Epoch 122/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2635 - accuracy: 0.9221 - val_loss: 0.2618 - val_accuracy: 0.9232\n",
      "Epoch 123/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2635 - accuracy: 0.9221 - val_loss: 0.2618 - val_accuracy: 0.9232\n",
      "Epoch 124/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2634 - accuracy: 0.9221 - val_loss: 0.2617 - val_accuracy: 0.9232\n",
      "Epoch 125/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2633 - accuracy: 0.9221 - val_loss: 0.2617 - val_accuracy: 0.9232\n",
      "Epoch 126/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2633 - accuracy: 0.9221 - val_loss: 0.2616 - val_accuracy: 0.9232\n",
      "Epoch 127/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2632 - accuracy: 0.9221 - val_loss: 0.2615 - val_accuracy: 0.9232\n",
      "Epoch 128/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2632 - accuracy: 0.9221 - val_loss: 0.2614 - val_accuracy: 0.9232\n",
      "Epoch 129/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2631 - accuracy: 0.9221 - val_loss: 0.2614 - val_accuracy: 0.9232\n",
      "Epoch 130/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2630 - accuracy: 0.9221 - val_loss: 0.2613 - val_accuracy: 0.9232\n",
      "Epoch 131/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2630 - accuracy: 0.9221 - val_loss: 0.2613 - val_accuracy: 0.9232\n",
      "Epoch 132/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2629 - accuracy: 0.9221 - val_loss: 0.2612 - val_accuracy: 0.9232\n",
      "Epoch 133/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2628 - accuracy: 0.9221 - val_loss: 0.2611 - val_accuracy: 0.9232\n",
      "Epoch 134/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2628 - accuracy: 0.9221 - val_loss: 0.2610 - val_accuracy: 0.9232\n",
      "Epoch 135/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2627 - accuracy: 0.9221 - val_loss: 0.2610 - val_accuracy: 0.9232\n",
      "Epoch 136/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2626 - accuracy: 0.9221 - val_loss: 0.2609 - val_accuracy: 0.9232\n",
      "Epoch 137/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2625 - accuracy: 0.9221 - val_loss: 0.2608 - val_accuracy: 0.9232\n",
      "Epoch 138/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2624 - accuracy: 0.9221 - val_loss: 0.2608 - val_accuracy: 0.9232\n",
      "Epoch 139/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2624 - accuracy: 0.9221 - val_loss: 0.2607 - val_accuracy: 0.9232\n",
      "Epoch 140/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2623 - accuracy: 0.9221 - val_loss: 0.2606 - val_accuracy: 0.9232\n",
      "Epoch 141/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2622 - accuracy: 0.9221 - val_loss: 0.2604 - val_accuracy: 0.9232\n",
      "Epoch 142/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2621 - accuracy: 0.9221 - val_loss: 0.2604 - val_accuracy: 0.9232\n",
      "Epoch 143/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2620 - accuracy: 0.9221 - val_loss: 0.2602 - val_accuracy: 0.9232\n",
      "Epoch 144/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2619 - accuracy: 0.9221 - val_loss: 0.2601 - val_accuracy: 0.9232\n",
      "Epoch 145/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2618 - accuracy: 0.9221 - val_loss: 0.2601 - val_accuracy: 0.9232\n",
      "Epoch 146/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2617 - accuracy: 0.9221 - val_loss: 0.2599 - val_accuracy: 0.9232\n",
      "Epoch 147/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2616 - accuracy: 0.9221 - val_loss: 0.2598 - val_accuracy: 0.9232\n",
      "Epoch 148/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2614 - accuracy: 0.9221 - val_loss: 0.2597 - val_accuracy: 0.9232\n",
      "Epoch 149/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2613 - accuracy: 0.9221 - val_loss: 0.2596 - val_accuracy: 0.9232\n",
      "Epoch 150/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2612 - accuracy: 0.9221 - val_loss: 0.2594 - val_accuracy: 0.9232\n",
      "Epoch 151/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2611 - accuracy: 0.9221 - val_loss: 0.2593 - val_accuracy: 0.9232\n",
      "Epoch 152/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2609 - accuracy: 0.9221 - val_loss: 0.2592 - val_accuracy: 0.9232\n",
      "Epoch 153/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2608 - accuracy: 0.9221 - val_loss: 0.2590 - val_accuracy: 0.9232\n",
      "Epoch 154/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2606 - accuracy: 0.9221 - val_loss: 0.2589 - val_accuracy: 0.9232\n",
      "Epoch 155/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2605 - accuracy: 0.9221 - val_loss: 0.2587 - val_accuracy: 0.9232\n",
      "Epoch 156/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2603 - accuracy: 0.9221 - val_loss: 0.2585 - val_accuracy: 0.9232\n",
      "Epoch 157/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2601 - accuracy: 0.9221 - val_loss: 0.2584 - val_accuracy: 0.9232\n",
      "Epoch 158/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2600 - accuracy: 0.9221 - val_loss: 0.2582 - val_accuracy: 0.9232\n",
      "Epoch 159/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2599 - accuracy: 0.9221 - val_loss: 0.2581 - val_accuracy: 0.9232\n",
      "Epoch 160/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2597 - accuracy: 0.9221 - val_loss: 0.2578 - val_accuracy: 0.9232\n",
      "Epoch 161/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2595 - accuracy: 0.9221 - val_loss: 0.2577 - val_accuracy: 0.9232\n",
      "Epoch 162/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2593 - accuracy: 0.9221 - val_loss: 0.2575 - val_accuracy: 0.9232\n",
      "Epoch 163/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2591 - accuracy: 0.9221 - val_loss: 0.2573 - val_accuracy: 0.9232\n",
      "Epoch 164/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2589 - accuracy: 0.9221 - val_loss: 0.2571 - val_accuracy: 0.9232\n",
      "Epoch 165/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2587 - accuracy: 0.9221 - val_loss: 0.2569 - val_accuracy: 0.9232\n",
      "Epoch 166/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2585 - accuracy: 0.9221 - val_loss: 0.2567 - val_accuracy: 0.9232\n",
      "Epoch 167/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2583 - accuracy: 0.9221 - val_loss: 0.2564 - val_accuracy: 0.9232\n",
      "Epoch 168/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2581 - accuracy: 0.9221 - val_loss: 0.2562 - val_accuracy: 0.9232\n",
      "Epoch 169/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2578 - accuracy: 0.9221 - val_loss: 0.2560 - val_accuracy: 0.9232\n",
      "Epoch 170/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2576 - accuracy: 0.9221 - val_loss: 0.2557 - val_accuracy: 0.9232\n",
      "Epoch 171/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2573 - accuracy: 0.9221 - val_loss: 0.2555 - val_accuracy: 0.9232\n",
      "Epoch 172/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2570 - accuracy: 0.9221 - val_loss: 0.2552 - val_accuracy: 0.9232\n",
      "Epoch 173/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2568 - accuracy: 0.9221 - val_loss: 0.2549 - val_accuracy: 0.9232\n",
      "Epoch 174/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2565 - accuracy: 0.9221 - val_loss: 0.2546 - val_accuracy: 0.9232\n",
      "Epoch 175/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2562 - accuracy: 0.9221 - val_loss: 0.2542 - val_accuracy: 0.9232\n",
      "Epoch 176/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2559 - accuracy: 0.9221 - val_loss: 0.2539 - val_accuracy: 0.9232\n",
      "Epoch 177/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2556 - accuracy: 0.9221 - val_loss: 0.2536 - val_accuracy: 0.9232\n",
      "Epoch 178/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2552 - accuracy: 0.9221 - val_loss: 0.2532 - val_accuracy: 0.9232\n",
      "Epoch 179/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2549 - accuracy: 0.9221 - val_loss: 0.2528 - val_accuracy: 0.9232\n",
      "Epoch 180/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2545 - accuracy: 0.9221 - val_loss: 0.2525 - val_accuracy: 0.9232\n",
      "Epoch 181/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2542 - accuracy: 0.9221 - val_loss: 0.2521 - val_accuracy: 0.9232\n",
      "Epoch 182/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2538 - accuracy: 0.9221 - val_loss: 0.2517 - val_accuracy: 0.9232\n",
      "Epoch 183/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2534 - accuracy: 0.9221 - val_loss: 0.2514 - val_accuracy: 0.9232\n",
      "Epoch 184/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2530 - accuracy: 0.9221 - val_loss: 0.2510 - val_accuracy: 0.9232\n",
      "Epoch 185/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2526 - accuracy: 0.9221 - val_loss: 0.2506 - val_accuracy: 0.9232\n",
      "Epoch 186/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2522 - accuracy: 0.9221 - val_loss: 0.2501 - val_accuracy: 0.9232\n",
      "Epoch 187/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2519 - accuracy: 0.9221 - val_loss: 0.2498 - val_accuracy: 0.9232\n",
      "Epoch 188/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2515 - accuracy: 0.9221 - val_loss: 0.2494 - val_accuracy: 0.9232\n",
      "Epoch 189/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2511 - accuracy: 0.9221 - val_loss: 0.2490 - val_accuracy: 0.9232\n",
      "Epoch 190/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2507 - accuracy: 0.9221 - val_loss: 0.2486 - val_accuracy: 0.9232\n",
      "Epoch 191/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2504 - accuracy: 0.9221 - val_loss: 0.2483 - val_accuracy: 0.9232\n",
      "Epoch 192/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2500 - accuracy: 0.9221 - val_loss: 0.2479 - val_accuracy: 0.9232\n",
      "Epoch 193/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2496 - accuracy: 0.9221 - val_loss: 0.2475 - val_accuracy: 0.9232\n",
      "Epoch 194/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2493 - accuracy: 0.9221 - val_loss: 0.2471 - val_accuracy: 0.9232\n",
      "Epoch 195/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2489 - accuracy: 0.9221 - val_loss: 0.2467 - val_accuracy: 0.9232\n",
      "Epoch 196/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2486 - accuracy: 0.9221 - val_loss: 0.2464 - val_accuracy: 0.9232\n",
      "Epoch 197/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2482 - accuracy: 0.9221 - val_loss: 0.2460 - val_accuracy: 0.9232\n",
      "Epoch 198/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2479 - accuracy: 0.9221 - val_loss: 0.2457 - val_accuracy: 0.9232\n",
      "Epoch 199/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2475 - accuracy: 0.9221 - val_loss: 0.2453 - val_accuracy: 0.9232\n",
      "Epoch 200/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2472 - accuracy: 0.9221 - val_loss: 0.2450 - val_accuracy: 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 640 samples, validate on 161 samples\n",
      "Epoch 1/200\n",
      "640/640 [==============================] - 0s 206us/step - loss: 0.6807 - accuracy: 0.6577 - val_loss: 0.6679 - val_accuracy: 0.7666\n",
      "Epoch 2/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6554 - accuracy: 0.8098 - val_loss: 0.6408 - val_accuracy: 0.8728\n",
      "Epoch 3/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.6244 - accuracy: 0.8791 - val_loss: 0.6053 - val_accuracy: 0.8875\n",
      "Epoch 4/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5816 - accuracy: 0.9072 - val_loss: 0.5530 - val_accuracy: 0.9219\n",
      "Epoch 5/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5149 - accuracy: 0.9215 - val_loss: 0.4679 - val_accuracy: 0.9219\n",
      "Epoch 6/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.4137 - accuracy: 0.9218 - val_loss: 0.3564 - val_accuracy: 0.9222\n",
      "Epoch 7/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.3202 - accuracy: 0.9218 - val_loss: 0.2923 - val_accuracy: 0.9222\n",
      "Epoch 8/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2846 - accuracy: 0.9218 - val_loss: 0.2772 - val_accuracy: 0.9222\n",
      "Epoch 9/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2765 - accuracy: 0.9218 - val_loss: 0.2736 - val_accuracy: 0.9222\n",
      "Epoch 10/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2737 - accuracy: 0.9218 - val_loss: 0.2721 - val_accuracy: 0.9222\n",
      "Epoch 11/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2723 - accuracy: 0.9218 - val_loss: 0.2714 - val_accuracy: 0.9222\n",
      "Epoch 12/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2714 - accuracy: 0.9218 - val_loss: 0.2708 - val_accuracy: 0.9222\n",
      "Epoch 13/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2708 - accuracy: 0.9218 - val_loss: 0.2704 - val_accuracy: 0.9222\n",
      "Epoch 14/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2703 - accuracy: 0.9218 - val_loss: 0.2701 - val_accuracy: 0.9222\n",
      "Epoch 15/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2699 - accuracy: 0.9218 - val_loss: 0.2698 - val_accuracy: 0.9222\n",
      "Epoch 16/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2695 - accuracy: 0.9218 - val_loss: 0.2696 - val_accuracy: 0.9222\n",
      "Epoch 17/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2693 - accuracy: 0.9218 - val_loss: 0.2693 - val_accuracy: 0.9222\n",
      "Epoch 18/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2691 - accuracy: 0.9218 - val_loss: 0.2691 - val_accuracy: 0.9222\n",
      "Epoch 19/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2689 - accuracy: 0.9218 - val_loss: 0.2690 - val_accuracy: 0.9222\n",
      "Epoch 20/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2687 - accuracy: 0.9218 - val_loss: 0.2688 - val_accuracy: 0.9222\n",
      "Epoch 21/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2685 - accuracy: 0.9218 - val_loss: 0.2687 - val_accuracy: 0.9222\n",
      "Epoch 22/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2684 - accuracy: 0.9218 - val_loss: 0.2686 - val_accuracy: 0.9222\n",
      "Epoch 23/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2682 - accuracy: 0.9218 - val_loss: 0.2684 - val_accuracy: 0.9222\n",
      "Epoch 24/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2681 - accuracy: 0.9218 - val_loss: 0.2682 - val_accuracy: 0.9222\n",
      "Epoch 25/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2680 - accuracy: 0.9218 - val_loss: 0.2681 - val_accuracy: 0.9222\n",
      "Epoch 26/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2679 - accuracy: 0.9218 - val_loss: 0.2680 - val_accuracy: 0.9222\n",
      "Epoch 27/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2678 - accuracy: 0.9218 - val_loss: 0.2678 - val_accuracy: 0.9222\n",
      "Epoch 28/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2677 - accuracy: 0.9218 - val_loss: 0.2677 - val_accuracy: 0.9222\n",
      "Epoch 29/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2676 - accuracy: 0.9218 - val_loss: 0.2676 - val_accuracy: 0.9222\n",
      "Epoch 30/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2675 - accuracy: 0.9218 - val_loss: 0.2676 - val_accuracy: 0.9222\n",
      "Epoch 31/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2674 - accuracy: 0.9218 - val_loss: 0.2675 - val_accuracy: 0.9222\n",
      "Epoch 32/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2673 - accuracy: 0.9218 - val_loss: 0.2673 - val_accuracy: 0.9222\n",
      "Epoch 33/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2672 - accuracy: 0.9218 - val_loss: 0.2672 - val_accuracy: 0.9222\n",
      "Epoch 34/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2671 - accuracy: 0.9218 - val_loss: 0.2671 - val_accuracy: 0.9222\n",
      "Epoch 35/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2670 - accuracy: 0.9218 - val_loss: 0.2670 - val_accuracy: 0.9222\n",
      "Epoch 36/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2669 - accuracy: 0.9218 - val_loss: 0.2670 - val_accuracy: 0.9222\n",
      "Epoch 37/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2669 - accuracy: 0.9218 - val_loss: 0.2668 - val_accuracy: 0.9222\n",
      "Epoch 38/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2668 - accuracy: 0.9218 - val_loss: 0.2667 - val_accuracy: 0.9222\n",
      "Epoch 39/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2667 - accuracy: 0.9218 - val_loss: 0.2667 - val_accuracy: 0.9222\n",
      "Epoch 40/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2667 - accuracy: 0.9218 - val_loss: 0.2666 - val_accuracy: 0.9222\n",
      "Epoch 41/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2666 - accuracy: 0.9218 - val_loss: 0.2664 - val_accuracy: 0.9222\n",
      "Epoch 42/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2664 - accuracy: 0.9218 - val_loss: 0.2663 - val_accuracy: 0.9222\n",
      "Epoch 43/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2664 - accuracy: 0.9218 - val_loss: 0.2663 - val_accuracy: 0.9222\n",
      "Epoch 44/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2663 - accuracy: 0.9218 - val_loss: 0.2662 - val_accuracy: 0.9222\n",
      "Epoch 45/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2662 - accuracy: 0.9218 - val_loss: 0.2661 - val_accuracy: 0.9222\n",
      "Epoch 46/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2662 - accuracy: 0.9218 - val_loss: 0.2660 - val_accuracy: 0.9222\n",
      "Epoch 47/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2661 - accuracy: 0.9218 - val_loss: 0.2660 - val_accuracy: 0.9222\n",
      "Epoch 48/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2660 - accuracy: 0.9218 - val_loss: 0.2659 - val_accuracy: 0.9222\n",
      "Epoch 49/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2659 - accuracy: 0.9218 - val_loss: 0.2658 - val_accuracy: 0.9222\n",
      "Epoch 50/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2658 - accuracy: 0.9218 - val_loss: 0.2658 - val_accuracy: 0.9222\n",
      "Epoch 51/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2658 - accuracy: 0.9218 - val_loss: 0.2656 - val_accuracy: 0.9222\n",
      "Epoch 52/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2657 - accuracy: 0.9218 - val_loss: 0.2655 - val_accuracy: 0.9222\n",
      "Epoch 53/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2656 - accuracy: 0.9218 - val_loss: 0.2654 - val_accuracy: 0.9222\n",
      "Epoch 54/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2656 - accuracy: 0.9218 - val_loss: 0.2653 - val_accuracy: 0.9222\n",
      "Epoch 55/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2654 - accuracy: 0.9218 - val_loss: 0.2653 - val_accuracy: 0.9222\n",
      "Epoch 56/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2654 - accuracy: 0.9218 - val_loss: 0.2652 - val_accuracy: 0.9222\n",
      "Epoch 57/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2653 - accuracy: 0.9218 - val_loss: 0.2651 - val_accuracy: 0.9222\n",
      "Epoch 58/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2652 - accuracy: 0.9218 - val_loss: 0.2650 - val_accuracy: 0.9222\n",
      "Epoch 59/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2651 - accuracy: 0.9218 - val_loss: 0.2649 - val_accuracy: 0.9222\n",
      "Epoch 60/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2650 - accuracy: 0.9218 - val_loss: 0.2648 - val_accuracy: 0.9222\n",
      "Epoch 61/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2649 - accuracy: 0.9218 - val_loss: 0.2646 - val_accuracy: 0.9222\n",
      "Epoch 62/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2648 - accuracy: 0.9218 - val_loss: 0.2645 - val_accuracy: 0.9222\n",
      "Epoch 63/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2648 - accuracy: 0.9218 - val_loss: 0.2645 - val_accuracy: 0.9222\n",
      "Epoch 64/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2646 - accuracy: 0.9218 - val_loss: 0.2645 - val_accuracy: 0.9222\n",
      "Epoch 65/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2646 - accuracy: 0.9218 - val_loss: 0.2645 - val_accuracy: 0.9222\n",
      "Epoch 66/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2645 - accuracy: 0.9218 - val_loss: 0.2643 - val_accuracy: 0.9222\n",
      "Epoch 67/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2644 - accuracy: 0.9218 - val_loss: 0.2642 - val_accuracy: 0.9222\n",
      "Epoch 68/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2643 - accuracy: 0.9218 - val_loss: 0.2642 - val_accuracy: 0.9222\n",
      "Epoch 69/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2642 - accuracy: 0.9218 - val_loss: 0.2641 - val_accuracy: 0.9222\n",
      "Epoch 70/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2640 - accuracy: 0.9218 - val_loss: 0.2641 - val_accuracy: 0.9222\n",
      "Epoch 71/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2640 - accuracy: 0.9218 - val_loss: 0.2640 - val_accuracy: 0.9222\n",
      "Epoch 72/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2638 - accuracy: 0.9218 - val_loss: 0.2638 - val_accuracy: 0.9222\n",
      "Epoch 73/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2637 - accuracy: 0.9218 - val_loss: 0.2636 - val_accuracy: 0.9222\n",
      "Epoch 74/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2636 - accuracy: 0.9218 - val_loss: 0.2635 - val_accuracy: 0.9222\n",
      "Epoch 75/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2635 - accuracy: 0.9218 - val_loss: 0.2634 - val_accuracy: 0.9222\n",
      "Epoch 76/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2633 - accuracy: 0.9218 - val_loss: 0.2632 - val_accuracy: 0.9222\n",
      "Epoch 77/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2632 - accuracy: 0.9218 - val_loss: 0.2631 - val_accuracy: 0.9222\n",
      "Epoch 78/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2631 - accuracy: 0.9218 - val_loss: 0.2630 - val_accuracy: 0.9222\n",
      "Epoch 79/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2630 - accuracy: 0.9218 - val_loss: 0.2628 - val_accuracy: 0.9222\n",
      "Epoch 80/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2628 - accuracy: 0.9218 - val_loss: 0.2626 - val_accuracy: 0.9222\n",
      "Epoch 81/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2627 - accuracy: 0.9218 - val_loss: 0.2625 - val_accuracy: 0.9222\n",
      "Epoch 82/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2626 - accuracy: 0.9218 - val_loss: 0.2625 - val_accuracy: 0.9222\n",
      "Epoch 83/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2623 - accuracy: 0.9218 - val_loss: 0.2623 - val_accuracy: 0.9222\n",
      "Epoch 84/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2622 - accuracy: 0.9218 - val_loss: 0.2621 - val_accuracy: 0.9222\n",
      "Epoch 85/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2621 - accuracy: 0.9218 - val_loss: 0.2620 - val_accuracy: 0.9222\n",
      "Epoch 86/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2619 - accuracy: 0.9218 - val_loss: 0.2618 - val_accuracy: 0.9222\n",
      "Epoch 87/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2617 - accuracy: 0.9218 - val_loss: 0.2617 - val_accuracy: 0.9222\n",
      "Epoch 88/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2616 - accuracy: 0.9218 - val_loss: 0.2615 - val_accuracy: 0.9222\n",
      "Epoch 89/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2614 - accuracy: 0.9218 - val_loss: 0.2613 - val_accuracy: 0.9222\n",
      "Epoch 90/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2612 - accuracy: 0.9218 - val_loss: 0.2611 - val_accuracy: 0.9222\n",
      "Epoch 91/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2610 - accuracy: 0.9218 - val_loss: 0.2610 - val_accuracy: 0.9222\n",
      "Epoch 92/200\n",
      "640/640 [==============================] - 0s 45us/step - loss: 0.2608 - accuracy: 0.9218 - val_loss: 0.2608 - val_accuracy: 0.9222\n",
      "Epoch 93/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2606 - accuracy: 0.9218 - val_loss: 0.2606 - val_accuracy: 0.9222\n",
      "Epoch 94/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2605 - accuracy: 0.9218 - val_loss: 0.2604 - val_accuracy: 0.9222\n",
      "Epoch 95/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2601 - accuracy: 0.9218 - val_loss: 0.2602 - val_accuracy: 0.9222\n",
      "Epoch 96/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2599 - accuracy: 0.9218 - val_loss: 0.2600 - val_accuracy: 0.9222\n",
      "Epoch 97/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2597 - accuracy: 0.9218 - val_loss: 0.2598 - val_accuracy: 0.9222\n",
      "Epoch 98/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2595 - accuracy: 0.9218 - val_loss: 0.2596 - val_accuracy: 0.9222\n",
      "Epoch 99/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2593 - accuracy: 0.9218 - val_loss: 0.2593 - val_accuracy: 0.9222\n",
      "Epoch 100/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2590 - accuracy: 0.9218 - val_loss: 0.2591 - val_accuracy: 0.9222\n",
      "Epoch 101/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2587 - accuracy: 0.9218 - val_loss: 0.2588 - val_accuracy: 0.9222\n",
      "Epoch 102/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2585 - accuracy: 0.9218 - val_loss: 0.2586 - val_accuracy: 0.9222\n",
      "Epoch 103/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2581 - accuracy: 0.9218 - val_loss: 0.2583 - val_accuracy: 0.9222\n",
      "Epoch 104/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2579 - accuracy: 0.9218 - val_loss: 0.2581 - val_accuracy: 0.9222\n",
      "Epoch 105/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2575 - accuracy: 0.9218 - val_loss: 0.2579 - val_accuracy: 0.9222\n",
      "Epoch 106/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2572 - accuracy: 0.9218 - val_loss: 0.2574 - val_accuracy: 0.9222\n",
      "Epoch 107/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2569 - accuracy: 0.9218 - val_loss: 0.2570 - val_accuracy: 0.9222\n",
      "Epoch 108/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2565 - accuracy: 0.9218 - val_loss: 0.2566 - val_accuracy: 0.9222\n",
      "Epoch 109/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2562 - accuracy: 0.9218 - val_loss: 0.2563 - val_accuracy: 0.9222\n",
      "Epoch 110/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2558 - accuracy: 0.9218 - val_loss: 0.2560 - val_accuracy: 0.9222\n",
      "Epoch 111/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2555 - accuracy: 0.9218 - val_loss: 0.2557 - val_accuracy: 0.9222\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 0s 41us/step - loss: 0.2551 - accuracy: 0.9218 - val_loss: 0.2553 - val_accuracy: 0.9222\n",
      "Epoch 113/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2547 - accuracy: 0.9218 - val_loss: 0.2551 - val_accuracy: 0.9222\n",
      "Epoch 114/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2544 - accuracy: 0.9218 - val_loss: 0.2547 - val_accuracy: 0.9222\n",
      "Epoch 115/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2540 - accuracy: 0.9218 - val_loss: 0.2544 - val_accuracy: 0.9222\n",
      "Epoch 116/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2536 - accuracy: 0.9218 - val_loss: 0.2540 - val_accuracy: 0.9222\n",
      "Epoch 117/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2532 - accuracy: 0.9218 - val_loss: 0.2537 - val_accuracy: 0.9222\n",
      "Epoch 118/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2527 - accuracy: 0.9218 - val_loss: 0.2533 - val_accuracy: 0.9222\n",
      "Epoch 119/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2524 - accuracy: 0.9218 - val_loss: 0.2529 - val_accuracy: 0.9222\n",
      "Epoch 120/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2520 - accuracy: 0.9218 - val_loss: 0.2526 - val_accuracy: 0.9222\n",
      "Epoch 121/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2515 - accuracy: 0.9218 - val_loss: 0.2523 - val_accuracy: 0.9222\n",
      "Epoch 122/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2512 - accuracy: 0.9218 - val_loss: 0.2519 - val_accuracy: 0.9222\n",
      "Epoch 123/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2507 - accuracy: 0.9218 - val_loss: 0.2515 - val_accuracy: 0.9222\n",
      "Epoch 124/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2504 - accuracy: 0.9218 - val_loss: 0.2512 - val_accuracy: 0.9222\n",
      "Epoch 125/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2499 - accuracy: 0.9218 - val_loss: 0.2508 - val_accuracy: 0.9222\n",
      "Epoch 126/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2496 - accuracy: 0.9218 - val_loss: 0.2504 - val_accuracy: 0.9222\n",
      "Epoch 127/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2492 - accuracy: 0.9218 - val_loss: 0.2500 - val_accuracy: 0.9222\n",
      "Epoch 128/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2489 - accuracy: 0.9218 - val_loss: 0.2496 - val_accuracy: 0.9222\n",
      "Epoch 129/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2484 - accuracy: 0.9218 - val_loss: 0.2494 - val_accuracy: 0.9222\n",
      "Epoch 130/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2481 - accuracy: 0.9218 - val_loss: 0.2490 - val_accuracy: 0.9222\n",
      "Epoch 131/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2477 - accuracy: 0.9218 - val_loss: 0.2487 - val_accuracy: 0.9222\n",
      "Epoch 132/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2473 - accuracy: 0.9218 - val_loss: 0.2484 - val_accuracy: 0.9222\n",
      "Epoch 133/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2469 - accuracy: 0.9218 - val_loss: 0.2481 - val_accuracy: 0.9222\n",
      "Epoch 134/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2466 - accuracy: 0.9218 - val_loss: 0.2477 - val_accuracy: 0.9222\n",
      "Epoch 135/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2463 - accuracy: 0.9218 - val_loss: 0.2476 - val_accuracy: 0.9222\n",
      "Epoch 136/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2460 - accuracy: 0.9218 - val_loss: 0.2473 - val_accuracy: 0.9222\n",
      "Epoch 137/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2456 - accuracy: 0.9218 - val_loss: 0.2470 - val_accuracy: 0.9222\n",
      "Epoch 138/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2453 - accuracy: 0.9218 - val_loss: 0.2469 - val_accuracy: 0.9222\n",
      "Epoch 139/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2450 - accuracy: 0.9218 - val_loss: 0.2465 - val_accuracy: 0.9222\n",
      "Epoch 140/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2446 - accuracy: 0.9218 - val_loss: 0.2463 - val_accuracy: 0.9222\n",
      "Epoch 141/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2443 - accuracy: 0.9218 - val_loss: 0.2459 - val_accuracy: 0.9222\n",
      "Epoch 142/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2441 - accuracy: 0.9218 - val_loss: 0.2457 - val_accuracy: 0.9222\n",
      "Epoch 143/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2437 - accuracy: 0.9218 - val_loss: 0.2454 - val_accuracy: 0.9222\n",
      "Epoch 144/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2435 - accuracy: 0.9218 - val_loss: 0.2452 - val_accuracy: 0.9222\n",
      "Epoch 145/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2433 - accuracy: 0.9218 - val_loss: 0.2451 - val_accuracy: 0.9222\n",
      "Epoch 146/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2430 - accuracy: 0.9218 - val_loss: 0.2449 - val_accuracy: 0.9222\n",
      "Epoch 147/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2426 - accuracy: 0.9218 - val_loss: 0.2447 - val_accuracy: 0.9222\n",
      "Epoch 148/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2424 - accuracy: 0.9218 - val_loss: 0.2446 - val_accuracy: 0.9222\n",
      "Epoch 149/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2421 - accuracy: 0.9218 - val_loss: 0.2443 - val_accuracy: 0.9222\n",
      "Epoch 150/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2418 - accuracy: 0.9218 - val_loss: 0.2439 - val_accuracy: 0.9222\n",
      "Epoch 151/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2416 - accuracy: 0.9218 - val_loss: 0.2436 - val_accuracy: 0.9222\n",
      "Epoch 152/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2413 - accuracy: 0.9218 - val_loss: 0.2434 - val_accuracy: 0.9222\n",
      "Epoch 153/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2411 - accuracy: 0.9218 - val_loss: 0.2435 - val_accuracy: 0.9222\n",
      "Epoch 154/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2408 - accuracy: 0.9218 - val_loss: 0.2434 - val_accuracy: 0.9222\n",
      "Epoch 155/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2405 - accuracy: 0.9218 - val_loss: 0.2429 - val_accuracy: 0.9222\n",
      "Epoch 156/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2403 - accuracy: 0.9218 - val_loss: 0.2427 - val_accuracy: 0.9222\n",
      "Epoch 157/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2401 - accuracy: 0.9218 - val_loss: 0.2425 - val_accuracy: 0.9222\n",
      "Epoch 158/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2398 - accuracy: 0.9218 - val_loss: 0.2423 - val_accuracy: 0.9222\n",
      "Epoch 159/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2396 - accuracy: 0.9218 - val_loss: 0.2422 - val_accuracy: 0.9222\n",
      "Epoch 160/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2394 - accuracy: 0.9218 - val_loss: 0.2420 - val_accuracy: 0.9222\n",
      "Epoch 161/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2391 - accuracy: 0.9218 - val_loss: 0.2418 - val_accuracy: 0.9222\n",
      "Epoch 162/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2388 - accuracy: 0.9218 - val_loss: 0.2415 - val_accuracy: 0.9222\n",
      "Epoch 163/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2386 - accuracy: 0.9218 - val_loss: 0.2416 - val_accuracy: 0.9222\n",
      "Epoch 164/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2384 - accuracy: 0.9218 - val_loss: 0.2415 - val_accuracy: 0.9222\n",
      "Epoch 165/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2380 - accuracy: 0.9218 - val_loss: 0.2409 - val_accuracy: 0.9222\n",
      "Epoch 166/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2379 - accuracy: 0.9218 - val_loss: 0.2408 - val_accuracy: 0.9222\n",
      "Epoch 167/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2376 - accuracy: 0.9218 - val_loss: 0.2407 - val_accuracy: 0.9222\n",
      "Epoch 168/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2373 - accuracy: 0.9218 - val_loss: 0.2406 - val_accuracy: 0.9222\n",
      "Epoch 169/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2372 - accuracy: 0.9218 - val_loss: 0.2404 - val_accuracy: 0.9222\n",
      "Epoch 170/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2368 - accuracy: 0.9218 - val_loss: 0.2401 - val_accuracy: 0.9222\n",
      "Epoch 171/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2367 - accuracy: 0.9218 - val_loss: 0.2399 - val_accuracy: 0.9222\n",
      "Epoch 172/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2364 - accuracy: 0.9218 - val_loss: 0.2399 - val_accuracy: 0.9222\n",
      "Epoch 173/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2362 - accuracy: 0.9218 - val_loss: 0.2397 - val_accuracy: 0.9222\n",
      "Epoch 174/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2360 - accuracy: 0.9218 - val_loss: 0.2393 - val_accuracy: 0.9222\n",
      "Epoch 175/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2357 - accuracy: 0.9218 - val_loss: 0.2392 - val_accuracy: 0.9222\n",
      "Epoch 176/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2354 - accuracy: 0.9217 - val_loss: 0.2389 - val_accuracy: 0.9222\n",
      "Epoch 177/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2351 - accuracy: 0.9217 - val_loss: 0.2388 - val_accuracy: 0.9222\n",
      "Epoch 178/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2348 - accuracy: 0.9217 - val_loss: 0.2384 - val_accuracy: 0.9222\n",
      "Epoch 179/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2345 - accuracy: 0.9218 - val_loss: 0.2385 - val_accuracy: 0.9222\n",
      "Epoch 180/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2343 - accuracy: 0.9218 - val_loss: 0.2380 - val_accuracy: 0.9222\n",
      "Epoch 181/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2340 - accuracy: 0.9219 - val_loss: 0.2378 - val_accuracy: 0.9222\n",
      "Epoch 182/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2337 - accuracy: 0.9216 - val_loss: 0.2377 - val_accuracy: 0.9222\n",
      "Epoch 183/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2335 - accuracy: 0.9218 - val_loss: 0.2375 - val_accuracy: 0.9222\n",
      "Epoch 184/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2332 - accuracy: 0.9215 - val_loss: 0.2371 - val_accuracy: 0.9222\n",
      "Epoch 185/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2329 - accuracy: 0.9215 - val_loss: 0.2370 - val_accuracy: 0.9222\n",
      "Epoch 186/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2326 - accuracy: 0.9216 - val_loss: 0.2368 - val_accuracy: 0.9222\n",
      "Epoch 187/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2324 - accuracy: 0.9215 - val_loss: 0.2367 - val_accuracy: 0.9222\n",
      "Epoch 188/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2321 - accuracy: 0.9216 - val_loss: 0.2364 - val_accuracy: 0.9222\n",
      "Epoch 189/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2318 - accuracy: 0.9218 - val_loss: 0.2360 - val_accuracy: 0.9222\n",
      "Epoch 190/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2315 - accuracy: 0.9216 - val_loss: 0.2357 - val_accuracy: 0.9222\n",
      "Epoch 191/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2312 - accuracy: 0.9215 - val_loss: 0.2357 - val_accuracy: 0.9222\n",
      "Epoch 192/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2309 - accuracy: 0.9215 - val_loss: 0.2352 - val_accuracy: 0.9222\n",
      "Epoch 193/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2306 - accuracy: 0.9219 - val_loss: 0.2351 - val_accuracy: 0.9222\n",
      "Epoch 194/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2302 - accuracy: 0.9221 - val_loss: 0.2348 - val_accuracy: 0.9222\n",
      "Epoch 195/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2299 - accuracy: 0.9218 - val_loss: 0.2346 - val_accuracy: 0.9222\n",
      "Epoch 196/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2297 - accuracy: 0.9219 - val_loss: 0.2343 - val_accuracy: 0.9222\n",
      "Epoch 197/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2294 - accuracy: 0.9220 - val_loss: 0.2336 - val_accuracy: 0.9242\n",
      "Epoch 198/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2290 - accuracy: 0.9227 - val_loss: 0.2338 - val_accuracy: 0.9242\n",
      "Epoch 199/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2287 - accuracy: 0.9240 - val_loss: 0.2335 - val_accuracy: 0.9242\n",
      "Epoch 200/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2283 - accuracy: 0.9226 - val_loss: 0.2332 - val_accuracy: 0.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Train on 640 samples, validate on 161 samples\n",
      "Epoch 1/200\n",
      "640/640 [==============================] - 0s 222us/step - loss: 0.6855 - accuracy: 0.5802 - val_loss: 0.6703 - val_accuracy: 0.6832\n",
      "Epoch 2/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6543 - accuracy: 0.7245 - val_loss: 0.6398 - val_accuracy: 0.7532\n",
      "Epoch 3/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6212 - accuracy: 0.7718 - val_loss: 0.6023 - val_accuracy: 0.7800\n",
      "Epoch 4/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.5748 - accuracy: 0.7919 - val_loss: 0.5444 - val_accuracy: 0.8202\n",
      "Epoch 5/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.5000 - accuracy: 0.8544 - val_loss: 0.4510 - val_accuracy: 0.8771\n",
      "Epoch 6/200\n",
      "640/640 [==============================] - 0s 42us/step - loss: 0.3962 - accuracy: 0.9038 - val_loss: 0.3474 - val_accuracy: 0.9229\n",
      "Epoch 7/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.3148 - accuracy: 0.9227 - val_loss: 0.2959 - val_accuracy: 0.9229\n",
      "Epoch 8/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2852 - accuracy: 0.9227 - val_loss: 0.2816 - val_accuracy: 0.9229\n",
      "Epoch 9/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2768 - accuracy: 0.9227 - val_loss: 0.2760 - val_accuracy: 0.9229\n",
      "Epoch 10/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2730 - accuracy: 0.9227 - val_loss: 0.2729 - val_accuracy: 0.9229\n",
      "Epoch 11/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2708 - accuracy: 0.9227 - val_loss: 0.2709 - val_accuracy: 0.9229\n",
      "Epoch 12/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2694 - accuracy: 0.9227 - val_loss: 0.2694 - val_accuracy: 0.9229\n",
      "Epoch 13/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2684 - accuracy: 0.9227 - val_loss: 0.2684 - val_accuracy: 0.9229\n",
      "Epoch 14/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2677 - accuracy: 0.9227 - val_loss: 0.2676 - val_accuracy: 0.9229\n",
      "Epoch 15/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2672 - accuracy: 0.9227 - val_loss: 0.2670 - val_accuracy: 0.9229\n",
      "Epoch 16/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2667 - accuracy: 0.9227 - val_loss: 0.2666 - val_accuracy: 0.9229\n",
      "Epoch 17/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2664 - accuracy: 0.9227 - val_loss: 0.2662 - val_accuracy: 0.9229\n",
      "Epoch 18/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2662 - accuracy: 0.9227 - val_loss: 0.2659 - val_accuracy: 0.9229\n",
      "Epoch 19/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2660 - accuracy: 0.9227 - val_loss: 0.2656 - val_accuracy: 0.9229\n",
      "Epoch 20/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2658 - accuracy: 0.9227 - val_loss: 0.2655 - val_accuracy: 0.9229\n",
      "Epoch 21/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2656 - accuracy: 0.9227 - val_loss: 0.2652 - val_accuracy: 0.9229\n",
      "Epoch 22/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2654 - accuracy: 0.9227 - val_loss: 0.2651 - val_accuracy: 0.9229\n",
      "Epoch 23/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2653 - accuracy: 0.9227 - val_loss: 0.2649 - val_accuracy: 0.9229\n",
      "Epoch 24/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2651 - accuracy: 0.9227 - val_loss: 0.2648 - val_accuracy: 0.9229\n",
      "Epoch 25/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2650 - accuracy: 0.9227 - val_loss: 0.2647 - val_accuracy: 0.9229\n",
      "Epoch 26/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2649 - accuracy: 0.9227 - val_loss: 0.2646 - val_accuracy: 0.9229\n",
      "Epoch 27/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2648 - accuracy: 0.9227 - val_loss: 0.2645 - val_accuracy: 0.9229\n",
      "Epoch 28/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2647 - accuracy: 0.9227 - val_loss: 0.2644 - val_accuracy: 0.9229\n",
      "Epoch 29/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2646 - accuracy: 0.9227 - val_loss: 0.2643 - val_accuracy: 0.9229\n",
      "Epoch 30/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2645 - accuracy: 0.9227 - val_loss: 0.2642 - val_accuracy: 0.9229\n",
      "Epoch 31/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2644 - accuracy: 0.9227 - val_loss: 0.2641 - val_accuracy: 0.9229\n",
      "Epoch 32/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2643 - accuracy: 0.9227 - val_loss: 0.2641 - val_accuracy: 0.9229\n",
      "Epoch 33/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2642 - accuracy: 0.9227 - val_loss: 0.2640 - val_accuracy: 0.9229\n",
      "Epoch 34/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2641 - accuracy: 0.9227 - val_loss: 0.2638 - val_accuracy: 0.9229\n",
      "Epoch 35/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2640 - accuracy: 0.9227 - val_loss: 0.2637 - val_accuracy: 0.9229\n",
      "Epoch 36/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2639 - accuracy: 0.9227 - val_loss: 0.2637 - val_accuracy: 0.9229\n",
      "Epoch 37/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2639 - accuracy: 0.9227 - val_loss: 0.2637 - val_accuracy: 0.9229\n",
      "Epoch 38/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2638 - accuracy: 0.9227 - val_loss: 0.2635 - val_accuracy: 0.9229\n",
      "Epoch 39/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2637 - accuracy: 0.9227 - val_loss: 0.2635 - val_accuracy: 0.9229\n",
      "Epoch 40/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2636 - accuracy: 0.9227 - val_loss: 0.2634 - val_accuracy: 0.9229\n",
      "Epoch 41/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2635 - accuracy: 0.9227 - val_loss: 0.2633 - val_accuracy: 0.9229\n",
      "Epoch 42/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2634 - accuracy: 0.9227 - val_loss: 0.2632 - val_accuracy: 0.9229\n",
      "Epoch 43/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2633 - accuracy: 0.9227 - val_loss: 0.2632 - val_accuracy: 0.9229\n",
      "Epoch 44/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2632 - accuracy: 0.9227 - val_loss: 0.2631 - val_accuracy: 0.9229\n",
      "Epoch 45/200\n",
      "640/640 [==============================] - 0s 42us/step - loss: 0.2631 - accuracy: 0.9227 - val_loss: 0.2630 - val_accuracy: 0.9229\n",
      "Epoch 46/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2631 - accuracy: 0.9227 - val_loss: 0.2629 - val_accuracy: 0.9229\n",
      "Epoch 47/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2630 - accuracy: 0.9227 - val_loss: 0.2627 - val_accuracy: 0.9229\n",
      "Epoch 48/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2629 - accuracy: 0.9227 - val_loss: 0.2628 - val_accuracy: 0.9229\n",
      "Epoch 49/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2628 - accuracy: 0.9227 - val_loss: 0.2627 - val_accuracy: 0.9229\n",
      "Epoch 50/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2627 - accuracy: 0.9227 - val_loss: 0.2626 - val_accuracy: 0.9229\n",
      "Epoch 51/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2626 - accuracy: 0.9227 - val_loss: 0.2625 - val_accuracy: 0.9229\n",
      "Epoch 52/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2625 - accuracy: 0.9227 - val_loss: 0.2624 - val_accuracy: 0.9229\n",
      "Epoch 53/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2624 - accuracy: 0.9227 - val_loss: 0.2623 - val_accuracy: 0.9229\n",
      "Epoch 54/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2623 - accuracy: 0.9227 - val_loss: 0.2622 - val_accuracy: 0.9229\n",
      "Epoch 55/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2622 - accuracy: 0.9227 - val_loss: 0.2622 - val_accuracy: 0.9229\n",
      "Epoch 56/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2621 - accuracy: 0.9227 - val_loss: 0.2622 - val_accuracy: 0.9229\n",
      "Epoch 57/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2620 - accuracy: 0.9227 - val_loss: 0.2621 - val_accuracy: 0.9229\n",
      "Epoch 58/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2619 - accuracy: 0.9227 - val_loss: 0.2620 - val_accuracy: 0.9229\n",
      "Epoch 59/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2617 - accuracy: 0.9227 - val_loss: 0.2619 - val_accuracy: 0.9229\n",
      "Epoch 60/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2617 - accuracy: 0.9227 - val_loss: 0.2618 - val_accuracy: 0.9229\n",
      "Epoch 61/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2615 - accuracy: 0.9227 - val_loss: 0.2617 - val_accuracy: 0.9229\n",
      "Epoch 62/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2613 - accuracy: 0.9227 - val_loss: 0.2616 - val_accuracy: 0.9229\n",
      "Epoch 63/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2612 - accuracy: 0.9227 - val_loss: 0.2615 - val_accuracy: 0.9229\n",
      "Epoch 64/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2611 - accuracy: 0.9227 - val_loss: 0.2613 - val_accuracy: 0.9229\n",
      "Epoch 65/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2610 - accuracy: 0.9227 - val_loss: 0.2612 - val_accuracy: 0.9229\n",
      "Epoch 66/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2608 - accuracy: 0.9227 - val_loss: 0.2611 - val_accuracy: 0.9229\n",
      "Epoch 67/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2607 - accuracy: 0.9227 - val_loss: 0.2610 - val_accuracy: 0.9229\n",
      "Epoch 68/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2605 - accuracy: 0.9227 - val_loss: 0.2609 - val_accuracy: 0.9229\n",
      "Epoch 69/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2604 - accuracy: 0.9227 - val_loss: 0.2606 - val_accuracy: 0.9229\n",
      "Epoch 70/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2602 - accuracy: 0.9227 - val_loss: 0.2605 - val_accuracy: 0.9229\n",
      "Epoch 71/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2600 - accuracy: 0.9227 - val_loss: 0.2604 - val_accuracy: 0.9229\n",
      "Epoch 72/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2599 - accuracy: 0.9227 - val_loss: 0.2602 - val_accuracy: 0.9229\n",
      "Epoch 73/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2597 - accuracy: 0.9227 - val_loss: 0.2601 - val_accuracy: 0.9229\n",
      "Epoch 74/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2595 - accuracy: 0.9227 - val_loss: 0.2600 - val_accuracy: 0.9229\n",
      "Epoch 75/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2593 - accuracy: 0.9227 - val_loss: 0.2599 - val_accuracy: 0.9229\n",
      "Epoch 76/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2591 - accuracy: 0.9227 - val_loss: 0.2597 - val_accuracy: 0.9229\n",
      "Epoch 77/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2589 - accuracy: 0.9227 - val_loss: 0.2596 - val_accuracy: 0.9229\n",
      "Epoch 78/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2586 - accuracy: 0.9227 - val_loss: 0.2593 - val_accuracy: 0.9229\n",
      "Epoch 79/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2584 - accuracy: 0.9227 - val_loss: 0.2591 - val_accuracy: 0.9229\n",
      "Epoch 80/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2582 - accuracy: 0.9227 - val_loss: 0.2590 - val_accuracy: 0.9229\n",
      "Epoch 81/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2580 - accuracy: 0.9227 - val_loss: 0.2588 - val_accuracy: 0.9229\n",
      "Epoch 82/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2577 - accuracy: 0.9227 - val_loss: 0.2586 - val_accuracy: 0.9229\n",
      "Epoch 83/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2575 - accuracy: 0.9227 - val_loss: 0.2584 - val_accuracy: 0.9229\n",
      "Epoch 84/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2572 - accuracy: 0.9227 - val_loss: 0.2581 - val_accuracy: 0.9229\n",
      "Epoch 85/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2569 - accuracy: 0.9227 - val_loss: 0.2579 - val_accuracy: 0.9229\n",
      "Epoch 86/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2566 - accuracy: 0.9227 - val_loss: 0.2577 - val_accuracy: 0.9229\n",
      "Epoch 87/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2563 - accuracy: 0.9227 - val_loss: 0.2575 - val_accuracy: 0.9229\n",
      "Epoch 88/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2560 - accuracy: 0.9227 - val_loss: 0.2572 - val_accuracy: 0.9229\n",
      "Epoch 89/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2557 - accuracy: 0.9227 - val_loss: 0.2570 - val_accuracy: 0.9229\n",
      "Epoch 90/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2554 - accuracy: 0.9227 - val_loss: 0.2566 - val_accuracy: 0.9229\n",
      "Epoch 91/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2551 - accuracy: 0.9227 - val_loss: 0.2563 - val_accuracy: 0.9229\n",
      "Epoch 92/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2546 - accuracy: 0.9227 - val_loss: 0.2561 - val_accuracy: 0.9229\n",
      "Epoch 93/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2543 - accuracy: 0.9227 - val_loss: 0.2558 - val_accuracy: 0.9229\n",
      "Epoch 94/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2539 - accuracy: 0.9227 - val_loss: 0.2554 - val_accuracy: 0.9229\n",
      "Epoch 95/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2535 - accuracy: 0.9227 - val_loss: 0.2552 - val_accuracy: 0.9229\n",
      "Epoch 96/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2532 - accuracy: 0.9227 - val_loss: 0.2549 - val_accuracy: 0.9229\n",
      "Epoch 97/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2527 - accuracy: 0.9227 - val_loss: 0.2546 - val_accuracy: 0.9229\n",
      "Epoch 98/200\n",
      "640/640 [==============================] - 0s 37us/step - loss: 0.2524 - accuracy: 0.9227 - val_loss: 0.2542 - val_accuracy: 0.9229\n",
      "Epoch 99/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2519 - accuracy: 0.9227 - val_loss: 0.2538 - val_accuracy: 0.9229\n",
      "Epoch 100/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2515 - accuracy: 0.9227 - val_loss: 0.2535 - val_accuracy: 0.9229\n",
      "Epoch 101/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2511 - accuracy: 0.9227 - val_loss: 0.2532 - val_accuracy: 0.9229\n",
      "Epoch 102/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2506 - accuracy: 0.9227 - val_loss: 0.2528 - val_accuracy: 0.9229\n",
      "Epoch 103/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2501 - accuracy: 0.9227 - val_loss: 0.2524 - val_accuracy: 0.9229\n",
      "Epoch 104/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2497 - accuracy: 0.9227 - val_loss: 0.2519 - val_accuracy: 0.9229\n",
      "Epoch 105/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2493 - accuracy: 0.9227 - val_loss: 0.2515 - val_accuracy: 0.9229\n",
      "Epoch 106/200\n",
      "640/640 [==============================] - 0s 35us/step - loss: 0.2488 - accuracy: 0.9227 - val_loss: 0.2512 - val_accuracy: 0.9229\n",
      "Epoch 107/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2484 - accuracy: 0.9227 - val_loss: 0.2508 - val_accuracy: 0.9229\n",
      "Epoch 108/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2479 - accuracy: 0.9227 - val_loss: 0.2504 - val_accuracy: 0.9229\n",
      "Epoch 109/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2475 - accuracy: 0.9227 - val_loss: 0.2500 - val_accuracy: 0.9229\n",
      "Epoch 110/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2471 - accuracy: 0.9227 - val_loss: 0.2496 - val_accuracy: 0.9229\n",
      "Epoch 111/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2466 - accuracy: 0.9227 - val_loss: 0.2493 - val_accuracy: 0.9229\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 0s 36us/step - loss: 0.2462 - accuracy: 0.9227 - val_loss: 0.2490 - val_accuracy: 0.9229\n",
      "Epoch 113/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2458 - accuracy: 0.9227 - val_loss: 0.2485 - val_accuracy: 0.9229\n",
      "Epoch 114/200\n",
      "640/640 [==============================] - 0s 35us/step - loss: 0.2453 - accuracy: 0.9227 - val_loss: 0.2482 - val_accuracy: 0.9229\n",
      "Epoch 115/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2449 - accuracy: 0.9227 - val_loss: 0.2479 - val_accuracy: 0.9229\n",
      "Epoch 116/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2445 - accuracy: 0.9227 - val_loss: 0.2475 - val_accuracy: 0.9229\n",
      "Epoch 117/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2441 - accuracy: 0.9227 - val_loss: 0.2470 - val_accuracy: 0.9229\n",
      "Epoch 118/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2438 - accuracy: 0.9227 - val_loss: 0.2468 - val_accuracy: 0.9229\n",
      "Epoch 119/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2434 - accuracy: 0.9227 - val_loss: 0.2465 - val_accuracy: 0.9229\n",
      "Epoch 120/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2430 - accuracy: 0.9227 - val_loss: 0.2463 - val_accuracy: 0.9229\n",
      "Epoch 121/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2427 - accuracy: 0.9227 - val_loss: 0.2460 - val_accuracy: 0.9229\n",
      "Epoch 122/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2424 - accuracy: 0.9227 - val_loss: 0.2456 - val_accuracy: 0.9229\n",
      "Epoch 123/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2421 - accuracy: 0.9229 - val_loss: 0.2453 - val_accuracy: 0.9229\n",
      "Epoch 124/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2418 - accuracy: 0.9227 - val_loss: 0.2451 - val_accuracy: 0.9229\n",
      "Epoch 125/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2415 - accuracy: 0.9230 - val_loss: 0.2448 - val_accuracy: 0.9229\n",
      "Epoch 126/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2413 - accuracy: 0.9230 - val_loss: 0.2446 - val_accuracy: 0.9229\n",
      "Epoch 127/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2410 - accuracy: 0.9231 - val_loss: 0.2443 - val_accuracy: 0.9229\n",
      "Epoch 128/200\n",
      "640/640 [==============================] - 0s 44us/step - loss: 0.2407 - accuracy: 0.9230 - val_loss: 0.2441 - val_accuracy: 0.9229\n",
      "Epoch 129/200\n",
      "640/640 [==============================] - 0s 42us/step - loss: 0.2404 - accuracy: 0.9231 - val_loss: 0.2439 - val_accuracy: 0.9225\n",
      "Epoch 130/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2402 - accuracy: 0.9233 - val_loss: 0.2435 - val_accuracy: 0.9225\n",
      "Epoch 131/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2399 - accuracy: 0.9243 - val_loss: 0.2432 - val_accuracy: 0.9225\n",
      "Epoch 132/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2396 - accuracy: 0.9245 - val_loss: 0.2430 - val_accuracy: 0.9225\n",
      "Epoch 133/200\n",
      "640/640 [==============================] - 0s 44us/step - loss: 0.2393 - accuracy: 0.9238 - val_loss: 0.2429 - val_accuracy: 0.9225\n",
      "Epoch 134/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2391 - accuracy: 0.9254 - val_loss: 0.2427 - val_accuracy: 0.9225\n",
      "Epoch 135/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2388 - accuracy: 0.9254 - val_loss: 0.2424 - val_accuracy: 0.9225\n",
      "Epoch 136/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2385 - accuracy: 0.9254 - val_loss: 0.2419 - val_accuracy: 0.9225\n",
      "Epoch 137/200\n",
      "640/640 [==============================] - 0s 42us/step - loss: 0.2382 - accuracy: 0.9254 - val_loss: 0.2418 - val_accuracy: 0.9225\n",
      "Epoch 138/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2380 - accuracy: 0.9254 - val_loss: 0.2415 - val_accuracy: 0.9225\n",
      "Epoch 139/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2376 - accuracy: 0.9254 - val_loss: 0.2412 - val_accuracy: 0.9225\n",
      "Epoch 140/200\n",
      "640/640 [==============================] - 0s 44us/step - loss: 0.2374 - accuracy: 0.9254 - val_loss: 0.2408 - val_accuracy: 0.9225\n",
      "Epoch 141/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2371 - accuracy: 0.9254 - val_loss: 0.2407 - val_accuracy: 0.9225\n",
      "Epoch 142/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2368 - accuracy: 0.9254 - val_loss: 0.2405 - val_accuracy: 0.9225\n",
      "Epoch 143/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2365 - accuracy: 0.9254 - val_loss: 0.2402 - val_accuracy: 0.9225\n",
      "Epoch 144/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2363 - accuracy: 0.9254 - val_loss: 0.2402 - val_accuracy: 0.9225\n",
      "Epoch 145/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2360 - accuracy: 0.9254 - val_loss: 0.2397 - val_accuracy: 0.9225\n",
      "Epoch 146/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2358 - accuracy: 0.9254 - val_loss: 0.2395 - val_accuracy: 0.9225\n",
      "Epoch 147/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2355 - accuracy: 0.9254 - val_loss: 0.2392 - val_accuracy: 0.9225\n",
      "Epoch 148/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2352 - accuracy: 0.9254 - val_loss: 0.2391 - val_accuracy: 0.9225\n",
      "Epoch 149/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2349 - accuracy: 0.9254 - val_loss: 0.2388 - val_accuracy: 0.9225\n",
      "Epoch 150/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2346 - accuracy: 0.9254 - val_loss: 0.2386 - val_accuracy: 0.9225\n",
      "Epoch 151/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2343 - accuracy: 0.9254 - val_loss: 0.2382 - val_accuracy: 0.9225\n",
      "Epoch 152/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2340 - accuracy: 0.9254 - val_loss: 0.2380 - val_accuracy: 0.9225\n",
      "Epoch 153/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2338 - accuracy: 0.9254 - val_loss: 0.2379 - val_accuracy: 0.9225\n",
      "Epoch 154/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2334 - accuracy: 0.9254 - val_loss: 0.2374 - val_accuracy: 0.9225\n",
      "Epoch 155/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2331 - accuracy: 0.9254 - val_loss: 0.2372 - val_accuracy: 0.9225\n",
      "Epoch 156/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2327 - accuracy: 0.9255 - val_loss: 0.2369 - val_accuracy: 0.9225\n",
      "Epoch 157/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2324 - accuracy: 0.9254 - val_loss: 0.2366 - val_accuracy: 0.9225\n",
      "Epoch 158/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2320 - accuracy: 0.9254 - val_loss: 0.2364 - val_accuracy: 0.9225\n",
      "Epoch 159/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2317 - accuracy: 0.9254 - val_loss: 0.2361 - val_accuracy: 0.9225\n",
      "Epoch 160/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2313 - accuracy: 0.9254 - val_loss: 0.2358 - val_accuracy: 0.9225\n",
      "Epoch 161/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2311 - accuracy: 0.9254 - val_loss: 0.2353 - val_accuracy: 0.9225\n",
      "Epoch 162/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2307 - accuracy: 0.9254 - val_loss: 0.2350 - val_accuracy: 0.9225\n",
      "Epoch 163/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2304 - accuracy: 0.9254 - val_loss: 0.2347 - val_accuracy: 0.9225\n",
      "Epoch 164/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2299 - accuracy: 0.9254 - val_loss: 0.2344 - val_accuracy: 0.9225\n",
      "Epoch 165/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2296 - accuracy: 0.9254 - val_loss: 0.2341 - val_accuracy: 0.9225\n",
      "Epoch 166/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2292 - accuracy: 0.9254 - val_loss: 0.2338 - val_accuracy: 0.9225\n",
      "Epoch 167/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2288 - accuracy: 0.9254 - val_loss: 0.2336 - val_accuracy: 0.9225\n",
      "Epoch 168/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2284 - accuracy: 0.9254 - val_loss: 0.2332 - val_accuracy: 0.9225\n",
      "Epoch 169/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2280 - accuracy: 0.9254 - val_loss: 0.2328 - val_accuracy: 0.9225\n",
      "Epoch 170/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2276 - accuracy: 0.9254 - val_loss: 0.2324 - val_accuracy: 0.9225\n",
      "Epoch 171/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2272 - accuracy: 0.9255 - val_loss: 0.2320 - val_accuracy: 0.9225\n",
      "Epoch 172/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2268 - accuracy: 0.9253 - val_loss: 0.2317 - val_accuracy: 0.9225\n",
      "Epoch 173/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2263 - accuracy: 0.9254 - val_loss: 0.2312 - val_accuracy: 0.9225\n",
      "Epoch 174/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2260 - accuracy: 0.9253 - val_loss: 0.2309 - val_accuracy: 0.9225\n",
      "Epoch 175/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2256 - accuracy: 0.9254 - val_loss: 0.2307 - val_accuracy: 0.9225\n",
      "Epoch 176/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2252 - accuracy: 0.9254 - val_loss: 0.2305 - val_accuracy: 0.9225\n",
      "Epoch 177/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2248 - accuracy: 0.9253 - val_loss: 0.2301 - val_accuracy: 0.9225\n",
      "Epoch 178/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2243 - accuracy: 0.9254 - val_loss: 0.2297 - val_accuracy: 0.9225\n",
      "Epoch 179/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2240 - accuracy: 0.9254 - val_loss: 0.2293 - val_accuracy: 0.9225\n",
      "Epoch 180/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2235 - accuracy: 0.9254 - val_loss: 0.2289 - val_accuracy: 0.9225\n",
      "Epoch 181/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2232 - accuracy: 0.9254 - val_loss: 0.2286 - val_accuracy: 0.9225\n",
      "Epoch 182/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2227 - accuracy: 0.9255 - val_loss: 0.2284 - val_accuracy: 0.9225\n",
      "Epoch 183/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2222 - accuracy: 0.9254 - val_loss: 0.2279 - val_accuracy: 0.9225\n",
      "Epoch 184/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2218 - accuracy: 0.9255 - val_loss: 0.2275 - val_accuracy: 0.9225\n",
      "Epoch 185/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2215 - accuracy: 0.9253 - val_loss: 0.2271 - val_accuracy: 0.9225\n",
      "Epoch 186/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2211 - accuracy: 0.9255 - val_loss: 0.2267 - val_accuracy: 0.9225\n",
      "Epoch 187/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2206 - accuracy: 0.9255 - val_loss: 0.2264 - val_accuracy: 0.9225\n",
      "Epoch 188/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2203 - accuracy: 0.9255 - val_loss: 0.2261 - val_accuracy: 0.9225\n",
      "Epoch 189/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2199 - accuracy: 0.9255 - val_loss: 0.2257 - val_accuracy: 0.9225\n",
      "Epoch 190/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2194 - accuracy: 0.9255 - val_loss: 0.2254 - val_accuracy: 0.9225\n",
      "Epoch 191/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2192 - accuracy: 0.9256 - val_loss: 0.2252 - val_accuracy: 0.9225\n",
      "Epoch 192/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2188 - accuracy: 0.9255 - val_loss: 0.2248 - val_accuracy: 0.9225\n",
      "Epoch 193/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2184 - accuracy: 0.9257 - val_loss: 0.2245 - val_accuracy: 0.9225\n",
      "Epoch 194/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2182 - accuracy: 0.9257 - val_loss: 0.2243 - val_accuracy: 0.9225\n",
      "Epoch 195/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2176 - accuracy: 0.9257 - val_loss: 0.2240 - val_accuracy: 0.9232\n",
      "Epoch 196/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2174 - accuracy: 0.9258 - val_loss: 0.2236 - val_accuracy: 0.9235\n",
      "Epoch 197/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2171 - accuracy: 0.9260 - val_loss: 0.2236 - val_accuracy: 0.9232\n",
      "Epoch 198/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2167 - accuracy: 0.9259 - val_loss: 0.2234 - val_accuracy: 0.9232\n",
      "Epoch 199/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2163 - accuracy: 0.9258 - val_loss: 0.2230 - val_accuracy: 0.9232\n",
      "Epoch 200/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2160 - accuracy: 0.9258 - val_loss: 0.2226 - val_accuracy: 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Train on 640 samples, validate on 161 samples\n",
      "Epoch 1/200\n",
      "640/640 [==============================] - 0s 220us/step - loss: 0.6921 - accuracy: 0.5447 - val_loss: 0.6833 - val_accuracy: 0.7002\n",
      "Epoch 2/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.6768 - accuracy: 0.7774 - val_loss: 0.6700 - val_accuracy: 0.8284\n",
      "Epoch 3/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.6639 - accuracy: 0.8465 - val_loss: 0.6574 - val_accuracy: 0.8630\n",
      "Epoch 4/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.6511 - accuracy: 0.8638 - val_loss: 0.6447 - val_accuracy: 0.8683\n",
      "Epoch 5/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.6381 - accuracy: 0.8650 - val_loss: 0.6316 - val_accuracy: 0.8686\n",
      "Epoch 6/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6246 - accuracy: 0.8610 - val_loss: 0.6178 - val_accuracy: 0.8709\n",
      "Epoch 7/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.6102 - accuracy: 0.8681 - val_loss: 0.6029 - val_accuracy: 0.8741\n",
      "Epoch 8/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.5944 - accuracy: 0.8692 - val_loss: 0.5864 - val_accuracy: 0.8745\n",
      "Epoch 9/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.5764 - accuracy: 0.8706 - val_loss: 0.5671 - val_accuracy: 0.8761\n",
      "Epoch 10/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5551 - accuracy: 0.8732 - val_loss: 0.5436 - val_accuracy: 0.8807\n",
      "Epoch 11/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.5283 - accuracy: 0.8808 - val_loss: 0.5135 - val_accuracy: 0.8895\n",
      "Epoch 12/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.4936 - accuracy: 0.8952 - val_loss: 0.4738 - val_accuracy: 0.9094\n",
      "Epoch 13/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.4484 - accuracy: 0.9136 - val_loss: 0.4236 - val_accuracy: 0.9199\n",
      "Epoch 14/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.3950 - accuracy: 0.9217 - val_loss: 0.3691 - val_accuracy: 0.9219\n",
      "Epoch 15/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.3445 - accuracy: 0.9223 - val_loss: 0.3252 - val_accuracy: 0.9219\n",
      "Epoch 16/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.3101 - accuracy: 0.9223 - val_loss: 0.3004 - val_accuracy: 0.9219\n",
      "Epoch 17/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2928 - accuracy: 0.9223 - val_loss: 0.2890 - val_accuracy: 0.9219\n",
      "Epoch 18/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2845 - accuracy: 0.9223 - val_loss: 0.2830 - val_accuracy: 0.9219\n",
      "Epoch 19/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2798 - accuracy: 0.9223 - val_loss: 0.2793 - val_accuracy: 0.9219\n",
      "Epoch 20/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2767 - accuracy: 0.9223 - val_loss: 0.2768 - val_accuracy: 0.9219\n",
      "Epoch 21/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2745 - accuracy: 0.9223 - val_loss: 0.2750 - val_accuracy: 0.9219\n",
      "Epoch 22/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2728 - accuracy: 0.9223 - val_loss: 0.2737 - val_accuracy: 0.9219\n",
      "Epoch 23/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2716 - accuracy: 0.9223 - val_loss: 0.2727 - val_accuracy: 0.9219\n",
      "Epoch 24/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2706 - accuracy: 0.9223 - val_loss: 0.2719 - val_accuracy: 0.9219\n",
      "Epoch 25/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2699 - accuracy: 0.9223 - val_loss: 0.2713 - val_accuracy: 0.9219\n",
      "Epoch 26/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2693 - accuracy: 0.9223 - val_loss: 0.2708 - val_accuracy: 0.9219\n",
      "Epoch 27/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2688 - accuracy: 0.9223 - val_loss: 0.2704 - val_accuracy: 0.9219\n",
      "Epoch 28/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2684 - accuracy: 0.9223 - val_loss: 0.2701 - val_accuracy: 0.9219\n",
      "Epoch 29/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2681 - accuracy: 0.9223 - val_loss: 0.2698 - val_accuracy: 0.9219\n",
      "Epoch 30/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2679 - accuracy: 0.9223 - val_loss: 0.2696 - val_accuracy: 0.9219\n",
      "Epoch 31/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2676 - accuracy: 0.9223 - val_loss: 0.2693 - val_accuracy: 0.9219\n",
      "Epoch 32/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2674 - accuracy: 0.9223 - val_loss: 0.2692 - val_accuracy: 0.9219\n",
      "Epoch 33/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2672 - accuracy: 0.9223 - val_loss: 0.2690 - val_accuracy: 0.9219\n",
      "Epoch 34/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2670 - accuracy: 0.9223 - val_loss: 0.2688 - val_accuracy: 0.9219\n",
      "Epoch 35/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2668 - accuracy: 0.9223 - val_loss: 0.2687 - val_accuracy: 0.9219\n",
      "Epoch 36/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2667 - accuracy: 0.9223 - val_loss: 0.2685 - val_accuracy: 0.9219\n",
      "Epoch 37/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2666 - accuracy: 0.9223 - val_loss: 0.2684 - val_accuracy: 0.9219\n",
      "Epoch 38/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2665 - accuracy: 0.9223 - val_loss: 0.2683 - val_accuracy: 0.9219\n",
      "Epoch 39/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2664 - accuracy: 0.9223 - val_loss: 0.2682 - val_accuracy: 0.9219\n",
      "Epoch 40/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2662 - accuracy: 0.9223 - val_loss: 0.2680 - val_accuracy: 0.9219\n",
      "Epoch 41/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2661 - accuracy: 0.9223 - val_loss: 0.2679 - val_accuracy: 0.9219\n",
      "Epoch 42/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2660 - accuracy: 0.9223 - val_loss: 0.2678 - val_accuracy: 0.9219\n",
      "Epoch 43/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2659 - accuracy: 0.9223 - val_loss: 0.2676 - val_accuracy: 0.9219\n",
      "Epoch 44/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2659 - accuracy: 0.9223 - val_loss: 0.2675 - val_accuracy: 0.9219\n",
      "Epoch 45/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2658 - accuracy: 0.9223 - val_loss: 0.2674 - val_accuracy: 0.9219\n",
      "Epoch 46/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2656 - accuracy: 0.9223 - val_loss: 0.2673 - val_accuracy: 0.9219\n",
      "Epoch 47/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2655 - accuracy: 0.9223 - val_loss: 0.2672 - val_accuracy: 0.9219\n",
      "Epoch 48/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2654 - accuracy: 0.9223 - val_loss: 0.2671 - val_accuracy: 0.9219\n",
      "Epoch 49/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2653 - accuracy: 0.9223 - val_loss: 0.2670 - val_accuracy: 0.9219\n",
      "Epoch 50/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2653 - accuracy: 0.9223 - val_loss: 0.2669 - val_accuracy: 0.9219\n",
      "Epoch 51/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2651 - accuracy: 0.9223 - val_loss: 0.2668 - val_accuracy: 0.9219\n",
      "Epoch 52/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2650 - accuracy: 0.9223 - val_loss: 0.2667 - val_accuracy: 0.9219\n",
      "Epoch 53/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2649 - accuracy: 0.9223 - val_loss: 0.2666 - val_accuracy: 0.9219\n",
      "Epoch 54/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2649 - accuracy: 0.9223 - val_loss: 0.2665 - val_accuracy: 0.9219\n",
      "Epoch 55/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2647 - accuracy: 0.9223 - val_loss: 0.2665 - val_accuracy: 0.9219\n",
      "Epoch 56/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2647 - accuracy: 0.9223 - val_loss: 0.2663 - val_accuracy: 0.9219\n",
      "Epoch 57/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2646 - accuracy: 0.9223 - val_loss: 0.2662 - val_accuracy: 0.9219\n",
      "Epoch 58/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2645 - accuracy: 0.9223 - val_loss: 0.2661 - val_accuracy: 0.9219\n",
      "Epoch 59/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2644 - accuracy: 0.9223 - val_loss: 0.2660 - val_accuracy: 0.9219\n",
      "Epoch 60/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2643 - accuracy: 0.9223 - val_loss: 0.2658 - val_accuracy: 0.9219\n",
      "Epoch 61/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2642 - accuracy: 0.9223 - val_loss: 0.2656 - val_accuracy: 0.9219\n",
      "Epoch 62/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2641 - accuracy: 0.9223 - val_loss: 0.2655 - val_accuracy: 0.9219\n",
      "Epoch 63/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2639 - accuracy: 0.9223 - val_loss: 0.2653 - val_accuracy: 0.9219\n",
      "Epoch 64/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2638 - accuracy: 0.9223 - val_loss: 0.2653 - val_accuracy: 0.9219\n",
      "Epoch 65/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2637 - accuracy: 0.9223 - val_loss: 0.2652 - val_accuracy: 0.9219\n",
      "Epoch 66/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2636 - accuracy: 0.9223 - val_loss: 0.2651 - val_accuracy: 0.9219\n",
      "Epoch 67/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2635 - accuracy: 0.9223 - val_loss: 0.2650 - val_accuracy: 0.9219\n",
      "Epoch 68/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2634 - accuracy: 0.9223 - val_loss: 0.2649 - val_accuracy: 0.9219\n",
      "Epoch 69/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2633 - accuracy: 0.9223 - val_loss: 0.2648 - val_accuracy: 0.9219\n",
      "Epoch 70/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2632 - accuracy: 0.9223 - val_loss: 0.2647 - val_accuracy: 0.9219\n",
      "Epoch 71/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2630 - accuracy: 0.9223 - val_loss: 0.2645 - val_accuracy: 0.9219\n",
      "Epoch 72/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2629 - accuracy: 0.9223 - val_loss: 0.2644 - val_accuracy: 0.9219\n",
      "Epoch 73/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2628 - accuracy: 0.9223 - val_loss: 0.2642 - val_accuracy: 0.9219\n",
      "Epoch 74/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2627 - accuracy: 0.9223 - val_loss: 0.2642 - val_accuracy: 0.9219\n",
      "Epoch 75/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2625 - accuracy: 0.9223 - val_loss: 0.2640 - val_accuracy: 0.9219\n",
      "Epoch 76/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2624 - accuracy: 0.9223 - val_loss: 0.2639 - val_accuracy: 0.9219\n",
      "Epoch 77/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2623 - accuracy: 0.9223 - val_loss: 0.2637 - val_accuracy: 0.9219\n",
      "Epoch 78/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2621 - accuracy: 0.9223 - val_loss: 0.2635 - val_accuracy: 0.9219\n",
      "Epoch 79/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2620 - accuracy: 0.9223 - val_loss: 0.2633 - val_accuracy: 0.9219\n",
      "Epoch 80/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2618 - accuracy: 0.9223 - val_loss: 0.2632 - val_accuracy: 0.9219\n",
      "Epoch 81/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2617 - accuracy: 0.9223 - val_loss: 0.2631 - val_accuracy: 0.9219\n",
      "Epoch 82/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2615 - accuracy: 0.9223 - val_loss: 0.2629 - val_accuracy: 0.9219\n",
      "Epoch 83/200\n",
      "640/640 [==============================] - 0s 42us/step - loss: 0.2614 - accuracy: 0.9223 - val_loss: 0.2627 - val_accuracy: 0.9219\n",
      "Epoch 84/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2612 - accuracy: 0.9223 - val_loss: 0.2625 - val_accuracy: 0.9219\n",
      "Epoch 85/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2610 - accuracy: 0.9223 - val_loss: 0.2623 - val_accuracy: 0.9219\n",
      "Epoch 86/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2608 - accuracy: 0.9223 - val_loss: 0.2621 - val_accuracy: 0.9219\n",
      "Epoch 87/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2607 - accuracy: 0.9223 - val_loss: 0.2620 - val_accuracy: 0.9219\n",
      "Epoch 88/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2605 - accuracy: 0.9223 - val_loss: 0.2617 - val_accuracy: 0.9219\n",
      "Epoch 89/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2603 - accuracy: 0.9223 - val_loss: 0.2615 - val_accuracy: 0.9219\n",
      "Epoch 90/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2601 - accuracy: 0.9223 - val_loss: 0.2613 - val_accuracy: 0.9219\n",
      "Epoch 91/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2598 - accuracy: 0.9223 - val_loss: 0.2610 - val_accuracy: 0.9219\n",
      "Epoch 92/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2596 - accuracy: 0.9223 - val_loss: 0.2608 - val_accuracy: 0.9219\n",
      "Epoch 93/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2593 - accuracy: 0.9223 - val_loss: 0.2606 - val_accuracy: 0.9219\n",
      "Epoch 94/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2591 - accuracy: 0.9223 - val_loss: 0.2603 - val_accuracy: 0.9219\n",
      "Epoch 95/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2588 - accuracy: 0.9223 - val_loss: 0.2601 - val_accuracy: 0.9219\n",
      "Epoch 96/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2586 - accuracy: 0.9223 - val_loss: 0.2599 - val_accuracy: 0.9219\n",
      "Epoch 97/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2583 - accuracy: 0.9223 - val_loss: 0.2597 - val_accuracy: 0.9219\n",
      "Epoch 98/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2581 - accuracy: 0.9223 - val_loss: 0.2594 - val_accuracy: 0.9219\n",
      "Epoch 99/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2578 - accuracy: 0.9223 - val_loss: 0.2590 - val_accuracy: 0.9219\n",
      "Epoch 100/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2576 - accuracy: 0.9223 - val_loss: 0.2587 - val_accuracy: 0.9219\n",
      "Epoch 101/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2573 - accuracy: 0.9223 - val_loss: 0.2585 - val_accuracy: 0.9219\n",
      "Epoch 102/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2570 - accuracy: 0.9223 - val_loss: 0.2583 - val_accuracy: 0.9219\n",
      "Epoch 103/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2568 - accuracy: 0.9223 - val_loss: 0.2579 - val_accuracy: 0.9219\n",
      "Epoch 104/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2564 - accuracy: 0.9223 - val_loss: 0.2576 - val_accuracy: 0.9219\n",
      "Epoch 105/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2562 - accuracy: 0.9223 - val_loss: 0.2574 - val_accuracy: 0.9219\n",
      "Epoch 106/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2559 - accuracy: 0.9223 - val_loss: 0.2570 - val_accuracy: 0.9219\n",
      "Epoch 107/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2556 - accuracy: 0.9223 - val_loss: 0.2567 - val_accuracy: 0.9219\n",
      "Epoch 108/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2553 - accuracy: 0.9223 - val_loss: 0.2564 - val_accuracy: 0.9219\n",
      "Epoch 109/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2550 - accuracy: 0.9223 - val_loss: 0.2561 - val_accuracy: 0.9219\n",
      "Epoch 110/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2547 - accuracy: 0.9223 - val_loss: 0.2558 - val_accuracy: 0.9219\n",
      "Epoch 111/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2544 - accuracy: 0.9223 - val_loss: 0.2555 - val_accuracy: 0.9219\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 0s 38us/step - loss: 0.2540 - accuracy: 0.9223 - val_loss: 0.2553 - val_accuracy: 0.9219\n",
      "Epoch 113/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2538 - accuracy: 0.9223 - val_loss: 0.2549 - val_accuracy: 0.9219\n",
      "Epoch 114/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2534 - accuracy: 0.9223 - val_loss: 0.2546 - val_accuracy: 0.9219\n",
      "Epoch 115/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.2544 - val_accuracy: 0.9219\n",
      "Epoch 116/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2528 - accuracy: 0.9224 - val_loss: 0.2542 - val_accuracy: 0.9219\n",
      "Epoch 117/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2525 - accuracy: 0.9225 - val_loss: 0.2537 - val_accuracy: 0.9225\n",
      "Epoch 118/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2522 - accuracy: 0.9225 - val_loss: 0.2534 - val_accuracy: 0.9225\n",
      "Epoch 119/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2519 - accuracy: 0.9226 - val_loss: 0.2531 - val_accuracy: 0.9225\n",
      "Epoch 120/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2515 - accuracy: 0.9227 - val_loss: 0.2528 - val_accuracy: 0.9225\n",
      "Epoch 121/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2512 - accuracy: 0.9227 - val_loss: 0.2525 - val_accuracy: 0.9225\n",
      "Epoch 122/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2509 - accuracy: 0.9227 - val_loss: 0.2522 - val_accuracy: 0.9225\n",
      "Epoch 123/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2506 - accuracy: 0.9227 - val_loss: 0.2519 - val_accuracy: 0.9225\n",
      "Epoch 124/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2502 - accuracy: 0.9227 - val_loss: 0.2515 - val_accuracy: 0.9225\n",
      "Epoch 125/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2499 - accuracy: 0.9227 - val_loss: 0.2512 - val_accuracy: 0.9225\n",
      "Epoch 126/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2496 - accuracy: 0.9227 - val_loss: 0.2510 - val_accuracy: 0.9225\n",
      "Epoch 127/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2493 - accuracy: 0.9228 - val_loss: 0.2508 - val_accuracy: 0.9229\n",
      "Epoch 128/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2490 - accuracy: 0.9227 - val_loss: 0.2505 - val_accuracy: 0.9229\n",
      "Epoch 129/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2487 - accuracy: 0.9227 - val_loss: 0.2503 - val_accuracy: 0.9229\n",
      "Epoch 130/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2484 - accuracy: 0.9227 - val_loss: 0.2499 - val_accuracy: 0.9229\n",
      "Epoch 131/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2481 - accuracy: 0.9226 - val_loss: 0.2496 - val_accuracy: 0.9229\n",
      "Epoch 132/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2478 - accuracy: 0.9234 - val_loss: 0.2494 - val_accuracy: 0.9229\n",
      "Epoch 133/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2475 - accuracy: 0.9226 - val_loss: 0.2492 - val_accuracy: 0.9229\n",
      "Epoch 134/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2472 - accuracy: 0.9232 - val_loss: 0.2489 - val_accuracy: 0.9248\n",
      "Epoch 135/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2470 - accuracy: 0.9238 - val_loss: 0.2487 - val_accuracy: 0.9248\n",
      "Epoch 136/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2467 - accuracy: 0.9238 - val_loss: 0.2485 - val_accuracy: 0.9248\n",
      "Epoch 137/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2465 - accuracy: 0.9246 - val_loss: 0.2482 - val_accuracy: 0.9248\n",
      "Epoch 138/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2463 - accuracy: 0.9245 - val_loss: 0.2481 - val_accuracy: 0.9248\n",
      "Epoch 139/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2461 - accuracy: 0.9246 - val_loss: 0.2478 - val_accuracy: 0.9248\n",
      "Epoch 140/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2457 - accuracy: 0.9245 - val_loss: 0.2477 - val_accuracy: 0.9251\n",
      "Epoch 141/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2456 - accuracy: 0.9248 - val_loss: 0.2476 - val_accuracy: 0.9248\n",
      "Epoch 142/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2454 - accuracy: 0.9245 - val_loss: 0.2474 - val_accuracy: 0.9248\n",
      "Epoch 143/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2452 - accuracy: 0.9247 - val_loss: 0.2473 - val_accuracy: 0.9248\n",
      "Epoch 144/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2449 - accuracy: 0.9246 - val_loss: 0.2471 - val_accuracy: 0.9251\n",
      "Epoch 145/200\n",
      "640/640 [==============================] - 0s 33us/step - loss: 0.2448 - accuracy: 0.9247 - val_loss: 0.2470 - val_accuracy: 0.9251\n",
      "Epoch 146/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2446 - accuracy: 0.9247 - val_loss: 0.2468 - val_accuracy: 0.9251\n",
      "Epoch 147/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2445 - accuracy: 0.9247 - val_loss: 0.2468 - val_accuracy: 0.9251\n",
      "Epoch 148/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2443 - accuracy: 0.9247 - val_loss: 0.2467 - val_accuracy: 0.9251\n",
      "Epoch 149/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2441 - accuracy: 0.9247 - val_loss: 0.2467 - val_accuracy: 0.9251\n",
      "Epoch 150/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2440 - accuracy: 0.9247 - val_loss: 0.2467 - val_accuracy: 0.9251\n",
      "Epoch 151/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2439 - accuracy: 0.9247 - val_loss: 0.2466 - val_accuracy: 0.9251\n",
      "Epoch 152/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2437 - accuracy: 0.9247 - val_loss: 0.2465 - val_accuracy: 0.9251\n",
      "Epoch 153/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2436 - accuracy: 0.9247 - val_loss: 0.2464 - val_accuracy: 0.9251\n",
      "Epoch 154/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2435 - accuracy: 0.9247 - val_loss: 0.2462 - val_accuracy: 0.9251\n",
      "Epoch 155/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2433 - accuracy: 0.9247 - val_loss: 0.2461 - val_accuracy: 0.9251\n",
      "Epoch 156/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2432 - accuracy: 0.9247 - val_loss: 0.2461 - val_accuracy: 0.9251\n",
      "Epoch 157/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2431 - accuracy: 0.9247 - val_loss: 0.2460 - val_accuracy: 0.9251\n",
      "Epoch 158/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2430 - accuracy: 0.9248 - val_loss: 0.2460 - val_accuracy: 0.9251\n",
      "Epoch 159/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2429 - accuracy: 0.9247 - val_loss: 0.2460 - val_accuracy: 0.9251\n",
      "Epoch 160/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2428 - accuracy: 0.9248 - val_loss: 0.2459 - val_accuracy: 0.9251\n",
      "Epoch 161/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2427 - accuracy: 0.9247 - val_loss: 0.2458 - val_accuracy: 0.9251\n",
      "Epoch 162/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2426 - accuracy: 0.9247 - val_loss: 0.2458 - val_accuracy: 0.9251\n",
      "Epoch 163/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2425 - accuracy: 0.9247 - val_loss: 0.2457 - val_accuracy: 0.9251\n",
      "Epoch 164/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2424 - accuracy: 0.9248 - val_loss: 0.2457 - val_accuracy: 0.9251\n",
      "Epoch 165/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2424 - accuracy: 0.9248 - val_loss: 0.2455 - val_accuracy: 0.9251\n",
      "Epoch 166/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2423 - accuracy: 0.9248 - val_loss: 0.2454 - val_accuracy: 0.9251\n",
      "Epoch 167/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2422 - accuracy: 0.9248 - val_loss: 0.2455 - val_accuracy: 0.9251\n",
      "Epoch 168/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2422 - accuracy: 0.9248 - val_loss: 0.2455 - val_accuracy: 0.9251\n",
      "Epoch 169/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2421 - accuracy: 0.9248 - val_loss: 0.2455 - val_accuracy: 0.9251\n",
      "Epoch 170/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2420 - accuracy: 0.9248 - val_loss: 0.2453 - val_accuracy: 0.9251\n",
      "Epoch 171/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2419 - accuracy: 0.9248 - val_loss: 0.2454 - val_accuracy: 0.9251\n",
      "Epoch 172/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2418 - accuracy: 0.9248 - val_loss: 0.2454 - val_accuracy: 0.9251\n",
      "Epoch 173/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2418 - accuracy: 0.9247 - val_loss: 0.2453 - val_accuracy: 0.9251\n",
      "Epoch 174/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2417 - accuracy: 0.9248 - val_loss: 0.2452 - val_accuracy: 0.9251\n",
      "Epoch 175/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2416 - accuracy: 0.9248 - val_loss: 0.2451 - val_accuracy: 0.9251\n",
      "Epoch 176/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2416 - accuracy: 0.9247 - val_loss: 0.2452 - val_accuracy: 0.9251\n",
      "Epoch 177/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2415 - accuracy: 0.9248 - val_loss: 0.2453 - val_accuracy: 0.9251\n",
      "Epoch 178/200\n",
      "640/640 [==============================] - 0s 39us/step - loss: 0.2414 - accuracy: 0.9248 - val_loss: 0.2452 - val_accuracy: 0.9251\n",
      "Epoch 179/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2414 - accuracy: 0.9248 - val_loss: 0.2452 - val_accuracy: 0.9251\n",
      "Epoch 180/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2413 - accuracy: 0.9248 - val_loss: 0.2450 - val_accuracy: 0.9251\n",
      "Epoch 181/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2412 - accuracy: 0.9248 - val_loss: 0.2451 - val_accuracy: 0.9251\n",
      "Epoch 182/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2412 - accuracy: 0.9247 - val_loss: 0.2450 - val_accuracy: 0.9251\n",
      "Epoch 183/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2411 - accuracy: 0.9248 - val_loss: 0.2449 - val_accuracy: 0.9251\n",
      "Epoch 184/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2411 - accuracy: 0.9247 - val_loss: 0.2450 - val_accuracy: 0.9251\n",
      "Epoch 185/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2409 - accuracy: 0.9247 - val_loss: 0.2449 - val_accuracy: 0.9251\n",
      "Epoch 186/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2410 - accuracy: 0.9248 - val_loss: 0.2448 - val_accuracy: 0.9251\n",
      "Epoch 187/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2409 - accuracy: 0.9248 - val_loss: 0.2449 - val_accuracy: 0.9251\n",
      "Epoch 188/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2408 - accuracy: 0.9248 - val_loss: 0.2449 - val_accuracy: 0.9251\n",
      "Epoch 189/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2408 - accuracy: 0.9248 - val_loss: 0.2447 - val_accuracy: 0.9251\n",
      "Epoch 190/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2407 - accuracy: 0.9248 - val_loss: 0.2447 - val_accuracy: 0.9251\n",
      "Epoch 191/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2406 - accuracy: 0.9248 - val_loss: 0.2447 - val_accuracy: 0.9251\n",
      "Epoch 192/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2405 - accuracy: 0.9248 - val_loss: 0.2446 - val_accuracy: 0.9251\n",
      "Epoch 193/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2405 - accuracy: 0.9248 - val_loss: 0.2446 - val_accuracy: 0.9251\n",
      "Epoch 194/200\n",
      "640/640 [==============================] - 0s 45us/step - loss: 0.2404 - accuracy: 0.9248 - val_loss: 0.2444 - val_accuracy: 0.9251\n",
      "Epoch 195/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2403 - accuracy: 0.9248 - val_loss: 0.2444 - val_accuracy: 0.9251\n",
      "Epoch 196/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2403 - accuracy: 0.9248 - val_loss: 0.2445 - val_accuracy: 0.9251\n",
      "Epoch 197/200\n",
      "640/640 [==============================] - 0s 38us/step - loss: 0.2402 - accuracy: 0.9248 - val_loss: 0.2443 - val_accuracy: 0.9251\n",
      "Epoch 198/200\n",
      "640/640 [==============================] - 0s 34us/step - loss: 0.2401 - accuracy: 0.9248 - val_loss: 0.2443 - val_accuracy: 0.9251\n",
      "Epoch 199/200\n",
      "640/640 [==============================] - 0s 41us/step - loss: 0.2401 - accuracy: 0.9248 - val_loss: 0.2442 - val_accuracy: 0.9251\n",
      "Epoch 200/200\n",
      "640/640 [==============================] - 0s 36us/step - loss: 0.2400 - accuracy: 0.9248 - val_loss: 0.2441 - val_accuracy: 0.9251\n",
      "Begin McNemar's test\n",
      "[200, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[200, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[161, 5]\n",
      "[28, 6]\n",
      "[163, 3]\n",
      "[27, 7]\n",
      "[136, 4]\n",
      "[56, 4]\n",
      "[121, 19]\n",
      "[59, 1]\n",
      "[126, 12]\n",
      "[49, 13]\n",
      "[129, 9]\n",
      "[53, 9]\n",
      "[143, 10]\n",
      "[43, 4]\n",
      "[150, 3]\n",
      "[39, 8]\n",
      "[171, 9]\n",
      "[16, 4]\n",
      "[178, 2]\n",
      "[16, 4]\n",
      "[111, 15]\n",
      "[70, 4]\n",
      "[119, 7]\n",
      "[54, 20]\n",
      "[113, 6]\n",
      "[79, 2]\n",
      "[117, 2]\n",
      "[69, 12]\n",
      "[143, 5]\n",
      "[51, 1]\n",
      "[146, 2]\n",
      "[51, 1]\n",
      "[147, 6]\n",
      "[44, 3]\n",
      "[151, 2]\n",
      "[44, 3]\n",
      "[121, 8]\n",
      "[50, 21]\n",
      "[109, 20]\n",
      "[64, 7]\n",
      "[109, 14]\n",
      "[73, 4]\n",
      "[119, 4]\n",
      "[52, 25]\n",
      "[145, 8]\n",
      "[45, 2]\n",
      "[150, 3]\n",
      "[44, 3]\n",
      "[149, 5]\n",
      "[40, 6]\n",
      "[152, 2]\n",
      "[42, 4]\n",
      "[156, 8]\n",
      "[28, 8]\n",
      "[162, 2]\n",
      "[30, 6]\n",
      "[154, 7]\n",
      "[35, 4]\n",
      "[157, 4]\n",
      "[35, 4]\n",
      "[138, 3]\n",
      "[51, 8]\n",
      "[138, 3]\n",
      "[55, 4]\n",
      "[131, 3]\n",
      "[49, 17]\n",
      "[131, 3]\n",
      "[49, 17]\n",
      "[137, 18]\n",
      "[41, 4]\n",
      "[142, 13]\n",
      "[40, 5]\n",
      "End McNemar's test\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9267098716941595 (+- 0.0038180079949522525)\n",
      "> F1: 0.5266242068451266(+- 0.026082720479859364)\n",
      "> Time: 1.5583959399999998 (+- 0.04632291920017986)\n",
      "#####################################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.93065331238544 (+- 0.003660287974982745)\n",
      "> F1: 0.5184956186239873(+- 0.030013668737330055)\n",
      "> Time: 0.0026004999999999995 (+- 0.0004898571342748822)\n",
      "#####################################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.8877612383491111 (+- 0.07060396627329324)\n",
      "> F1: 0.27771360203076656(+- 0.03616525926727746)\n",
      "> Time: 0.04032864 (+- 0.000574573774549449)\n",
      "#####################################################################################\n",
      "> AUC for class : 0.03266331658291455 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.3440958503686363 (+- 0.09622142669917373)\n",
      "X^2 for MWPM and NN: 17.454545454545453\n",
      "X^2 for PLUT and NN: 20.833333333333332\n",
      "> AUC for class X01: 0.772736648402472 (+- 0.08895190004744631)\n",
      "X^2 for MWPM and NN: 46.81666666666667\n",
      "X^2 for PLUT and NN: 21.55128205128205\n",
      "> AUC for class X02: 0.7017571153344743 (+- 0.10285811896990116)\n",
      "X^2 for MWPM and NN: 23.672131147540984\n",
      "X^2 for PLUT and NN: 32.66129032258065\n",
      "> AUC for class X10: 0.7009025873567831 (+- 0.07585715974940817)\n",
      "X^2 for MWPM and NN: 21.81132075471698\n",
      "X^2 for PLUT and NN: 32.595238095238095\n",
      "> AUC for class X11: 0.8587897440050332 (+- 0.04824914190060137)\n",
      "X^2 for MWPM and NN: 2.56\n",
      "X^2 for PLUT and NN: 12.5\n",
      "> AUC for class X12: 0.7381005199010037 (+- 0.050116296153448134)\n",
      "X^2 for MWPM and NN: 36.89411764705882\n",
      "X^2 for PLUT and NN: 37.77049180327869\n",
      "> AUC for class X20: 0.7170587594091546 (+- 0.10123610059276973)\n",
      "X^2 for MWPM and NN: 64.4235294117647\n",
      "X^2 for PLUT and NN: 65.12676056338029\n",
      "> AUC for class X21: 0.8958970337798918 (+- 0.06546351154833058)\n",
      "X^2 for MWPM and NN: 39.44642857142857\n",
      "X^2 for PLUT and NN: 47.16981132075472\n",
      "> AUC for class X22: 0.7244407684271505 (+- 0.11697698165700607)\n",
      "X^2 for MWPM and NN: 30.42\n",
      "X^2 for PLUT and NN: 40.19565217391305\n",
      "> AUC for class Z00: 0.6894876836891758 (+- 0.10499753080301874)\n",
      "X^2 for MWPM and NN: 31.879310344827587\n",
      "X^2 for PLUT and NN: 24.107142857142858\n",
      "> AUC for class Z01: 0.7246327929096943 (+- 0.035982282200429455)\n",
      "X^2 for MWPM and NN: 41.37931034482759\n",
      "X^2 for PLUT and NN: 42.875\n",
      "> AUC for class Z02: 0.809902890403559 (+- 0.15707611274613636)\n",
      "X^2 for MWPM and NN: 27.245283018867923\n",
      "X^2 for PLUT and NN: 37.53191489361702\n",
      "> AUC for class Z10: 0.8634197945934527 (+- 0.06991062115309173)\n",
      "X^2 for MWPM and NN: 28.8\n",
      "X^2 for PLUT and NN: 38.20454545454545\n",
      "> AUC for class Z11: 0.8219221258041032 (+- 0.1368657822694532)\n",
      "X^2 for MWPM and NN: 12.25\n",
      "X^2 for PLUT and NN: 26.28125\n",
      "> AUC for class Z12: 0.8960449159797352 (+- 0.08296067185481885)\n",
      "X^2 for MWPM and NN: 20.023809523809526\n",
      "X^2 for PLUT and NN: 26.256410256410255\n",
      "> AUC for class Z20: 0.7490209218192435 (+- 0.10474083284061234)\n",
      "X^2 for MWPM and NN: 44.46296296296296\n",
      "X^2 for PLUT and NN: 48.43103448275862\n",
      "> AUC for class Z21: 0.6916623455831029 (+- 0.06037042006267765)\n",
      "X^2 for MWPM and NN: 42.48076923076923\n",
      "X^2 for PLUT and NN: 42.48076923076923\n",
      "> AUC for class Z22: 0.6830144580166693 (+- 0.07553984062005746)\n",
      "X^2 for MWPM and NN: 9.76271186440678\n",
      "X^2 for PLUT and NN: 14.79245283018868\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.24090121317157712, 0.23483057525610718, 0.3087362171331637, 0.32653061224489793, 0.27756939234808703]\n",
      "TOTAL F1 PLUT: [0.4760147601476014, 0.5381818181818182, 0.5046728971962617, 0.5098743267504489, 0.5637342908438061]\n",
      "TOTAL F1 MWPM: [0.4818024263431543, 0.5454545454545454, 0.5263157894736843, 0.5211726384364821, 0.5583756345177665]\n",
      "TOTAL ACC NN: [0.9230165481567383, 0.921842098236084, 0.9255263805389404, 0.9218422174453735, 0.7465789473684201]\n",
      "TOTAL ACC PLUT: [0.9256349829798364, 0.933157894736841, 0.9302631578947357, 0.9281578947368407, 0.9360526315789461]\n",
      "TOTAL ACC MWPM: [0.9217072532076446, 0.9289473684210516, 0.9289473684210514, 0.922631578947367, 0.931315789473683]\n",
      "TOTAL TIME NN: [0.0396071, 0.0400088, 0.0410089, 0.0400093, 0.0410091]\n",
      "TOTAL TIME PLUT: [0.0020005, 0.0030003, 0.0030006, 0.0030005, 0.0020006]\n",
      "TOTAL TIME MWPM: [1.5156126999999997, 1.6213685999999996, 1.5153451000000004, 1.607366, 1.5322872999999997]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d3.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d3.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d3.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    \n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d3 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    \n",
    "    decoding_d3, time_mwpm = do_new_decoding(x_test_d3, 3, .03)\n",
    "    decoding_d3['combine'] = decoding_d3[[0, 1]].values.tolist()\n",
    "    decoding_d3['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d3 = np.array(decoding_d3[0])\n",
    "\n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "    pred_mwpm = mlb_d3.transform(decoding_d3)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb_d3)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "\n",
    "    lookup_d3 = lookup_decoder(3)\n",
    "\n",
    "    lookup_d3 = train_plut(lookup_d3, inputs_train, targets[train])\n",
    "\n",
    "    start = time.time_ns()\n",
    "    pred_plut_d3 = test_plut(lookup_d3, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d3)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d3, mlb_d3)\n",
    "\n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d3, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "\n",
    "    model = compile_FFNN_cv_model_DepthThree(3)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "\n",
    "    history = model.fit(\n",
    "        inputs_train, targets[train],\n",
    "        validation_split=.2,\n",
    "        epochs=200,\n",
    "        verbose=1)\n",
    "\n",
    "   # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs_test, targets[test], verbose=0)\n",
    "\n",
    "    #get the time to predicting test\n",
    "    start = time.time_ns()\n",
    "    predictions_d3 = model.predict(inputs_test) #change here\n",
    "    end = time.time_ns()\n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "\n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d3.copy() #change here\n",
    "    pred[pred>=.1]=1 \n",
    "    pred[pred<.1]=0\n",
    "    \n",
    "    if fold_no <5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb_d3)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #comput ROC AUC for classes and the mircoaverage\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d3.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d3[:, i]) #change here\n",
    "        aucs_classes[mlb_d3.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "print(\"Begin McNemar's test\")\n",
    "for class_ in mlb_d3.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1] \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "print(\"End McNemar's test\")    \n",
    "\n",
    "############print mean and stdev of AUC of each class#####################      \n",
    "    \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"#####################################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 3 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'X00': 17.454545454545453, 'X01': 46.81666666666667, 'X02': 23.672131147540984, 'X10': 21.81132075471698, 'X11': 2.56, 'X12': 36.89411764705882, 'X20': 64.4235294117647, 'X21': 39.44642857142857, 'X22': 30.42, 'Z00': 31.879310344827587, 'Z01': 41.37931034482759, 'Z02': 27.245283018867923, 'Z10': 28.8, 'Z11': 12.25, 'Z12': 20.023809523809526, 'Z20': 44.46296296296296, 'Z21': 42.48076923076923, 'Z22': 9.76271186440678}\n",
      "{'': 0, 'X00': 20.833333333333332, 'X01': 21.55128205128205, 'X02': 32.66129032258065, 'X10': 32.595238095238095, 'X11': 12.5, 'X12': 37.77049180327869, 'X20': 65.12676056338029, 'X21': 47.16981132075472, 'X22': 40.19565217391305, 'Z00': 24.107142857142858, 'Z01': 42.875, 'Z02': 37.53191489361702, 'Z10': 38.20454545454545, 'Z11': 26.28125, 'Z12': 26.256410256410255, 'Z20': 48.43103448275862, 'Z21': 42.48076923076923, 'Z22': 14.79245283018868}\n"
     ]
    }
   ],
   "source": [
    "print(mcnemar_results_mwpm)\n",
    "print(mcnemar_results_plut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D5 Training, Testing, and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data sets from CSV\n",
    "\n",
    "#### Data sets for D5:\n",
    "* Original:\n",
    "    - \"depth5_all_combos.csv\"\n",
    "* Randomly-sampled, with replacement (version 2):\n",
    "    - \"v2samples-d5-1000.csv\"\n",
    "    - \"v2samples-d5-10000.csv\"\n",
    "    - \"v2samples-d5-100000.csv\"\n",
    "* Randomly-sampled, without replacement (version 3):\n",
    "    - \"v3samples-d5-1000.csv\"\n",
    "    - \"v3samples-d5-10000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and formatted in 40.07048797607422 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainData_d5 = pd.read_csv(\"SAMPLES/v3samples-d5-10000.csv\")\n",
    "\n",
    "trainData_d5 = trainData_d5.applymap(lambda x: add_noise(x,.01)) #was .05\n",
    "\n",
    "#These four lines remove duplicates\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].astype(str)\n",
    "trainData_d5 = trainData_d5.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "testData_d5_MWPM = graph_with_errs_d5(trainData_d5)\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(trainData_d5['Labels'])\n",
    "df = pd.DataFrame(mlb.transform(trainData_d5['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d5 = trainData_d5.drop(['Labels'], axis=1)\n",
    "trainData_d5 = pd.concat([df[\"Labels\"], testData_d5_MWPM, trainData_d5], axis=1, ignore_index=True)\n",
    "trainData_d5.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\",\"Z1\",\"X2\",\"X3\",\"Z4\",\"X5\",\"Z6\",\"Z7\",\"X8\",\"Z9\",\"X10\",\"Z11\",\"Z12\",\"X13\",\"Z14\",\"X15\",\"Z16\",\"Z17\",\"X18\",\"X19\",\"Z20\",\"X21\",\"X22\",\"Z23\"]\n",
    "\n",
    "y_d5 = trainData_d5[\"Labels\"] \n",
    "x_d5 = trainData_d5.drop([\"Labels\"], axis=1) \n",
    "\n",
    "x_d5 = x_d5.replace([-1], 0)\n",
    "print(\"Data imported and formatted in %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 5625 samples, validate on 1875 samples\n",
      "Epoch 1/500\n",
      "5625/5625 [==============================] - 1s 97us/step - loss: 0.4833 - accuracy: 0.8327 - val_loss: 0.2564 - val_accuracy: 0.9288\n",
      "Epoch 2/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2542 - accuracy: 0.9290 - val_loss: 0.2540 - val_accuracy: 0.9288\n",
      "Epoch 3/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2533 - accuracy: 0.9290 - val_loss: 0.2536 - val_accuracy: 0.9288\n",
      "Epoch 4/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2529 - accuracy: 0.9290 - val_loss: 0.2534 - val_accuracy: 0.9288\n",
      "Epoch 5/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2527 - accuracy: 0.9290 - val_loss: 0.2529 - val_accuracy: 0.9288\n",
      "Epoch 6/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2523 - accuracy: 0.9290 - val_loss: 0.2526 - val_accuracy: 0.9288\n",
      "Epoch 7/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2520 - accuracy: 0.9290 - val_loss: 0.2523 - val_accuracy: 0.9288\n",
      "Epoch 8/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2517 - accuracy: 0.9290 - val_loss: 0.2521 - val_accuracy: 0.9288\n",
      "Epoch 9/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2514 - accuracy: 0.9290 - val_loss: 0.2519 - val_accuracy: 0.9288\n",
      "Epoch 10/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2511 - accuracy: 0.9290 - val_loss: 0.2516 - val_accuracy: 0.9288\n",
      "Epoch 11/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2508 - accuracy: 0.9290 - val_loss: 0.2512 - val_accuracy: 0.9288\n",
      "Epoch 12/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2504 - accuracy: 0.9290 - val_loss: 0.2507 - val_accuracy: 0.9288\n",
      "Epoch 13/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2500 - accuracy: 0.9290 - val_loss: 0.2504 - val_accuracy: 0.9288\n",
      "Epoch 14/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2496 - accuracy: 0.9290 - val_loss: 0.2500 - val_accuracy: 0.9288\n",
      "Epoch 15/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2491 - accuracy: 0.9290 - val_loss: 0.2495 - val_accuracy: 0.9288\n",
      "Epoch 16/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2486 - accuracy: 0.9290 - val_loss: 0.2491 - val_accuracy: 0.9288\n",
      "Epoch 17/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2481 - accuracy: 0.9290 - val_loss: 0.2484 - val_accuracy: 0.9288\n",
      "Epoch 18/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2475 - accuracy: 0.9290 - val_loss: 0.2477 - val_accuracy: 0.9288\n",
      "Epoch 19/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2468 - accuracy: 0.9290 - val_loss: 0.2471 - val_accuracy: 0.9288\n",
      "Epoch 20/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2460 - accuracy: 0.9290 - val_loss: 0.2463 - val_accuracy: 0.9288\n",
      "Epoch 21/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2452 - accuracy: 0.9290 - val_loss: 0.2454 - val_accuracy: 0.9288\n",
      "Epoch 22/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2442 - accuracy: 0.9290 - val_loss: 0.2445 - val_accuracy: 0.9288\n",
      "Epoch 23/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.2431 - accuracy: 0.9290 - val_loss: 0.2433 - val_accuracy: 0.9288\n",
      "Epoch 24/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2420 - accuracy: 0.9290 - val_loss: 0.2421 - val_accuracy: 0.9288\n",
      "Epoch 25/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.2407 - accuracy: 0.9290 - val_loss: 0.2409 - val_accuracy: 0.9288\n",
      "Epoch 26/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2395 - accuracy: 0.9290 - val_loss: 0.2397 - val_accuracy: 0.9288\n",
      "Epoch 27/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2382 - accuracy: 0.9290 - val_loss: 0.2384 - val_accuracy: 0.9288\n",
      "Epoch 28/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2369 - accuracy: 0.9290 - val_loss: 0.2371 - val_accuracy: 0.9288\n",
      "Epoch 29/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2356 - accuracy: 0.9291 - val_loss: 0.2359 - val_accuracy: 0.9288\n",
      "Epoch 30/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2343 - accuracy: 0.9291 - val_loss: 0.2347 - val_accuracy: 0.9288\n",
      "Epoch 31/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2330 - accuracy: 0.9291 - val_loss: 0.2333 - val_accuracy: 0.9288\n",
      "Epoch 32/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2317 - accuracy: 0.9292 - val_loss: 0.2321 - val_accuracy: 0.9289\n",
      "Epoch 33/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2304 - accuracy: 0.9292 - val_loss: 0.2310 - val_accuracy: 0.9290\n",
      "Epoch 34/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2292 - accuracy: 0.9292 - val_loss: 0.2298 - val_accuracy: 0.9289\n",
      "Epoch 35/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2279 - accuracy: 0.9292 - val_loss: 0.2287 - val_accuracy: 0.9290\n",
      "Epoch 36/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2266 - accuracy: 0.9294 - val_loss: 0.2276 - val_accuracy: 0.9290\n",
      "Epoch 37/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2253 - accuracy: 0.9294 - val_loss: 0.2262 - val_accuracy: 0.9290\n",
      "Epoch 38/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2240 - accuracy: 0.9294 - val_loss: 0.2249 - val_accuracy: 0.9290\n",
      "Epoch 39/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2228 - accuracy: 0.9294 - val_loss: 0.2236 - val_accuracy: 0.9291\n",
      "Epoch 40/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2214 - accuracy: 0.9294 - val_loss: 0.2224 - val_accuracy: 0.9292\n",
      "Epoch 41/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2202 - accuracy: 0.9295 - val_loss: 0.2215 - val_accuracy: 0.9295\n",
      "Epoch 42/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2190 - accuracy: 0.9295 - val_loss: 0.2201 - val_accuracy: 0.9294\n",
      "Epoch 43/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.2177 - accuracy: 0.9295 - val_loss: 0.2189 - val_accuracy: 0.9294\n",
      "Epoch 44/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.2166 - accuracy: 0.9296 - val_loss: 0.2177 - val_accuracy: 0.9295\n",
      "Epoch 45/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.2155 - accuracy: 0.9296 - val_loss: 0.2168 - val_accuracy: 0.9295\n",
      "Epoch 46/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2143 - accuracy: 0.9296 - val_loss: 0.2158 - val_accuracy: 0.9294\n",
      "Epoch 47/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.2132 - accuracy: 0.9296 - val_loss: 0.2151 - val_accuracy: 0.9295\n",
      "Epoch 48/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2122 - accuracy: 0.9297 - val_loss: 0.2136 - val_accuracy: 0.9296\n",
      "Epoch 49/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2112 - accuracy: 0.9297 - val_loss: 0.2128 - val_accuracy: 0.9295\n",
      "Epoch 50/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.2102 - accuracy: 0.9297 - val_loss: 0.2119 - val_accuracy: 0.9296\n",
      "Epoch 51/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2093 - accuracy: 0.9298 - val_loss: 0.2113 - val_accuracy: 0.9295\n",
      "Epoch 52/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2084 - accuracy: 0.9298 - val_loss: 0.2103 - val_accuracy: 0.9300\n",
      "Epoch 53/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.2076 - accuracy: 0.9299 - val_loss: 0.2096 - val_accuracy: 0.9297\n",
      "Epoch 54/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2068 - accuracy: 0.9299 - val_loss: 0.2089 - val_accuracy: 0.9297\n",
      "Epoch 55/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2060 - accuracy: 0.9299 - val_loss: 0.2081 - val_accuracy: 0.9299\n",
      "Epoch 56/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2052 - accuracy: 0.9300 - val_loss: 0.2074 - val_accuracy: 0.9299\n",
      "Epoch 57/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2045 - accuracy: 0.9301 - val_loss: 0.2067 - val_accuracy: 0.9299\n",
      "Epoch 58/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.2038 - accuracy: 0.9302 - val_loss: 0.2060 - val_accuracy: 0.9301\n",
      "Epoch 59/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2031 - accuracy: 0.9302 - val_loss: 0.2053 - val_accuracy: 0.9300\n",
      "Epoch 60/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2025 - accuracy: 0.9303 - val_loss: 0.2047 - val_accuracy: 0.9300\n",
      "Epoch 61/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2018 - accuracy: 0.9303 - val_loss: 0.2044 - val_accuracy: 0.9303\n",
      "Epoch 62/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.2011 - accuracy: 0.9304 - val_loss: 0.2040 - val_accuracy: 0.9301\n",
      "Epoch 63/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.2004 - accuracy: 0.9306 - val_loss: 0.2032 - val_accuracy: 0.9307\n",
      "Epoch 64/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1999 - accuracy: 0.9306 - val_loss: 0.2025 - val_accuracy: 0.9305\n",
      "Epoch 65/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1993 - accuracy: 0.9306 - val_loss: 0.2022 - val_accuracy: 0.9307\n",
      "Epoch 66/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1987 - accuracy: 0.9306 - val_loss: 0.2015 - val_accuracy: 0.9306\n",
      "Epoch 67/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1982 - accuracy: 0.9306 - val_loss: 0.2009 - val_accuracy: 0.9308\n",
      "Epoch 68/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1976 - accuracy: 0.9307 - val_loss: 0.2004 - val_accuracy: 0.9310\n",
      "Epoch 69/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1970 - accuracy: 0.9309 - val_loss: 0.1997 - val_accuracy: 0.9308\n",
      "Epoch 70/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1965 - accuracy: 0.9309 - val_loss: 0.1995 - val_accuracy: 0.9310\n",
      "Epoch 71/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1959 - accuracy: 0.9310 - val_loss: 0.1988 - val_accuracy: 0.9312\n",
      "Epoch 72/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1953 - accuracy: 0.9311 - val_loss: 0.1982 - val_accuracy: 0.9313\n",
      "Epoch 73/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1947 - accuracy: 0.9311 - val_loss: 0.1977 - val_accuracy: 0.9312\n",
      "Epoch 74/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1940 - accuracy: 0.9312 - val_loss: 0.1971 - val_accuracy: 0.9314\n",
      "Epoch 75/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1935 - accuracy: 0.9313 - val_loss: 0.1966 - val_accuracy: 0.9316\n",
      "Epoch 76/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1929 - accuracy: 0.9316 - val_loss: 0.1962 - val_accuracy: 0.9317\n",
      "Epoch 77/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1923 - accuracy: 0.9315 - val_loss: 0.1954 - val_accuracy: 0.9317\n",
      "Epoch 78/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1917 - accuracy: 0.9316 - val_loss: 0.1950 - val_accuracy: 0.9319\n",
      "Epoch 79/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1911 - accuracy: 0.9318 - val_loss: 0.1945 - val_accuracy: 0.9320\n",
      "Epoch 80/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1905 - accuracy: 0.9320 - val_loss: 0.1940 - val_accuracy: 0.9321\n",
      "Epoch 81/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1900 - accuracy: 0.9321 - val_loss: 0.1935 - val_accuracy: 0.9322\n",
      "Epoch 82/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1894 - accuracy: 0.9321 - val_loss: 0.1931 - val_accuracy: 0.9324\n",
      "Epoch 83/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1889 - accuracy: 0.9324 - val_loss: 0.1928 - val_accuracy: 0.9323\n",
      "Epoch 84/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1884 - accuracy: 0.9325 - val_loss: 0.1921 - val_accuracy: 0.9325\n",
      "Epoch 85/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1879 - accuracy: 0.9326 - val_loss: 0.1916 - val_accuracy: 0.9325\n",
      "Epoch 86/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1874 - accuracy: 0.9327 - val_loss: 0.1910 - val_accuracy: 0.9327\n",
      "Epoch 87/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1868 - accuracy: 0.9328 - val_loss: 0.1908 - val_accuracy: 0.9330\n",
      "Epoch 88/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1863 - accuracy: 0.9330 - val_loss: 0.1901 - val_accuracy: 0.9331\n",
      "Epoch 89/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1858 - accuracy: 0.9330 - val_loss: 0.1900 - val_accuracy: 0.9332\n",
      "Epoch 90/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1853 - accuracy: 0.9333 - val_loss: 0.1893 - val_accuracy: 0.9332\n",
      "Epoch 91/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1848 - accuracy: 0.9334 - val_loss: 0.1887 - val_accuracy: 0.9334\n",
      "Epoch 92/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1843 - accuracy: 0.9336 - val_loss: 0.1884 - val_accuracy: 0.9334\n",
      "Epoch 93/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1837 - accuracy: 0.9337 - val_loss: 0.1877 - val_accuracy: 0.9338\n",
      "Epoch 94/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1833 - accuracy: 0.9337 - val_loss: 0.1873 - val_accuracy: 0.9338\n",
      "Epoch 95/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1828 - accuracy: 0.9340 - val_loss: 0.1866 - val_accuracy: 0.9340\n",
      "Epoch 96/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1823 - accuracy: 0.9343 - val_loss: 0.1865 - val_accuracy: 0.9340\n",
      "Epoch 97/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1818 - accuracy: 0.9342 - val_loss: 0.1862 - val_accuracy: 0.9342\n",
      "Epoch 98/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1813 - accuracy: 0.9344 - val_loss: 0.1855 - val_accuracy: 0.9341\n",
      "Epoch 99/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1809 - accuracy: 0.9345 - val_loss: 0.1852 - val_accuracy: 0.9343\n",
      "Epoch 100/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1804 - accuracy: 0.9346 - val_loss: 0.1845 - val_accuracy: 0.9341\n",
      "Epoch 101/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1800 - accuracy: 0.9347 - val_loss: 0.1841 - val_accuracy: 0.9342\n",
      "Epoch 102/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1795 - accuracy: 0.9348 - val_loss: 0.1837 - val_accuracy: 0.9347\n",
      "Epoch 103/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1791 - accuracy: 0.9349 - val_loss: 0.1836 - val_accuracy: 0.9348\n",
      "Epoch 104/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1786 - accuracy: 0.9353 - val_loss: 0.1833 - val_accuracy: 0.9349\n",
      "Epoch 105/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1782 - accuracy: 0.9354 - val_loss: 0.1826 - val_accuracy: 0.9349\n",
      "Epoch 106/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1777 - accuracy: 0.9354 - val_loss: 0.1820 - val_accuracy: 0.9350\n",
      "Epoch 107/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1772 - accuracy: 0.9355 - val_loss: 0.1818 - val_accuracy: 0.9348\n",
      "Epoch 108/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1767 - accuracy: 0.9357 - val_loss: 0.1812 - val_accuracy: 0.9350\n",
      "Epoch 109/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1763 - accuracy: 0.9358 - val_loss: 0.1807 - val_accuracy: 0.9350\n",
      "Epoch 110/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1758 - accuracy: 0.9357 - val_loss: 0.1803 - val_accuracy: 0.9354\n",
      "Epoch 111/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1754 - accuracy: 0.9358 - val_loss: 0.1798 - val_accuracy: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1750 - accuracy: 0.9361 - val_loss: 0.1795 - val_accuracy: 0.9354\n",
      "Epoch 113/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1745 - accuracy: 0.9363 - val_loss: 0.1791 - val_accuracy: 0.9356\n",
      "Epoch 114/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1741 - accuracy: 0.9362 - val_loss: 0.1787 - val_accuracy: 0.9360\n",
      "Epoch 115/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1736 - accuracy: 0.9365 - val_loss: 0.1782 - val_accuracy: 0.9361\n",
      "Epoch 116/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1731 - accuracy: 0.9367 - val_loss: 0.1786 - val_accuracy: 0.9357\n",
      "Epoch 117/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1728 - accuracy: 0.9367 - val_loss: 0.1773 - val_accuracy: 0.9363\n",
      "Epoch 118/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1722 - accuracy: 0.9369 - val_loss: 0.1774 - val_accuracy: 0.9359\n",
      "Epoch 119/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1719 - accuracy: 0.9370 - val_loss: 0.1766 - val_accuracy: 0.9364\n",
      "Epoch 120/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1715 - accuracy: 0.9371 - val_loss: 0.1763 - val_accuracy: 0.9364\n",
      "Epoch 121/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1710 - accuracy: 0.9372 - val_loss: 0.1760 - val_accuracy: 0.9366\n",
      "Epoch 122/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1707 - accuracy: 0.9373 - val_loss: 0.1756 - val_accuracy: 0.9368\n",
      "Epoch 123/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1702 - accuracy: 0.9376 - val_loss: 0.1749 - val_accuracy: 0.9369\n",
      "Epoch 124/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1698 - accuracy: 0.9376 - val_loss: 0.1748 - val_accuracy: 0.9368\n",
      "Epoch 125/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1694 - accuracy: 0.9377 - val_loss: 0.1743 - val_accuracy: 0.9374\n",
      "Epoch 126/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1690 - accuracy: 0.9380 - val_loss: 0.1742 - val_accuracy: 0.9372\n",
      "Epoch 127/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1687 - accuracy: 0.9380 - val_loss: 0.1738 - val_accuracy: 0.9374\n",
      "Epoch 128/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1683 - accuracy: 0.9381 - val_loss: 0.1733 - val_accuracy: 0.9373\n",
      "Epoch 129/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1679 - accuracy: 0.9382 - val_loss: 0.1730 - val_accuracy: 0.9373\n",
      "Epoch 130/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1675 - accuracy: 0.9383 - val_loss: 0.1729 - val_accuracy: 0.9378\n",
      "Epoch 131/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1672 - accuracy: 0.9384 - val_loss: 0.1725 - val_accuracy: 0.9375\n",
      "Epoch 132/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1668 - accuracy: 0.9385 - val_loss: 0.1719 - val_accuracy: 0.9383\n",
      "Epoch 133/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1665 - accuracy: 0.9387 - val_loss: 0.1721 - val_accuracy: 0.9379\n",
      "Epoch 134/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1661 - accuracy: 0.9389 - val_loss: 0.1713 - val_accuracy: 0.9383\n",
      "Epoch 135/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1658 - accuracy: 0.9390 - val_loss: 0.1715 - val_accuracy: 0.9382\n",
      "Epoch 136/500\n",
      "5625/5625 [==============================] - 0s 81us/step - loss: 0.1654 - accuracy: 0.9391 - val_loss: 0.1708 - val_accuracy: 0.9384\n",
      "Epoch 137/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1651 - accuracy: 0.9392 - val_loss: 0.1703 - val_accuracy: 0.9386\n",
      "Epoch 138/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1648 - accuracy: 0.9392 - val_loss: 0.1703 - val_accuracy: 0.9387\n",
      "Epoch 139/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1645 - accuracy: 0.9394 - val_loss: 0.1697 - val_accuracy: 0.9388\n",
      "Epoch 140/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1641 - accuracy: 0.9393 - val_loss: 0.1696 - val_accuracy: 0.9389\n",
      "Epoch 141/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1639 - accuracy: 0.9395 - val_loss: 0.1697 - val_accuracy: 0.9389\n",
      "Epoch 142/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1636 - accuracy: 0.9397 - val_loss: 0.1690 - val_accuracy: 0.9390\n",
      "Epoch 143/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1633 - accuracy: 0.9399 - val_loss: 0.1691 - val_accuracy: 0.9389\n",
      "Epoch 144/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1631 - accuracy: 0.9398 - val_loss: 0.1685 - val_accuracy: 0.9393\n",
      "Epoch 145/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1628 - accuracy: 0.9399 - val_loss: 0.1686 - val_accuracy: 0.9390\n",
      "Epoch 146/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1626 - accuracy: 0.9400 - val_loss: 0.1683 - val_accuracy: 0.9394\n",
      "Epoch 147/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1623 - accuracy: 0.9401 - val_loss: 0.1680 - val_accuracy: 0.9391\n",
      "Epoch 148/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1621 - accuracy: 0.9401 - val_loss: 0.1680 - val_accuracy: 0.9394\n",
      "Epoch 149/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1619 - accuracy: 0.9401 - val_loss: 0.1676 - val_accuracy: 0.9392\n",
      "Epoch 150/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1616 - accuracy: 0.9403 - val_loss: 0.1680 - val_accuracy: 0.9394\n",
      "Epoch 151/500\n",
      "5625/5625 [==============================] - 0s 81us/step - loss: 0.1615 - accuracy: 0.9403 - val_loss: 0.1671 - val_accuracy: 0.9399\n",
      "Epoch 152/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1612 - accuracy: 0.9403 - val_loss: 0.1673 - val_accuracy: 0.9395\n",
      "Epoch 153/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1610 - accuracy: 0.9405 - val_loss: 0.1671 - val_accuracy: 0.9395\n",
      "Epoch 154/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1608 - accuracy: 0.9407 - val_loss: 0.1667 - val_accuracy: 0.9398\n",
      "Epoch 155/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1606 - accuracy: 0.9408 - val_loss: 0.1665 - val_accuracy: 0.9397\n",
      "Epoch 156/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1603 - accuracy: 0.9407 - val_loss: 0.1669 - val_accuracy: 0.9399\n",
      "Epoch 157/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1602 - accuracy: 0.9409 - val_loss: 0.1664 - val_accuracy: 0.9398\n",
      "Epoch 158/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1599 - accuracy: 0.9410 - val_loss: 0.1660 - val_accuracy: 0.9402\n",
      "Epoch 159/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1598 - accuracy: 0.9409 - val_loss: 0.1657 - val_accuracy: 0.9402\n",
      "Epoch 160/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1596 - accuracy: 0.9409 - val_loss: 0.1654 - val_accuracy: 0.9403\n",
      "Epoch 161/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1593 - accuracy: 0.9410 - val_loss: 0.1657 - val_accuracy: 0.9402\n",
      "Epoch 162/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1592 - accuracy: 0.9412 - val_loss: 0.1654 - val_accuracy: 0.9401\n",
      "Epoch 163/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1590 - accuracy: 0.9412 - val_loss: 0.1652 - val_accuracy: 0.9405\n",
      "Epoch 164/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1588 - accuracy: 0.9413 - val_loss: 0.1647 - val_accuracy: 0.9408\n",
      "Epoch 165/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1586 - accuracy: 0.9414 - val_loss: 0.1653 - val_accuracy: 0.9408\n",
      "Epoch 166/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1583 - accuracy: 0.9414 - val_loss: 0.1649 - val_accuracy: 0.9406\n",
      "Epoch 167/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1582 - accuracy: 0.9415 - val_loss: 0.1644 - val_accuracy: 0.9405\n",
      "Epoch 168/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1579 - accuracy: 0.9416 - val_loss: 0.1647 - val_accuracy: 0.9405\n",
      "Epoch 169/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1578 - accuracy: 0.9416 - val_loss: 0.1646 - val_accuracy: 0.9405\n",
      "Epoch 170/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1575 - accuracy: 0.9416 - val_loss: 0.1639 - val_accuracy: 0.9405\n",
      "Epoch 171/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1574 - accuracy: 0.9417 - val_loss: 0.1640 - val_accuracy: 0.9407\n",
      "Epoch 172/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1572 - accuracy: 0.9420 - val_loss: 0.1640 - val_accuracy: 0.9403\n",
      "Epoch 173/500\n",
      "5625/5625 [==============================] - 1s 91us/step - loss: 0.1570 - accuracy: 0.9419 - val_loss: 0.1641 - val_accuracy: 0.9408\n",
      "Epoch 174/500\n",
      "5625/5625 [==============================] - 0s 84us/step - loss: 0.1569 - accuracy: 0.9417 - val_loss: 0.1633 - val_accuracy: 0.9407\n",
      "Epoch 175/500\n",
      "5625/5625 [==============================] - 0s 85us/step - loss: 0.1567 - accuracy: 0.9419 - val_loss: 0.1634 - val_accuracy: 0.9411\n",
      "Epoch 176/500\n",
      "5625/5625 [==============================] - 0s 80us/step - loss: 0.1565 - accuracy: 0.9420 - val_loss: 0.1636 - val_accuracy: 0.9411\n",
      "Epoch 177/500\n",
      "5625/5625 [==============================] - 0s 85us/step - loss: 0.1563 - accuracy: 0.9420 - val_loss: 0.1632 - val_accuracy: 0.9410\n",
      "Epoch 178/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1560 - accuracy: 0.9420 - val_loss: 0.1631 - val_accuracy: 0.9410\n",
      "Epoch 179/500\n",
      "5625/5625 [==============================] - 1s 89us/step - loss: 0.1559 - accuracy: 0.9421 - val_loss: 0.1628 - val_accuracy: 0.9408\n",
      "Epoch 180/500\n",
      "5625/5625 [==============================] - 0s 82us/step - loss: 0.1556 - accuracy: 0.9423 - val_loss: 0.1630 - val_accuracy: 0.9414\n",
      "Epoch 181/500\n",
      "5625/5625 [==============================] - 0s 81us/step - loss: 0.1555 - accuracy: 0.9424 - val_loss: 0.1628 - val_accuracy: 0.9416\n",
      "Epoch 182/500\n",
      "5625/5625 [==============================] - 0s 81us/step - loss: 0.1553 - accuracy: 0.9425 - val_loss: 0.1623 - val_accuracy: 0.9414\n",
      "Epoch 183/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1551 - accuracy: 0.9425 - val_loss: 0.1621 - val_accuracy: 0.9412\n",
      "Epoch 184/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1549 - accuracy: 0.9425 - val_loss: 0.1621 - val_accuracy: 0.9417\n",
      "Epoch 185/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1547 - accuracy: 0.9426 - val_loss: 0.1618 - val_accuracy: 0.9414\n",
      "Epoch 186/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1545 - accuracy: 0.9427 - val_loss: 0.1620 - val_accuracy: 0.9415\n",
      "Epoch 187/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1543 - accuracy: 0.9427 - val_loss: 0.1617 - val_accuracy: 0.9415\n",
      "Epoch 188/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1542 - accuracy: 0.9427 - val_loss: 0.1615 - val_accuracy: 0.9415\n",
      "Epoch 189/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1539 - accuracy: 0.9428 - val_loss: 0.1612 - val_accuracy: 0.9412\n",
      "Epoch 190/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1537 - accuracy: 0.9427 - val_loss: 0.1610 - val_accuracy: 0.9419\n",
      "Epoch 191/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1535 - accuracy: 0.9429 - val_loss: 0.1610 - val_accuracy: 0.9411\n",
      "Epoch 192/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1533 - accuracy: 0.9429 - val_loss: 0.1605 - val_accuracy: 0.9418\n",
      "Epoch 193/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1531 - accuracy: 0.9429 - val_loss: 0.1612 - val_accuracy: 0.9414\n",
      "Epoch 194/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1529 - accuracy: 0.9430 - val_loss: 0.1603 - val_accuracy: 0.9418\n",
      "Epoch 195/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1527 - accuracy: 0.9432 - val_loss: 0.1603 - val_accuracy: 0.9418\n",
      "Epoch 196/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1525 - accuracy: 0.9433 - val_loss: 0.1603 - val_accuracy: 0.9420\n",
      "Epoch 197/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1523 - accuracy: 0.9431 - val_loss: 0.1600 - val_accuracy: 0.9417\n",
      "Epoch 198/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1520 - accuracy: 0.9433 - val_loss: 0.1600 - val_accuracy: 0.9420\n",
      "Epoch 199/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1519 - accuracy: 0.9434 - val_loss: 0.1596 - val_accuracy: 0.9421\n",
      "Epoch 200/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1517 - accuracy: 0.9436 - val_loss: 0.1599 - val_accuracy: 0.9421\n",
      "Epoch 201/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1515 - accuracy: 0.9437 - val_loss: 0.1595 - val_accuracy: 0.9419\n",
      "Epoch 202/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1513 - accuracy: 0.9437 - val_loss: 0.1591 - val_accuracy: 0.9424\n",
      "Epoch 203/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1510 - accuracy: 0.9439 - val_loss: 0.1589 - val_accuracy: 0.9424\n",
      "Epoch 204/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1508 - accuracy: 0.9436 - val_loss: 0.1588 - val_accuracy: 0.9425\n",
      "Epoch 205/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1506 - accuracy: 0.9438 - val_loss: 0.1588 - val_accuracy: 0.9424\n",
      "Epoch 206/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1504 - accuracy: 0.9439 - val_loss: 0.1586 - val_accuracy: 0.9424\n",
      "Epoch 207/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1502 - accuracy: 0.9439 - val_loss: 0.1582 - val_accuracy: 0.9423\n",
      "Epoch 208/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1501 - accuracy: 0.9441 - val_loss: 0.1581 - val_accuracy: 0.9426\n",
      "Epoch 209/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1498 - accuracy: 0.9441 - val_loss: 0.1578 - val_accuracy: 0.9426\n",
      "Epoch 210/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1495 - accuracy: 0.9442 - val_loss: 0.1576 - val_accuracy: 0.9427\n",
      "Epoch 211/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1494 - accuracy: 0.9443 - val_loss: 0.1580 - val_accuracy: 0.9427\n",
      "Epoch 212/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1491 - accuracy: 0.9444 - val_loss: 0.1576 - val_accuracy: 0.9427\n",
      "Epoch 213/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1490 - accuracy: 0.9443 - val_loss: 0.1573 - val_accuracy: 0.9430\n",
      "Epoch 214/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1488 - accuracy: 0.9444 - val_loss: 0.1573 - val_accuracy: 0.9428\n",
      "Epoch 215/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1485 - accuracy: 0.9444 - val_loss: 0.1576 - val_accuracy: 0.9435\n",
      "Epoch 216/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1484 - accuracy: 0.9447 - val_loss: 0.1568 - val_accuracy: 0.9432\n",
      "Epoch 217/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1481 - accuracy: 0.9447 - val_loss: 0.1566 - val_accuracy: 0.9427\n",
      "Epoch 218/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1480 - accuracy: 0.9448 - val_loss: 0.1565 - val_accuracy: 0.9435\n",
      "Epoch 219/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1478 - accuracy: 0.9448 - val_loss: 0.1563 - val_accuracy: 0.9428\n",
      "Epoch 220/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1476 - accuracy: 0.9447 - val_loss: 0.1565 - val_accuracy: 0.9433\n",
      "Epoch 221/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1473 - accuracy: 0.9449 - val_loss: 0.1561 - val_accuracy: 0.9434\n",
      "Epoch 222/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1471 - accuracy: 0.9450 - val_loss: 0.1559 - val_accuracy: 0.9434\n",
      "Epoch 223/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1470 - accuracy: 0.9452 - val_loss: 0.1558 - val_accuracy: 0.9438\n",
      "Epoch 224/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1468 - accuracy: 0.9451 - val_loss: 0.1558 - val_accuracy: 0.9432\n",
      "Epoch 225/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1466 - accuracy: 0.9451 - val_loss: 0.1557 - val_accuracy: 0.9437\n",
      "Epoch 226/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1464 - accuracy: 0.9453 - val_loss: 0.1557 - val_accuracy: 0.9435\n",
      "Epoch 227/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1462 - accuracy: 0.9452 - val_loss: 0.1552 - val_accuracy: 0.9435\n",
      "Epoch 228/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1460 - accuracy: 0.9454 - val_loss: 0.1550 - val_accuracy: 0.9444\n",
      "Epoch 229/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1460 - accuracy: 0.9455 - val_loss: 0.1553 - val_accuracy: 0.9434\n",
      "Epoch 230/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1457 - accuracy: 0.9453 - val_loss: 0.1551 - val_accuracy: 0.9439\n",
      "Epoch 231/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1455 - accuracy: 0.9456 - val_loss: 0.1552 - val_accuracy: 0.9439\n",
      "Epoch 232/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1454 - accuracy: 0.9458 - val_loss: 0.1545 - val_accuracy: 0.9437\n",
      "Epoch 233/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1452 - accuracy: 0.9456 - val_loss: 0.1545 - val_accuracy: 0.9441\n",
      "Epoch 234/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1450 - accuracy: 0.9457 - val_loss: 0.1541 - val_accuracy: 0.9439\n",
      "Epoch 235/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1448 - accuracy: 0.9458 - val_loss: 0.1545 - val_accuracy: 0.9439\n",
      "Epoch 236/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1447 - accuracy: 0.9458 - val_loss: 0.1544 - val_accuracy: 0.9441\n",
      "Epoch 237/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1445 - accuracy: 0.9459 - val_loss: 0.1544 - val_accuracy: 0.9438\n",
      "Epoch 238/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1443 - accuracy: 0.9459 - val_loss: 0.1538 - val_accuracy: 0.9440\n",
      "Epoch 239/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1441 - accuracy: 0.9460 - val_loss: 0.1542 - val_accuracy: 0.9437\n",
      "Epoch 240/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1440 - accuracy: 0.9460 - val_loss: 0.1537 - val_accuracy: 0.9442\n",
      "Epoch 241/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1438 - accuracy: 0.9463 - val_loss: 0.1539 - val_accuracy: 0.9441\n",
      "Epoch 242/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1437 - accuracy: 0.9462 - val_loss: 0.1537 - val_accuracy: 0.9445\n",
      "Epoch 243/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1435 - accuracy: 0.9462 - val_loss: 0.1536 - val_accuracy: 0.9444\n",
      "Epoch 244/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1434 - accuracy: 0.9463 - val_loss: 0.1531 - val_accuracy: 0.9444\n",
      "Epoch 245/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1431 - accuracy: 0.9464 - val_loss: 0.1530 - val_accuracy: 0.9445\n",
      "Epoch 246/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1429 - accuracy: 0.9466 - val_loss: 0.1529 - val_accuracy: 0.9447\n",
      "Epoch 247/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1428 - accuracy: 0.9464 - val_loss: 0.1532 - val_accuracy: 0.9444\n",
      "Epoch 248/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1426 - accuracy: 0.9467 - val_loss: 0.1530 - val_accuracy: 0.9444\n",
      "Epoch 249/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1426 - accuracy: 0.9466 - val_loss: 0.1527 - val_accuracy: 0.9446\n",
      "Epoch 250/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1423 - accuracy: 0.9465 - val_loss: 0.1526 - val_accuracy: 0.9448\n",
      "Epoch 251/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1421 - accuracy: 0.9465 - val_loss: 0.1527 - val_accuracy: 0.9449\n",
      "Epoch 252/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1420 - accuracy: 0.9468 - val_loss: 0.1525 - val_accuracy: 0.9449\n",
      "Epoch 253/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1419 - accuracy: 0.9466 - val_loss: 0.1527 - val_accuracy: 0.9444\n",
      "Epoch 254/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1417 - accuracy: 0.9465 - val_loss: 0.1522 - val_accuracy: 0.9444\n",
      "Epoch 255/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1415 - accuracy: 0.9470 - val_loss: 0.1523 - val_accuracy: 0.9445\n",
      "Epoch 256/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1414 - accuracy: 0.9469 - val_loss: 0.1519 - val_accuracy: 0.9454\n",
      "Epoch 257/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1412 - accuracy: 0.9470 - val_loss: 0.1516 - val_accuracy: 0.9449\n",
      "Epoch 258/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1411 - accuracy: 0.9470 - val_loss: 0.1518 - val_accuracy: 0.9446\n",
      "Epoch 259/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1409 - accuracy: 0.9469 - val_loss: 0.1517 - val_accuracy: 0.9449\n",
      "Epoch 260/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1407 - accuracy: 0.9472 - val_loss: 0.1519 - val_accuracy: 0.9452\n",
      "Epoch 261/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1406 - accuracy: 0.9472 - val_loss: 0.1520 - val_accuracy: 0.9448\n",
      "Epoch 262/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1405 - accuracy: 0.9472 - val_loss: 0.1513 - val_accuracy: 0.9450\n",
      "Epoch 263/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1403 - accuracy: 0.9472 - val_loss: 0.1514 - val_accuracy: 0.9451\n",
      "Epoch 264/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1402 - accuracy: 0.9473 - val_loss: 0.1513 - val_accuracy: 0.9451\n",
      "Epoch 265/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1400 - accuracy: 0.9474 - val_loss: 0.1510 - val_accuracy: 0.9450\n",
      "Epoch 266/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1398 - accuracy: 0.9474 - val_loss: 0.1507 - val_accuracy: 0.9455\n",
      "Epoch 267/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1397 - accuracy: 0.9474 - val_loss: 0.1509 - val_accuracy: 0.9455\n",
      "Epoch 268/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1396 - accuracy: 0.9475 - val_loss: 0.1513 - val_accuracy: 0.9450\n",
      "Epoch 269/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1394 - accuracy: 0.9473 - val_loss: 0.1508 - val_accuracy: 0.9454\n",
      "Epoch 270/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1393 - accuracy: 0.9476 - val_loss: 0.1503 - val_accuracy: 0.9457\n",
      "Epoch 271/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1391 - accuracy: 0.9478 - val_loss: 0.1507 - val_accuracy: 0.9455\n",
      "Epoch 272/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1390 - accuracy: 0.9477 - val_loss: 0.1510 - val_accuracy: 0.9452\n",
      "Epoch 273/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1388 - accuracy: 0.9476 - val_loss: 0.1506 - val_accuracy: 0.9449\n",
      "Epoch 274/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1386 - accuracy: 0.9477 - val_loss: 0.1510 - val_accuracy: 0.9456\n",
      "Epoch 275/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1385 - accuracy: 0.9479 - val_loss: 0.1506 - val_accuracy: 0.9454\n",
      "Epoch 276/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1383 - accuracy: 0.9479 - val_loss: 0.1500 - val_accuracy: 0.9458\n",
      "Epoch 277/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1382 - accuracy: 0.9479 - val_loss: 0.1500 - val_accuracy: 0.9454\n",
      "Epoch 278/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1381 - accuracy: 0.9479 - val_loss: 0.1501 - val_accuracy: 0.9456\n",
      "Epoch 279/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1379 - accuracy: 0.9478 - val_loss: 0.1503 - val_accuracy: 0.9454\n",
      "Epoch 280/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1378 - accuracy: 0.9479 - val_loss: 0.1499 - val_accuracy: 0.9456\n",
      "Epoch 281/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1376 - accuracy: 0.9478 - val_loss: 0.1501 - val_accuracy: 0.9453\n",
      "Epoch 282/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1375 - accuracy: 0.9479 - val_loss: 0.1497 - val_accuracy: 0.9456\n",
      "Epoch 283/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1374 - accuracy: 0.9481 - val_loss: 0.1495 - val_accuracy: 0.9459\n",
      "Epoch 284/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1373 - accuracy: 0.9483 - val_loss: 0.1499 - val_accuracy: 0.9458\n",
      "Epoch 285/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1371 - accuracy: 0.9481 - val_loss: 0.1494 - val_accuracy: 0.9458\n",
      "Epoch 286/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1370 - accuracy: 0.9482 - val_loss: 0.1493 - val_accuracy: 0.9454\n",
      "Epoch 287/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1368 - accuracy: 0.9483 - val_loss: 0.1492 - val_accuracy: 0.9458\n",
      "Epoch 288/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1367 - accuracy: 0.9483 - val_loss: 0.1497 - val_accuracy: 0.9451\n",
      "Epoch 289/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1365 - accuracy: 0.9484 - val_loss: 0.1497 - val_accuracy: 0.9456\n",
      "Epoch 290/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1364 - accuracy: 0.9482 - val_loss: 0.1490 - val_accuracy: 0.9456\n",
      "Epoch 291/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1362 - accuracy: 0.9487 - val_loss: 0.1494 - val_accuracy: 0.9454\n",
      "Epoch 292/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1361 - accuracy: 0.9485 - val_loss: 0.1489 - val_accuracy: 0.9460\n",
      "Epoch 293/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1360 - accuracy: 0.9486 - val_loss: 0.1493 - val_accuracy: 0.9458\n",
      "Epoch 294/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1358 - accuracy: 0.9484 - val_loss: 0.1493 - val_accuracy: 0.9457\n",
      "Epoch 295/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1359 - accuracy: 0.9485 - val_loss: 0.1494 - val_accuracy: 0.9461\n",
      "Epoch 296/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1356 - accuracy: 0.9489 - val_loss: 0.1487 - val_accuracy: 0.9459\n",
      "Epoch 297/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1354 - accuracy: 0.9486 - val_loss: 0.1490 - val_accuracy: 0.9461\n",
      "Epoch 298/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1355 - accuracy: 0.9486 - val_loss: 0.1485 - val_accuracy: 0.9461\n",
      "Epoch 299/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1352 - accuracy: 0.9488 - val_loss: 0.1487 - val_accuracy: 0.9462\n",
      "Epoch 300/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1352 - accuracy: 0.9488 - val_loss: 0.1486 - val_accuracy: 0.9458\n",
      "Epoch 301/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1350 - accuracy: 0.9486 - val_loss: 0.1483 - val_accuracy: 0.9463\n",
      "Epoch 302/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1349 - accuracy: 0.9492 - val_loss: 0.1482 - val_accuracy: 0.9461\n",
      "Epoch 303/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1348 - accuracy: 0.9489 - val_loss: 0.1486 - val_accuracy: 0.9461\n",
      "Epoch 304/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1346 - accuracy: 0.9490 - val_loss: 0.1485 - val_accuracy: 0.9457\n",
      "Epoch 305/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1345 - accuracy: 0.9491 - val_loss: 0.1481 - val_accuracy: 0.9462\n",
      "Epoch 306/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1343 - accuracy: 0.9489 - val_loss: 0.1479 - val_accuracy: 0.9464\n",
      "Epoch 307/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1342 - accuracy: 0.9492 - val_loss: 0.1480 - val_accuracy: 0.9463\n",
      "Epoch 308/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1341 - accuracy: 0.9492 - val_loss: 0.1478 - val_accuracy: 0.9463\n",
      "Epoch 309/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1340 - accuracy: 0.9492 - val_loss: 0.1476 - val_accuracy: 0.9465\n",
      "Epoch 310/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1338 - accuracy: 0.9491 - val_loss: 0.1478 - val_accuracy: 0.9460\n",
      "Epoch 311/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1338 - accuracy: 0.9491 - val_loss: 0.1475 - val_accuracy: 0.9464\n",
      "Epoch 312/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1336 - accuracy: 0.9490 - val_loss: 0.1475 - val_accuracy: 0.9463\n",
      "Epoch 313/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1336 - accuracy: 0.9493 - val_loss: 0.1475 - val_accuracy: 0.9466\n",
      "Epoch 314/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1334 - accuracy: 0.9492 - val_loss: 0.1481 - val_accuracy: 0.9460\n",
      "Epoch 315/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1333 - accuracy: 0.9494 - val_loss: 0.1476 - val_accuracy: 0.9465\n",
      "Epoch 316/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1332 - accuracy: 0.9492 - val_loss: 0.1473 - val_accuracy: 0.9464\n",
      "Epoch 317/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1331 - accuracy: 0.9496 - val_loss: 0.1474 - val_accuracy: 0.9463\n",
      "Epoch 318/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1329 - accuracy: 0.9493 - val_loss: 0.1472 - val_accuracy: 0.9468\n",
      "Epoch 319/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1328 - accuracy: 0.9495 - val_loss: 0.1475 - val_accuracy: 0.9462\n",
      "Epoch 320/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1327 - accuracy: 0.9496 - val_loss: 0.1471 - val_accuracy: 0.9469\n",
      "Epoch 321/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1326 - accuracy: 0.9495 - val_loss: 0.1473 - val_accuracy: 0.9463\n",
      "Epoch 322/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1325 - accuracy: 0.9495 - val_loss: 0.1471 - val_accuracy: 0.9467\n",
      "Epoch 323/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1324 - accuracy: 0.9497 - val_loss: 0.1472 - val_accuracy: 0.9467\n",
      "Epoch 324/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1322 - accuracy: 0.9497 - val_loss: 0.1469 - val_accuracy: 0.9466\n",
      "Epoch 325/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1321 - accuracy: 0.9496 - val_loss: 0.1477 - val_accuracy: 0.9460\n",
      "Epoch 326/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1320 - accuracy: 0.9496 - val_loss: 0.1468 - val_accuracy: 0.9466\n",
      "Epoch 327/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1319 - accuracy: 0.9500 - val_loss: 0.1468 - val_accuracy: 0.9460\n",
      "Epoch 328/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1318 - accuracy: 0.9496 - val_loss: 0.1468 - val_accuracy: 0.9466\n",
      "Epoch 329/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1317 - accuracy: 0.9497 - val_loss: 0.1469 - val_accuracy: 0.9465\n",
      "Epoch 330/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1316 - accuracy: 0.9500 - val_loss: 0.1469 - val_accuracy: 0.9468\n",
      "Epoch 331/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1315 - accuracy: 0.9498 - val_loss: 0.1470 - val_accuracy: 0.9466\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1313 - accuracy: 0.9499 - val_loss: 0.1469 - val_accuracy: 0.9467\n",
      "Epoch 333/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1312 - accuracy: 0.9497 - val_loss: 0.1465 - val_accuracy: 0.9466\n",
      "Epoch 334/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1310 - accuracy: 0.9499 - val_loss: 0.1472 - val_accuracy: 0.9464\n",
      "Epoch 335/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1310 - accuracy: 0.9499 - val_loss: 0.1463 - val_accuracy: 0.9471\n",
      "Epoch 336/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1309 - accuracy: 0.9500 - val_loss: 0.1465 - val_accuracy: 0.9464\n",
      "Epoch 337/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1308 - accuracy: 0.9501 - val_loss: 0.1463 - val_accuracy: 0.9467\n",
      "Epoch 338/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1307 - accuracy: 0.9499 - val_loss: 0.1466 - val_accuracy: 0.9465\n",
      "Epoch 339/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1306 - accuracy: 0.9501 - val_loss: 0.1466 - val_accuracy: 0.9462\n",
      "Epoch 340/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1305 - accuracy: 0.9502 - val_loss: 0.1461 - val_accuracy: 0.9471\n",
      "Epoch 341/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1303 - accuracy: 0.9501 - val_loss: 0.1459 - val_accuracy: 0.9469\n",
      "Epoch 342/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1302 - accuracy: 0.9503 - val_loss: 0.1466 - val_accuracy: 0.9464\n",
      "Epoch 343/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1302 - accuracy: 0.9503 - val_loss: 0.1469 - val_accuracy: 0.9469\n",
      "Epoch 344/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1301 - accuracy: 0.9505 - val_loss: 0.1461 - val_accuracy: 0.9471\n",
      "Epoch 345/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1300 - accuracy: 0.9502 - val_loss: 0.1461 - val_accuracy: 0.9462\n",
      "Epoch 346/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1298 - accuracy: 0.9503 - val_loss: 0.1457 - val_accuracy: 0.9468\n",
      "Epoch 347/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1296 - accuracy: 0.9503 - val_loss: 0.1461 - val_accuracy: 0.9466\n",
      "Epoch 348/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1296 - accuracy: 0.9504 - val_loss: 0.1458 - val_accuracy: 0.9469\n",
      "Epoch 349/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1296 - accuracy: 0.9502 - val_loss: 0.1458 - val_accuracy: 0.9467\n",
      "Epoch 350/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1294 - accuracy: 0.9503 - val_loss: 0.1464 - val_accuracy: 0.9466\n",
      "Epoch 351/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1292 - accuracy: 0.9505 - val_loss: 0.1457 - val_accuracy: 0.9470\n",
      "Epoch 352/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1292 - accuracy: 0.9504 - val_loss: 0.1460 - val_accuracy: 0.9470\n",
      "Epoch 353/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1292 - accuracy: 0.9505 - val_loss: 0.1455 - val_accuracy: 0.9465\n",
      "Epoch 354/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1290 - accuracy: 0.9507 - val_loss: 0.1459 - val_accuracy: 0.9467\n",
      "Epoch 355/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1289 - accuracy: 0.9507 - val_loss: 0.1458 - val_accuracy: 0.9469\n",
      "Epoch 356/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1287 - accuracy: 0.9508 - val_loss: 0.1456 - val_accuracy: 0.9469\n",
      "Epoch 357/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1288 - accuracy: 0.9505 - val_loss: 0.1456 - val_accuracy: 0.9470\n",
      "Epoch 358/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1286 - accuracy: 0.9506 - val_loss: 0.1457 - val_accuracy: 0.9467\n",
      "Epoch 359/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1286 - accuracy: 0.9508 - val_loss: 0.1459 - val_accuracy: 0.9469\n",
      "Epoch 360/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1284 - accuracy: 0.9507 - val_loss: 0.1452 - val_accuracy: 0.9471\n",
      "Epoch 361/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1284 - accuracy: 0.9509 - val_loss: 0.1453 - val_accuracy: 0.9471\n",
      "Epoch 362/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1283 - accuracy: 0.9508 - val_loss: 0.1451 - val_accuracy: 0.9472\n",
      "Epoch 363/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1282 - accuracy: 0.9508 - val_loss: 0.1452 - val_accuracy: 0.9468\n",
      "Epoch 364/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1281 - accuracy: 0.9510 - val_loss: 0.1452 - val_accuracy: 0.9472\n",
      "Epoch 365/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1279 - accuracy: 0.9508 - val_loss: 0.1455 - val_accuracy: 0.9466\n",
      "Epoch 366/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1279 - accuracy: 0.9511 - val_loss: 0.1448 - val_accuracy: 0.9474\n",
      "Epoch 367/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1277 - accuracy: 0.9511 - val_loss: 0.1457 - val_accuracy: 0.9470\n",
      "Epoch 368/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1275 - accuracy: 0.9510 - val_loss: 0.1452 - val_accuracy: 0.9470\n",
      "Epoch 369/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1276 - accuracy: 0.9510 - val_loss: 0.1450 - val_accuracy: 0.9472\n",
      "Epoch 370/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1274 - accuracy: 0.9512 - val_loss: 0.1454 - val_accuracy: 0.9470\n",
      "Epoch 371/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1274 - accuracy: 0.9510 - val_loss: 0.1450 - val_accuracy: 0.9473\n",
      "Epoch 372/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1272 - accuracy: 0.9512 - val_loss: 0.1452 - val_accuracy: 0.9473\n",
      "Epoch 373/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1271 - accuracy: 0.9512 - val_loss: 0.1449 - val_accuracy: 0.9475\n",
      "Epoch 374/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1270 - accuracy: 0.9512 - val_loss: 0.1449 - val_accuracy: 0.9473\n",
      "Epoch 375/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1270 - accuracy: 0.9511 - val_loss: 0.1451 - val_accuracy: 0.9472\n",
      "Epoch 376/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1268 - accuracy: 0.9511 - val_loss: 0.1449 - val_accuracy: 0.9470\n",
      "Epoch 377/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1267 - accuracy: 0.9513 - val_loss: 0.1447 - val_accuracy: 0.9471\n",
      "Epoch 378/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1266 - accuracy: 0.9513 - val_loss: 0.1449 - val_accuracy: 0.9473\n",
      "Epoch 379/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1265 - accuracy: 0.9513 - val_loss: 0.1450 - val_accuracy: 0.9474\n",
      "Epoch 380/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1265 - accuracy: 0.9513 - val_loss: 0.1448 - val_accuracy: 0.9474\n",
      "Epoch 381/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1264 - accuracy: 0.9512 - val_loss: 0.1455 - val_accuracy: 0.9471\n",
      "Epoch 382/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1263 - accuracy: 0.9514 - val_loss: 0.1453 - val_accuracy: 0.9466\n",
      "Epoch 383/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1262 - accuracy: 0.9515 - val_loss: 0.1449 - val_accuracy: 0.9471\n",
      "Epoch 384/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1262 - accuracy: 0.9516 - val_loss: 0.1456 - val_accuracy: 0.9467\n",
      "Epoch 385/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1260 - accuracy: 0.9515 - val_loss: 0.1451 - val_accuracy: 0.9471\n",
      "Epoch 386/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1259 - accuracy: 0.9517 - val_loss: 0.1450 - val_accuracy: 0.9469\n",
      "Epoch 387/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1259 - accuracy: 0.9514 - val_loss: 0.1446 - val_accuracy: 0.9473\n",
      "Epoch 388/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1257 - accuracy: 0.9516 - val_loss: 0.1445 - val_accuracy: 0.9470\n",
      "Epoch 389/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1256 - accuracy: 0.9514 - val_loss: 0.1447 - val_accuracy: 0.9472\n",
      "Epoch 390/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1256 - accuracy: 0.9514 - val_loss: 0.1446 - val_accuracy: 0.9469\n",
      "Epoch 391/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1254 - accuracy: 0.9515 - val_loss: 0.1447 - val_accuracy: 0.9469\n",
      "Epoch 392/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1254 - accuracy: 0.9517 - val_loss: 0.1452 - val_accuracy: 0.9469\n",
      "Epoch 393/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1254 - accuracy: 0.9517 - val_loss: 0.1445 - val_accuracy: 0.9467\n",
      "Epoch 394/500\n",
      "5625/5625 [==============================] - 0s 70us/step - loss: 0.1253 - accuracy: 0.9518 - val_loss: 0.1449 - val_accuracy: 0.9465\n",
      "Epoch 395/500\n",
      "5625/5625 [==============================] - 0s 72us/step - loss: 0.1251 - accuracy: 0.9518 - val_loss: 0.1454 - val_accuracy: 0.9466\n",
      "Epoch 396/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1250 - accuracy: 0.9519 - val_loss: 0.1441 - val_accuracy: 0.9473\n",
      "Epoch 397/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1249 - accuracy: 0.9520 - val_loss: 0.1451 - val_accuracy: 0.9472\n",
      "Epoch 398/500\n",
      "5625/5625 [==============================] - 1s 116us/step - loss: 0.1249 - accuracy: 0.9519 - val_loss: 0.1448 - val_accuracy: 0.9472\n",
      "Epoch 399/500\n",
      "5625/5625 [==============================] - 0s 83us/step - loss: 0.1248 - accuracy: 0.9520 - val_loss: 0.1448 - val_accuracy: 0.9471\n",
      "Epoch 400/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1246 - accuracy: 0.9519 - val_loss: 0.1448 - val_accuracy: 0.9469\n",
      "Epoch 401/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1246 - accuracy: 0.9519 - val_loss: 0.1444 - val_accuracy: 0.9474\n",
      "Epoch 402/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1245 - accuracy: 0.9522 - val_loss: 0.1441 - val_accuracy: 0.9472\n",
      "Epoch 403/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1245 - accuracy: 0.9518 - val_loss: 0.1442 - val_accuracy: 0.9474\n",
      "Epoch 404/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1243 - accuracy: 0.9519 - val_loss: 0.1441 - val_accuracy: 0.9469\n",
      "Epoch 405/500\n",
      "5625/5625 [==============================] - 0s 80us/step - loss: 0.1243 - accuracy: 0.9520 - val_loss: 0.1448 - val_accuracy: 0.9467\n",
      "Epoch 406/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1242 - accuracy: 0.9519 - val_loss: 0.1440 - val_accuracy: 0.9471\n",
      "Epoch 407/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1241 - accuracy: 0.9521 - val_loss: 0.1444 - val_accuracy: 0.9473\n",
      "Epoch 408/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1239 - accuracy: 0.9522 - val_loss: 0.1446 - val_accuracy: 0.9470\n",
      "Epoch 409/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1240 - accuracy: 0.9519 - val_loss: 0.1449 - val_accuracy: 0.9467\n",
      "Epoch 410/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1238 - accuracy: 0.9522 - val_loss: 0.1444 - val_accuracy: 0.9468\n",
      "Epoch 411/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1237 - accuracy: 0.9521 - val_loss: 0.1444 - val_accuracy: 0.9468\n",
      "Epoch 412/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1236 - accuracy: 0.9522 - val_loss: 0.1441 - val_accuracy: 0.9475\n",
      "Epoch 413/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1235 - accuracy: 0.9524 - val_loss: 0.1442 - val_accuracy: 0.9471\n",
      "Epoch 414/500\n",
      "5625/5625 [==============================] - 0s 74us/step - loss: 0.1234 - accuracy: 0.9522 - val_loss: 0.1442 - val_accuracy: 0.9472\n",
      "Epoch 415/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1233 - accuracy: 0.9526 - val_loss: 0.1448 - val_accuracy: 0.9469\n",
      "Epoch 416/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1233 - accuracy: 0.9524 - val_loss: 0.1442 - val_accuracy: 0.9471\n",
      "Epoch 417/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1233 - accuracy: 0.9522 - val_loss: 0.1442 - val_accuracy: 0.9470\n",
      "Epoch 418/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1231 - accuracy: 0.9523 - val_loss: 0.1441 - val_accuracy: 0.9473\n",
      "Epoch 419/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1231 - accuracy: 0.9525 - val_loss: 0.1439 - val_accuracy: 0.9473\n",
      "Epoch 420/500\n",
      "5625/5625 [==============================] - 0s 77us/step - loss: 0.1230 - accuracy: 0.9525 - val_loss: 0.1439 - val_accuracy: 0.9472\n",
      "Epoch 421/500\n",
      "5625/5625 [==============================] - 0s 71us/step - loss: 0.1227 - accuracy: 0.9526 - val_loss: 0.1447 - val_accuracy: 0.9467\n",
      "Epoch 422/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1227 - accuracy: 0.9524 - val_loss: 0.1444 - val_accuracy: 0.9471\n",
      "Epoch 423/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1228 - accuracy: 0.9526 - val_loss: 0.1441 - val_accuracy: 0.9467\n",
      "Epoch 424/500\n",
      "5625/5625 [==============================] - 0s 78us/step - loss: 0.1226 - accuracy: 0.9527 - val_loss: 0.1442 - val_accuracy: 0.9475\n",
      "Epoch 425/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1226 - accuracy: 0.9525 - val_loss: 0.1448 - val_accuracy: 0.9470\n",
      "Epoch 426/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1224 - accuracy: 0.9526 - val_loss: 0.1441 - val_accuracy: 0.9472\n",
      "Epoch 427/500\n",
      "5625/5625 [==============================] - 0s 76us/step - loss: 0.1224 - accuracy: 0.9525 - val_loss: 0.1441 - val_accuracy: 0.9469\n",
      "Epoch 428/500\n",
      "5625/5625 [==============================] - 0s 75us/step - loss: 0.1223 - accuracy: 0.9528 - val_loss: 0.1440 - val_accuracy: 0.9475\n",
      "Epoch 429/500\n",
      "5625/5625 [==============================] - 0s 80us/step - loss: 0.1222 - accuracy: 0.9527 - val_loss: 0.1443 - val_accuracy: 0.9472\n",
      "Epoch 430/500\n",
      "5625/5625 [==============================] - 0s 69us/step - loss: 0.1221 - accuracy: 0.9527 - val_loss: 0.1444 - val_accuracy: 0.9463\n",
      "Epoch 431/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1221 - accuracy: 0.9529 - val_loss: 0.1441 - val_accuracy: 0.9471\n",
      "Epoch 432/500\n",
      "5625/5625 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.95 - 0s 70us/step - loss: 0.1219 - accuracy: 0.9531 - val_loss: 0.1444 - val_accuracy: 0.9472\n",
      "Epoch 433/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1220 - accuracy: 0.9528 - val_loss: 0.1441 - val_accuracy: 0.9470\n",
      "Epoch 434/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1218 - accuracy: 0.9527 - val_loss: 0.1436 - val_accuracy: 0.9473\n",
      "Epoch 435/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1216 - accuracy: 0.9530 - val_loss: 0.1439 - val_accuracy: 0.9469\n",
      "Epoch 436/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1218 - accuracy: 0.9528 - val_loss: 0.1441 - val_accuracy: 0.9469\n",
      "Epoch 437/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1216 - accuracy: 0.9529 - val_loss: 0.1439 - val_accuracy: 0.9469\n",
      "Epoch 438/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1215 - accuracy: 0.9527 - val_loss: 0.1441 - val_accuracy: 0.9470\n",
      "Epoch 439/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1214 - accuracy: 0.9530 - val_loss: 0.1440 - val_accuracy: 0.9470\n",
      "Epoch 440/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1213 - accuracy: 0.9530 - val_loss: 0.1439 - val_accuracy: 0.9470\n",
      "Epoch 441/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1212 - accuracy: 0.9531 - val_loss: 0.1446 - val_accuracy: 0.9468\n",
      "Epoch 442/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1212 - accuracy: 0.9531 - val_loss: 0.1442 - val_accuracy: 0.9468\n",
      "Epoch 443/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1211 - accuracy: 0.9530 - val_loss: 0.1442 - val_accuracy: 0.9471\n",
      "Epoch 444/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1210 - accuracy: 0.9531 - val_loss: 0.1441 - val_accuracy: 0.9475\n",
      "Epoch 445/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1208 - accuracy: 0.9530 - val_loss: 0.1437 - val_accuracy: 0.9475\n",
      "Epoch 446/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1208 - accuracy: 0.9532 - val_loss: 0.1438 - val_accuracy: 0.9476\n",
      "Epoch 447/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1207 - accuracy: 0.9529 - val_loss: 0.1438 - val_accuracy: 0.9473\n",
      "Epoch 448/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1207 - accuracy: 0.9532 - val_loss: 0.1441 - val_accuracy: 0.9471\n",
      "Epoch 449/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1206 - accuracy: 0.9531 - val_loss: 0.1443 - val_accuracy: 0.9471\n",
      "Epoch 450/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1204 - accuracy: 0.9532 - val_loss: 0.1445 - val_accuracy: 0.9472\n",
      "Epoch 451/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1204 - accuracy: 0.9534 - val_loss: 0.1446 - val_accuracy: 0.9472\n",
      "Epoch 452/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1203 - accuracy: 0.9533 - val_loss: 0.1446 - val_accuracy: 0.9471\n",
      "Epoch 453/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1203 - accuracy: 0.9532 - val_loss: 0.1436 - val_accuracy: 0.9472\n",
      "Epoch 454/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1202 - accuracy: 0.9534 - val_loss: 0.1437 - val_accuracy: 0.9474\n",
      "Epoch 455/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1201 - accuracy: 0.9536 - val_loss: 0.1442 - val_accuracy: 0.9471\n",
      "Epoch 456/500\n",
      "5625/5625 [==============================] - 0s 69us/step - loss: 0.1200 - accuracy: 0.9535 - val_loss: 0.1442 - val_accuracy: 0.9469\n",
      "Epoch 457/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1200 - accuracy: 0.9533 - val_loss: 0.1441 - val_accuracy: 0.9471\n",
      "Epoch 458/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1199 - accuracy: 0.9534 - val_loss: 0.1437 - val_accuracy: 0.9472\n",
      "Epoch 459/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1198 - accuracy: 0.9535 - val_loss: 0.1438 - val_accuracy: 0.9475\n",
      "Epoch 460/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1198 - accuracy: 0.9534 - val_loss: 0.1437 - val_accuracy: 0.9474\n",
      "Epoch 461/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1196 - accuracy: 0.9535 - val_loss: 0.1436 - val_accuracy: 0.9474\n",
      "Epoch 462/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1196 - accuracy: 0.9535 - val_loss: 0.1441 - val_accuracy: 0.9469\n",
      "Epoch 463/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1195 - accuracy: 0.9534 - val_loss: 0.1439 - val_accuracy: 0.9471\n",
      "Epoch 464/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1194 - accuracy: 0.9535 - val_loss: 0.1445 - val_accuracy: 0.9471\n",
      "Epoch 465/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1194 - accuracy: 0.9537 - val_loss: 0.1443 - val_accuracy: 0.9472\n",
      "Epoch 466/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1194 - accuracy: 0.9537 - val_loss: 0.1436 - val_accuracy: 0.9471\n",
      "Epoch 467/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1191 - accuracy: 0.9536 - val_loss: 0.1451 - val_accuracy: 0.9466\n",
      "Epoch 468/500\n",
      "5625/5625 [==============================] - 0s 69us/step - loss: 0.1192 - accuracy: 0.9538 - val_loss: 0.1440 - val_accuracy: 0.9471\n",
      "Epoch 469/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1190 - accuracy: 0.9536 - val_loss: 0.1441 - val_accuracy: 0.9466\n",
      "Epoch 470/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1189 - accuracy: 0.9539 - val_loss: 0.1442 - val_accuracy: 0.9470\n",
      "Epoch 471/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1189 - accuracy: 0.9537 - val_loss: 0.1447 - val_accuracy: 0.9466\n",
      "Epoch 472/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1188 - accuracy: 0.9538 - val_loss: 0.1440 - val_accuracy: 0.9468\n",
      "Epoch 473/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1186 - accuracy: 0.9538 - val_loss: 0.1438 - val_accuracy: 0.9468\n",
      "Epoch 474/500\n",
      "5625/5625 [==============================] - 0s 73us/step - loss: 0.1186 - accuracy: 0.9540 - val_loss: 0.1450 - val_accuracy: 0.9472\n",
      "Epoch 475/500\n",
      "5625/5625 [==============================] - 0s 69us/step - loss: 0.1185 - accuracy: 0.9538 - val_loss: 0.1440 - val_accuracy: 0.9469\n",
      "Epoch 476/500\n",
      "5625/5625 [==============================] - 0s 69us/step - loss: 0.1184 - accuracy: 0.9541 - val_loss: 0.1445 - val_accuracy: 0.9473\n",
      "Epoch 477/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1185 - accuracy: 0.9538 - val_loss: 0.1440 - val_accuracy: 0.9478\n",
      "Epoch 478/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1183 - accuracy: 0.9539 - val_loss: 0.1438 - val_accuracy: 0.9474\n",
      "Epoch 479/500\n",
      "5625/5625 [==============================] - 0s 69us/step - loss: 0.1183 - accuracy: 0.9538 - val_loss: 0.1440 - val_accuracy: 0.9470\n",
      "Epoch 480/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1181 - accuracy: 0.9540 - val_loss: 0.1445 - val_accuracy: 0.9470\n",
      "Epoch 481/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1181 - accuracy: 0.9540 - val_loss: 0.1439 - val_accuracy: 0.9469\n",
      "Epoch 482/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1180 - accuracy: 0.9539 - val_loss: 0.1442 - val_accuracy: 0.9466\n",
      "Epoch 483/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1181 - accuracy: 0.9539 - val_loss: 0.1436 - val_accuracy: 0.9474\n",
      "Epoch 484/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1179 - accuracy: 0.9543 - val_loss: 0.1438 - val_accuracy: 0.9472\n",
      "Epoch 485/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1178 - accuracy: 0.9540 - val_loss: 0.1447 - val_accuracy: 0.9469\n",
      "Epoch 486/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1177 - accuracy: 0.9539 - val_loss: 0.1437 - val_accuracy: 0.9473\n",
      "Epoch 487/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1177 - accuracy: 0.9542 - val_loss: 0.1442 - val_accuracy: 0.9468\n",
      "Epoch 488/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1175 - accuracy: 0.9542 - val_loss: 0.1456 - val_accuracy: 0.9468\n",
      "Epoch 489/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1174 - accuracy: 0.9542 - val_loss: 0.1440 - val_accuracy: 0.9470\n",
      "Epoch 490/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1173 - accuracy: 0.9543 - val_loss: 0.1446 - val_accuracy: 0.9473\n",
      "Epoch 491/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1173 - accuracy: 0.9541 - val_loss: 0.1443 - val_accuracy: 0.9472\n",
      "Epoch 492/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1172 - accuracy: 0.9542 - val_loss: 0.1440 - val_accuracy: 0.9471\n",
      "Epoch 493/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1171 - accuracy: 0.9541 - val_loss: 0.1445 - val_accuracy: 0.9471\n",
      "Epoch 494/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1171 - accuracy: 0.9542 - val_loss: 0.1438 - val_accuracy: 0.9473\n",
      "Epoch 495/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1170 - accuracy: 0.9545 - val_loss: 0.1439 - val_accuracy: 0.9471\n",
      "Epoch 496/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1169 - accuracy: 0.9544 - val_loss: 0.1440 - val_accuracy: 0.9467\n",
      "Epoch 497/500\n",
      "5625/5625 [==============================] - 0s 68us/step - loss: 0.1169 - accuracy: 0.9545 - val_loss: 0.1442 - val_accuracy: 0.9466\n",
      "Epoch 498/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1168 - accuracy: 0.9543 - val_loss: 0.1447 - val_accuracy: 0.9471\n",
      "Epoch 499/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1166 - accuracy: 0.9545 - val_loss: 0.1437 - val_accuracy: 0.9472\n",
      "Epoch 500/500\n",
      "5625/5625 [==============================] - 0s 67us/step - loss: 0.1166 - accuracy: 0.9546 - val_loss: 0.1449 - val_accuracy: 0.9467\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_367 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_368 (Dense)            (None, 240)               6000      \n",
      "_________________________________________________________________\n",
      "dense_369 (Dense)            (None, 240)               57840     \n",
      "_________________________________________________________________\n",
      "dense_370 (Dense)            (None, 240)               57840     \n",
      "_________________________________________________________________\n",
      "dense_371 (Dense)            (None, 240)               57840     \n",
      "_________________________________________________________________\n",
      "dense_372 (Dense)            (None, 51)                12291     \n",
      "=================================================================\n",
      "Total params: 192,411\n",
      "Trainable params: 192,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model trained in 205.47789454460144 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "inputs = x_d5.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d5.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "train_input_d5 = inputs[:,2:]\n",
    "train_output_d5 = targets\n",
    "\n",
    "x_train_d5, x_test_d5, Y_train_d5, Y_test_d5 = train_test_split(train_input_d5, train_output_d5, train_size=0.75, shuffle=True)\n",
    "\n",
    "model_d5 = compile_FFNN_cv_model_DepthFive(5)\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d5.fit(\n",
    "    x=x_train_d5,\n",
    "    y=Y_train_d5,\n",
    "    validation_split=.25,\n",
    "    epochs=500\n",
    ")\n",
    "model_d5.summary()\n",
    "print(\"Model trained in %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:   4065\n",
      "False positive:  1702\n",
      "True negative:   114457\n",
      "False negative:  4776\n"
     ]
    }
   ],
   "source": [
    "predictions_d5 = model_d5.predict(x_test_d5)\n",
    "\n",
    "y_pred = predictions_d5\n",
    "y_test = Y_test_d5\n",
    "\n",
    "y_pred[y_pred>=.5]=1 \n",
    "y_pred[y_pred<.5]=0\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(y_pred)-1):\n",
    "    for j in range(len(y_test[0])-1):\n",
    "        if y_test[i][j] == y_pred[i][j] and y_test[i][j] == 1:\n",
    "            TP += 1\n",
    "        if y_pred[i][j] == 1 and y_test[i][j] != y_pred[i][j]:\n",
    "            FP += 1\n",
    "        if y_test[i][j] == y_pred[i][j] and y_test[i][j] == 0:\n",
    "            TN += 1\n",
    "        if y_pred[i][j] == 0 and y_test[i][j] != y_pred[i][j]:\n",
    "            FN += 1\n",
    "            \n",
    "print(\"True positive:   \" + str(TP) + \"\\nFalse positive:  \" + str(FP) + \"\\nTrue negative:   \" + str(TN) + \"\\nFalse negative:  \" + str(FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Epoch vs. Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss (MSE)')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 5 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Epoch vs. Accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 5 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d5.save(\"model_d5_v3-10k.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.4216 - accuracy: 0.8535 - val_loss: 0.2670 - val_accuracy: 0.9246\n",
      "Epoch 2/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2494 - accuracy: 0.9307 - val_loss: 0.2655 - val_accuracy: 0.9246\n",
      "Epoch 3/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2488 - accuracy: 0.9307 - val_loss: 0.2651 - val_accuracy: 0.9246\n",
      "Epoch 4/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2485 - accuracy: 0.9307 - val_loss: 0.2649 - val_accuracy: 0.9246\n",
      "Epoch 5/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2481 - accuracy: 0.9307 - val_loss: 0.2643 - val_accuracy: 0.9246\n",
      "Epoch 6/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2478 - accuracy: 0.9307 - val_loss: 0.2641 - val_accuracy: 0.9246\n",
      "Epoch 7/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2474 - accuracy: 0.9307 - val_loss: 0.2636 - val_accuracy: 0.9246\n",
      "Epoch 8/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2470 - accuracy: 0.9307 - val_loss: 0.2632 - val_accuracy: 0.9246\n",
      "Epoch 9/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.2467 - accuracy: 0.9307 - val_loss: 0.2628 - val_accuracy: 0.9246\n",
      "Epoch 10/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2463 - accuracy: 0.9307 - val_loss: 0.2625 - val_accuracy: 0.9246\n",
      "Epoch 11/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2458 - accuracy: 0.9307 - val_loss: 0.2619 - val_accuracy: 0.9246\n",
      "Epoch 12/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2453 - accuracy: 0.9307 - val_loss: 0.2614 - val_accuracy: 0.9246\n",
      "Epoch 13/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2448 - accuracy: 0.9307 - val_loss: 0.2610 - val_accuracy: 0.9246\n",
      "Epoch 14/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2442 - accuracy: 0.9307 - val_loss: 0.2603 - val_accuracy: 0.9246\n",
      "Epoch 15/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2436 - accuracy: 0.9307 - val_loss: 0.2598 - val_accuracy: 0.9246\n",
      "Epoch 16/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2429 - accuracy: 0.9307 - val_loss: 0.2589 - val_accuracy: 0.9246\n",
      "Epoch 17/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2421 - accuracy: 0.9307 - val_loss: 0.2581 - val_accuracy: 0.9246\n",
      "Epoch 18/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2413 - accuracy: 0.9307 - val_loss: 0.2571 - val_accuracy: 0.9246\n",
      "Epoch 19/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2403 - accuracy: 0.9307 - val_loss: 0.2560 - val_accuracy: 0.9246\n",
      "Epoch 20/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2392 - accuracy: 0.9307 - val_loss: 0.2550 - val_accuracy: 0.9246\n",
      "Epoch 21/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2380 - accuracy: 0.9307 - val_loss: 0.2536 - val_accuracy: 0.9246\n",
      "Epoch 22/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2368 - accuracy: 0.9307 - val_loss: 0.2522 - val_accuracy: 0.9246\n",
      "Epoch 23/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2355 - accuracy: 0.9307 - val_loss: 0.2511 - val_accuracy: 0.9246\n",
      "Epoch 24/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2341 - accuracy: 0.9307 - val_loss: 0.2496 - val_accuracy: 0.9246\n",
      "Epoch 25/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2327 - accuracy: 0.9307 - val_loss: 0.2484 - val_accuracy: 0.9246\n",
      "Epoch 26/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2313 - accuracy: 0.9307 - val_loss: 0.2469 - val_accuracy: 0.9246\n",
      "Epoch 27/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2299 - accuracy: 0.9307 - val_loss: 0.2454 - val_accuracy: 0.9246\n",
      "Epoch 28/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2284 - accuracy: 0.9307 - val_loss: 0.2440 - val_accuracy: 0.9246\n",
      "Epoch 29/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2269 - accuracy: 0.9307 - val_loss: 0.2426 - val_accuracy: 0.9246\n",
      "Epoch 30/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2253 - accuracy: 0.9307 - val_loss: 0.2410 - val_accuracy: 0.9246\n",
      "Epoch 31/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2238 - accuracy: 0.9307 - val_loss: 0.2395 - val_accuracy: 0.9245\n",
      "Epoch 32/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2222 - accuracy: 0.9307 - val_loss: 0.2379 - val_accuracy: 0.9245\n",
      "Epoch 33/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2206 - accuracy: 0.9308 - val_loss: 0.2367 - val_accuracy: 0.9246\n",
      "Epoch 34/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2190 - accuracy: 0.9307 - val_loss: 0.2350 - val_accuracy: 0.9247\n",
      "Epoch 35/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2174 - accuracy: 0.9309 - val_loss: 0.2336 - val_accuracy: 0.9247\n",
      "Epoch 36/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2158 - accuracy: 0.9309 - val_loss: 0.2320 - val_accuracy: 0.9248\n",
      "Epoch 37/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2142 - accuracy: 0.9310 - val_loss: 0.2303 - val_accuracy: 0.9247\n",
      "Epoch 38/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2126 - accuracy: 0.9310 - val_loss: 0.2288 - val_accuracy: 0.9249\n",
      "Epoch 39/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2108 - accuracy: 0.9311 - val_loss: 0.2272 - val_accuracy: 0.9248\n",
      "Epoch 40/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2092 - accuracy: 0.9312 - val_loss: 0.2256 - val_accuracy: 0.9250\n",
      "Epoch 41/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2076 - accuracy: 0.9312 - val_loss: 0.2240 - val_accuracy: 0.9251\n",
      "Epoch 42/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2059 - accuracy: 0.9314 - val_loss: 0.2223 - val_accuracy: 0.9252\n",
      "Epoch 43/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.2042 - accuracy: 0.9315 - val_loss: 0.2208 - val_accuracy: 0.9253\n",
      "Epoch 44/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2026 - accuracy: 0.9316 - val_loss: 0.2195 - val_accuracy: 0.9254\n",
      "Epoch 45/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2010 - accuracy: 0.9317 - val_loss: 0.2179 - val_accuracy: 0.9255\n",
      "Epoch 46/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1994 - accuracy: 0.9318 - val_loss: 0.2160 - val_accuracy: 0.9256\n",
      "Epoch 47/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1978 - accuracy: 0.9320 - val_loss: 0.2155 - val_accuracy: 0.9261\n",
      "Epoch 48/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1963 - accuracy: 0.9321 - val_loss: 0.2136 - val_accuracy: 0.9258\n",
      "Epoch 49/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1949 - accuracy: 0.9324 - val_loss: 0.2124 - val_accuracy: 0.9256\n",
      "Epoch 50/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1935 - accuracy: 0.9327 - val_loss: 0.2108 - val_accuracy: 0.9266\n",
      "Epoch 51/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1921 - accuracy: 0.9330 - val_loss: 0.2095 - val_accuracy: 0.9266\n",
      "Epoch 52/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1908 - accuracy: 0.9332 - val_loss: 0.2084 - val_accuracy: 0.9268\n",
      "Epoch 53/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1896 - accuracy: 0.9336 - val_loss: 0.2070 - val_accuracy: 0.9274\n",
      "Epoch 54/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1884 - accuracy: 0.9338 - val_loss: 0.2065 - val_accuracy: 0.9274\n",
      "Epoch 55/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1873 - accuracy: 0.9342 - val_loss: 0.2052 - val_accuracy: 0.9272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1863 - accuracy: 0.9345 - val_loss: 0.2042 - val_accuracy: 0.9274\n",
      "Epoch 57/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1853 - accuracy: 0.9347 - val_loss: 0.2034 - val_accuracy: 0.9277\n",
      "Epoch 58/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1843 - accuracy: 0.9348 - val_loss: 0.2023 - val_accuracy: 0.9280\n",
      "Epoch 59/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1834 - accuracy: 0.9350 - val_loss: 0.2017 - val_accuracy: 0.9277\n",
      "Epoch 60/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1825 - accuracy: 0.9353 - val_loss: 0.2008 - val_accuracy: 0.9282\n",
      "Epoch 61/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1815 - accuracy: 0.9355 - val_loss: 0.2002 - val_accuracy: 0.9286\n",
      "Epoch 62/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1807 - accuracy: 0.9355 - val_loss: 0.1991 - val_accuracy: 0.9288\n",
      "Epoch 63/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1799 - accuracy: 0.9360 - val_loss: 0.1983 - val_accuracy: 0.9288\n",
      "Epoch 64/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1791 - accuracy: 0.9361 - val_loss: 0.1976 - val_accuracy: 0.9287\n",
      "Epoch 65/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1784 - accuracy: 0.9362 - val_loss: 0.1973 - val_accuracy: 0.9287\n",
      "Epoch 66/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1777 - accuracy: 0.9365 - val_loss: 0.1963 - val_accuracy: 0.9292\n",
      "Epoch 67/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1770 - accuracy: 0.9366 - val_loss: 0.1962 - val_accuracy: 0.9290\n",
      "Epoch 68/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1765 - accuracy: 0.9367 - val_loss: 0.1952 - val_accuracy: 0.9291\n",
      "Epoch 69/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1759 - accuracy: 0.9368 - val_loss: 0.1947 - val_accuracy: 0.9292\n",
      "Epoch 70/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1753 - accuracy: 0.9371 - val_loss: 0.1944 - val_accuracy: 0.9295\n",
      "Epoch 71/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1747 - accuracy: 0.9372 - val_loss: 0.1939 - val_accuracy: 0.9297\n",
      "Epoch 72/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1742 - accuracy: 0.9375 - val_loss: 0.1936 - val_accuracy: 0.9296\n",
      "Epoch 73/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1737 - accuracy: 0.9375 - val_loss: 0.1936 - val_accuracy: 0.9296\n",
      "Epoch 74/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1733 - accuracy: 0.9376 - val_loss: 0.1928 - val_accuracy: 0.9301\n",
      "Epoch 75/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1728 - accuracy: 0.9378 - val_loss: 0.1923 - val_accuracy: 0.9300\n",
      "Epoch 76/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1725 - accuracy: 0.9379 - val_loss: 0.1922 - val_accuracy: 0.9301\n",
      "Epoch 77/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1720 - accuracy: 0.9381 - val_loss: 0.1916 - val_accuracy: 0.9304\n",
      "Epoch 78/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1715 - accuracy: 0.9381 - val_loss: 0.1910 - val_accuracy: 0.9301\n",
      "Epoch 79/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1712 - accuracy: 0.9383 - val_loss: 0.1906 - val_accuracy: 0.9303\n",
      "Epoch 80/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1707 - accuracy: 0.9384 - val_loss: 0.1907 - val_accuracy: 0.9302\n",
      "Epoch 81/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1703 - accuracy: 0.9385 - val_loss: 0.1903 - val_accuracy: 0.9305\n",
      "Epoch 82/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1700 - accuracy: 0.9386 - val_loss: 0.1899 - val_accuracy: 0.9306\n",
      "Epoch 83/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1696 - accuracy: 0.9389 - val_loss: 0.1899 - val_accuracy: 0.9302\n",
      "Epoch 84/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1692 - accuracy: 0.9387 - val_loss: 0.1888 - val_accuracy: 0.9309\n",
      "Epoch 85/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1688 - accuracy: 0.9388 - val_loss: 0.1889 - val_accuracy: 0.9305\n",
      "Epoch 86/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1684 - accuracy: 0.9390 - val_loss: 0.1885 - val_accuracy: 0.9309\n",
      "Epoch 87/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1680 - accuracy: 0.9391 - val_loss: 0.1880 - val_accuracy: 0.9305\n",
      "Epoch 88/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1676 - accuracy: 0.9392 - val_loss: 0.1878 - val_accuracy: 0.9307\n",
      "Epoch 89/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1672 - accuracy: 0.9394 - val_loss: 0.1879 - val_accuracy: 0.9309\n",
      "Epoch 90/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1668 - accuracy: 0.9395 - val_loss: 0.1876 - val_accuracy: 0.9310\n",
      "Epoch 91/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1664 - accuracy: 0.9397 - val_loss: 0.1866 - val_accuracy: 0.9311\n",
      "Epoch 92/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1660 - accuracy: 0.9399 - val_loss: 0.1863 - val_accuracy: 0.9310\n",
      "Epoch 93/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1656 - accuracy: 0.9399 - val_loss: 0.1861 - val_accuracy: 0.9314\n",
      "Epoch 94/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1652 - accuracy: 0.9401 - val_loss: 0.1852 - val_accuracy: 0.9319\n",
      "Epoch 95/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1649 - accuracy: 0.9401 - val_loss: 0.1851 - val_accuracy: 0.9318\n",
      "Epoch 96/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1645 - accuracy: 0.9403 - val_loss: 0.1848 - val_accuracy: 0.9316\n",
      "Epoch 97/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1640 - accuracy: 0.9403 - val_loss: 0.1844 - val_accuracy: 0.9320\n",
      "Epoch 98/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1636 - accuracy: 0.9405 - val_loss: 0.1840 - val_accuracy: 0.9322\n",
      "Epoch 99/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1633 - accuracy: 0.9406 - val_loss: 0.1839 - val_accuracy: 0.9321\n",
      "Epoch 100/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1629 - accuracy: 0.9406 - val_loss: 0.1835 - val_accuracy: 0.9320\n",
      "Epoch 101/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1625 - accuracy: 0.9410 - val_loss: 0.1834 - val_accuracy: 0.9317\n",
      "Epoch 102/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1621 - accuracy: 0.9411 - val_loss: 0.1826 - val_accuracy: 0.9326\n",
      "Epoch 103/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1618 - accuracy: 0.9412 - val_loss: 0.1825 - val_accuracy: 0.9325\n",
      "Epoch 104/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1614 - accuracy: 0.9413 - val_loss: 0.1823 - val_accuracy: 0.9324\n",
      "Epoch 105/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1611 - accuracy: 0.9416 - val_loss: 0.1817 - val_accuracy: 0.9327\n",
      "Epoch 106/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1607 - accuracy: 0.9415 - val_loss: 0.1817 - val_accuracy: 0.9326\n",
      "Epoch 107/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1603 - accuracy: 0.9418 - val_loss: 0.1816 - val_accuracy: 0.9328\n",
      "Epoch 108/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1601 - accuracy: 0.9419 - val_loss: 0.1805 - val_accuracy: 0.9328\n",
      "Epoch 109/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1596 - accuracy: 0.9419 - val_loss: 0.1806 - val_accuracy: 0.9335\n",
      "Epoch 110/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1592 - accuracy: 0.9420 - val_loss: 0.1803 - val_accuracy: 0.9330\n",
      "Epoch 111/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1588 - accuracy: 0.9421 - val_loss: 0.1799 - val_accuracy: 0.9335\n",
      "Epoch 112/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1585 - accuracy: 0.9422 - val_loss: 0.1800 - val_accuracy: 0.9335\n",
      "Epoch 113/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1581 - accuracy: 0.9425 - val_loss: 0.1797 - val_accuracy: 0.9336\n",
      "Epoch 114/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1578 - accuracy: 0.9425 - val_loss: 0.1788 - val_accuracy: 0.9342\n",
      "Epoch 115/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1575 - accuracy: 0.9425 - val_loss: 0.1788 - val_accuracy: 0.9343\n",
      "Epoch 116/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1571 - accuracy: 0.9428 - val_loss: 0.1784 - val_accuracy: 0.9342\n",
      "Epoch 117/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1567 - accuracy: 0.9428 - val_loss: 0.1781 - val_accuracy: 0.9343\n",
      "Epoch 118/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1564 - accuracy: 0.9429 - val_loss: 0.1779 - val_accuracy: 0.9343\n",
      "Epoch 119/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1562 - accuracy: 0.9430 - val_loss: 0.1776 - val_accuracy: 0.9351\n",
      "Epoch 120/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1558 - accuracy: 0.9433 - val_loss: 0.1776 - val_accuracy: 0.9346\n",
      "Epoch 121/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1556 - accuracy: 0.9433 - val_loss: 0.1771 - val_accuracy: 0.9352\n",
      "Epoch 122/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1551 - accuracy: 0.9436 - val_loss: 0.1771 - val_accuracy: 0.9351\n",
      "Epoch 123/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1549 - accuracy: 0.9435 - val_loss: 0.1764 - val_accuracy: 0.9349\n",
      "Epoch 124/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1547 - accuracy: 0.9436 - val_loss: 0.1763 - val_accuracy: 0.9349\n",
      "Epoch 125/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1544 - accuracy: 0.9436 - val_loss: 0.1766 - val_accuracy: 0.9350\n",
      "Epoch 126/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1541 - accuracy: 0.9440 - val_loss: 0.1758 - val_accuracy: 0.9350\n",
      "Epoch 127/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1539 - accuracy: 0.9439 - val_loss: 0.1756 - val_accuracy: 0.9354\n",
      "Epoch 128/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1536 - accuracy: 0.9440 - val_loss: 0.1755 - val_accuracy: 0.9357\n",
      "Epoch 129/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1534 - accuracy: 0.9441 - val_loss: 0.1756 - val_accuracy: 0.9355\n",
      "Epoch 130/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1531 - accuracy: 0.9441 - val_loss: 0.1751 - val_accuracy: 0.9358\n",
      "Epoch 131/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1528 - accuracy: 0.9442 - val_loss: 0.1750 - val_accuracy: 0.9358\n",
      "Epoch 132/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1525 - accuracy: 0.9442 - val_loss: 0.1745 - val_accuracy: 0.9358\n",
      "Epoch 133/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1523 - accuracy: 0.9444 - val_loss: 0.1744 - val_accuracy: 0.9360\n",
      "Epoch 134/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1521 - accuracy: 0.9444 - val_loss: 0.1742 - val_accuracy: 0.9357\n",
      "Epoch 135/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1518 - accuracy: 0.9444 - val_loss: 0.1741 - val_accuracy: 0.9362\n",
      "Epoch 136/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1516 - accuracy: 0.9447 - val_loss: 0.1736 - val_accuracy: 0.9357\n",
      "Epoch 137/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1514 - accuracy: 0.9448 - val_loss: 0.1735 - val_accuracy: 0.9357\n",
      "Epoch 138/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1512 - accuracy: 0.9449 - val_loss: 0.1733 - val_accuracy: 0.9361\n",
      "Epoch 139/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1510 - accuracy: 0.9450 - val_loss: 0.1728 - val_accuracy: 0.9364\n",
      "Epoch 140/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1507 - accuracy: 0.9451 - val_loss: 0.1729 - val_accuracy: 0.9362\n",
      "Epoch 141/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1504 - accuracy: 0.9449 - val_loss: 0.1730 - val_accuracy: 0.9367\n",
      "Epoch 142/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1502 - accuracy: 0.9453 - val_loss: 0.1726 - val_accuracy: 0.9361\n",
      "Epoch 143/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1500 - accuracy: 0.9453 - val_loss: 0.1719 - val_accuracy: 0.9368\n",
      "Epoch 144/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1498 - accuracy: 0.9453 - val_loss: 0.1724 - val_accuracy: 0.9364\n",
      "Epoch 145/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1496 - accuracy: 0.9453 - val_loss: 0.1719 - val_accuracy: 0.9369\n",
      "Epoch 146/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1493 - accuracy: 0.9455 - val_loss: 0.1718 - val_accuracy: 0.9370\n",
      "Epoch 147/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1491 - accuracy: 0.9455 - val_loss: 0.1710 - val_accuracy: 0.9372\n",
      "Epoch 148/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1489 - accuracy: 0.9457 - val_loss: 0.1716 - val_accuracy: 0.9374\n",
      "Epoch 149/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1487 - accuracy: 0.9456 - val_loss: 0.1713 - val_accuracy: 0.9368\n",
      "Epoch 150/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1485 - accuracy: 0.9456 - val_loss: 0.1710 - val_accuracy: 0.9374\n",
      "Epoch 151/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1483 - accuracy: 0.9458 - val_loss: 0.1706 - val_accuracy: 0.9372\n",
      "Epoch 152/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1481 - accuracy: 0.9460 - val_loss: 0.1711 - val_accuracy: 0.9374\n",
      "Epoch 153/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1480 - accuracy: 0.9459 - val_loss: 0.1700 - val_accuracy: 0.9373\n",
      "Epoch 154/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1477 - accuracy: 0.9459 - val_loss: 0.1710 - val_accuracy: 0.9372\n",
      "Epoch 155/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1475 - accuracy: 0.9462 - val_loss: 0.1701 - val_accuracy: 0.9374\n",
      "Epoch 156/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1473 - accuracy: 0.9461 - val_loss: 0.1702 - val_accuracy: 0.9373\n",
      "Epoch 157/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1471 - accuracy: 0.9461 - val_loss: 0.1696 - val_accuracy: 0.9376\n",
      "Epoch 158/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1469 - accuracy: 0.9463 - val_loss: 0.1700 - val_accuracy: 0.9375\n",
      "Epoch 159/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1467 - accuracy: 0.9464 - val_loss: 0.1693 - val_accuracy: 0.9375\n",
      "Epoch 160/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1466 - accuracy: 0.9465 - val_loss: 0.1690 - val_accuracy: 0.9376\n",
      "Epoch 161/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1464 - accuracy: 0.9465 - val_loss: 0.1696 - val_accuracy: 0.9375\n",
      "Epoch 162/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1462 - accuracy: 0.9464 - val_loss: 0.1691 - val_accuracy: 0.9380\n",
      "Epoch 163/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1460 - accuracy: 0.9466 - val_loss: 0.1689 - val_accuracy: 0.9377\n",
      "Epoch 164/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1458 - accuracy: 0.9466 - val_loss: 0.1685 - val_accuracy: 0.9379\n",
      "Epoch 165/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1456 - accuracy: 0.9469 - val_loss: 0.1689 - val_accuracy: 0.9376\n",
      "Epoch 166/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1455 - accuracy: 0.9467 - val_loss: 0.1681 - val_accuracy: 0.9376\n",
      "Epoch 167/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1452 - accuracy: 0.9469 - val_loss: 0.1684 - val_accuracy: 0.9377\n",
      "Epoch 168/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1451 - accuracy: 0.9470 - val_loss: 0.1685 - val_accuracy: 0.9381\n",
      "Epoch 169/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1448 - accuracy: 0.9469 - val_loss: 0.1685 - val_accuracy: 0.9380\n",
      "Epoch 170/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1447 - accuracy: 0.9470 - val_loss: 0.1678 - val_accuracy: 0.9381\n",
      "Epoch 171/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1445 - accuracy: 0.9470 - val_loss: 0.1677 - val_accuracy: 0.9377\n",
      "Epoch 172/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1444 - accuracy: 0.9471 - val_loss: 0.1677 - val_accuracy: 0.9380\n",
      "Epoch 173/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1442 - accuracy: 0.9472 - val_loss: 0.1669 - val_accuracy: 0.9387\n",
      "Epoch 174/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1440 - accuracy: 0.9472 - val_loss: 0.1671 - val_accuracy: 0.9383\n",
      "Epoch 175/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1438 - accuracy: 0.9474 - val_loss: 0.1669 - val_accuracy: 0.9384\n",
      "Epoch 176/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1437 - accuracy: 0.9473 - val_loss: 0.1669 - val_accuracy: 0.9382\n",
      "Epoch 177/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1435 - accuracy: 0.9473 - val_loss: 0.1670 - val_accuracy: 0.9384\n",
      "Epoch 178/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1434 - accuracy: 0.9472 - val_loss: 0.1668 - val_accuracy: 0.9383\n",
      "Epoch 179/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1432 - accuracy: 0.9476 - val_loss: 0.1664 - val_accuracy: 0.9381\n",
      "Epoch 180/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1430 - accuracy: 0.9474 - val_loss: 0.1660 - val_accuracy: 0.9385\n",
      "Epoch 181/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1428 - accuracy: 0.9476 - val_loss: 0.1661 - val_accuracy: 0.9390\n",
      "Epoch 182/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1426 - accuracy: 0.9476 - val_loss: 0.1657 - val_accuracy: 0.9389\n",
      "Epoch 183/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1425 - accuracy: 0.9476 - val_loss: 0.1659 - val_accuracy: 0.9387\n",
      "Epoch 184/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1423 - accuracy: 0.9478 - val_loss: 0.1661 - val_accuracy: 0.9389\n",
      "Epoch 185/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1422 - accuracy: 0.9479 - val_loss: 0.1654 - val_accuracy: 0.9392\n",
      "Epoch 186/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1420 - accuracy: 0.9478 - val_loss: 0.1654 - val_accuracy: 0.9385\n",
      "Epoch 187/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1419 - accuracy: 0.9480 - val_loss: 0.1658 - val_accuracy: 0.9389\n",
      "Epoch 188/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1416 - accuracy: 0.9480 - val_loss: 0.1653 - val_accuracy: 0.9392\n",
      "Epoch 189/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1415 - accuracy: 0.9481 - val_loss: 0.1648 - val_accuracy: 0.9393\n",
      "Epoch 190/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1413 - accuracy: 0.9480 - val_loss: 0.1651 - val_accuracy: 0.9390\n",
      "Epoch 191/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1412 - accuracy: 0.9481 - val_loss: 0.1649 - val_accuracy: 0.9388\n",
      "Epoch 192/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1410 - accuracy: 0.9481 - val_loss: 0.1644 - val_accuracy: 0.9390\n",
      "Epoch 193/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1408 - accuracy: 0.9483 - val_loss: 0.1643 - val_accuracy: 0.9393\n",
      "Epoch 194/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1406 - accuracy: 0.9483 - val_loss: 0.1642 - val_accuracy: 0.9395\n",
      "Epoch 195/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1405 - accuracy: 0.9485 - val_loss: 0.1644 - val_accuracy: 0.9396\n",
      "Epoch 196/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1403 - accuracy: 0.9484 - val_loss: 0.1643 - val_accuracy: 0.9389\n",
      "Epoch 197/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1402 - accuracy: 0.9485 - val_loss: 0.1640 - val_accuracy: 0.9395\n",
      "Epoch 198/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1401 - accuracy: 0.9484 - val_loss: 0.1638 - val_accuracy: 0.9394\n",
      "Epoch 199/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1398 - accuracy: 0.9485 - val_loss: 0.1641 - val_accuracy: 0.9399\n",
      "Epoch 200/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1397 - accuracy: 0.9487 - val_loss: 0.1635 - val_accuracy: 0.9398\n",
      "Epoch 201/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1395 - accuracy: 0.9487 - val_loss: 0.1634 - val_accuracy: 0.9397\n",
      "Epoch 202/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1394 - accuracy: 0.9487 - val_loss: 0.1630 - val_accuracy: 0.9402\n",
      "Epoch 203/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1392 - accuracy: 0.9488 - val_loss: 0.1632 - val_accuracy: 0.9398\n",
      "Epoch 204/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1391 - accuracy: 0.9488 - val_loss: 0.1631 - val_accuracy: 0.9400\n",
      "Epoch 205/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1389 - accuracy: 0.9489 - val_loss: 0.1629 - val_accuracy: 0.9400\n",
      "Epoch 206/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1387 - accuracy: 0.9489 - val_loss: 0.1635 - val_accuracy: 0.9400\n",
      "Epoch 207/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1386 - accuracy: 0.9490 - val_loss: 0.1626 - val_accuracy: 0.9402\n",
      "Epoch 208/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1385 - accuracy: 0.9489 - val_loss: 0.1628 - val_accuracy: 0.9400\n",
      "Epoch 209/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1383 - accuracy: 0.9492 - val_loss: 0.1627 - val_accuracy: 0.9400\n",
      "Epoch 210/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1382 - accuracy: 0.9491 - val_loss: 0.1627 - val_accuracy: 0.9400\n",
      "Epoch 211/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1380 - accuracy: 0.9492 - val_loss: 0.1627 - val_accuracy: 0.9398\n",
      "Epoch 212/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1378 - accuracy: 0.9492 - val_loss: 0.1623 - val_accuracy: 0.9404\n",
      "Epoch 213/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1377 - accuracy: 0.9492 - val_loss: 0.1630 - val_accuracy: 0.9401\n",
      "Epoch 214/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1376 - accuracy: 0.9493 - val_loss: 0.1620 - val_accuracy: 0.9404\n",
      "Epoch 215/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1374 - accuracy: 0.9493 - val_loss: 0.1624 - val_accuracy: 0.9400\n",
      "Epoch 216/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1373 - accuracy: 0.9493 - val_loss: 0.1614 - val_accuracy: 0.9407\n",
      "Epoch 217/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1371 - accuracy: 0.9495 - val_loss: 0.1616 - val_accuracy: 0.9404\n",
      "Epoch 218/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1370 - accuracy: 0.9493 - val_loss: 0.1612 - val_accuracy: 0.9410\n",
      "Epoch 219/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1369 - accuracy: 0.9496 - val_loss: 0.1615 - val_accuracy: 0.9405\n",
      "Epoch 220/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1367 - accuracy: 0.9495 - val_loss: 0.1619 - val_accuracy: 0.9408\n",
      "Epoch 221/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1365 - accuracy: 0.9495 - val_loss: 0.1615 - val_accuracy: 0.9410\n",
      "Epoch 222/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1364 - accuracy: 0.9496 - val_loss: 0.1610 - val_accuracy: 0.9407\n",
      "Epoch 223/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1363 - accuracy: 0.9497 - val_loss: 0.1608 - val_accuracy: 0.9411\n",
      "Epoch 224/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1361 - accuracy: 0.9497 - val_loss: 0.1608 - val_accuracy: 0.9407\n",
      "Epoch 225/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1360 - accuracy: 0.9497 - val_loss: 0.1610 - val_accuracy: 0.9409\n",
      "Epoch 226/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1358 - accuracy: 0.9497 - val_loss: 0.1603 - val_accuracy: 0.9410\n",
      "Epoch 227/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1357 - accuracy: 0.9499 - val_loss: 0.1606 - val_accuracy: 0.9411\n",
      "Epoch 228/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1356 - accuracy: 0.9497 - val_loss: 0.1611 - val_accuracy: 0.9406\n",
      "Epoch 229/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1355 - accuracy: 0.9499 - val_loss: 0.1604 - val_accuracy: 0.9406\n",
      "Epoch 230/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1353 - accuracy: 0.9500 - val_loss: 0.1604 - val_accuracy: 0.9413\n",
      "Epoch 231/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1352 - accuracy: 0.9500 - val_loss: 0.1601 - val_accuracy: 0.9409\n",
      "Epoch 232/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1350 - accuracy: 0.9499 - val_loss: 0.1603 - val_accuracy: 0.9414\n",
      "Epoch 233/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1349 - accuracy: 0.9500 - val_loss: 0.1594 - val_accuracy: 0.9410\n",
      "Epoch 234/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1347 - accuracy: 0.9500 - val_loss: 0.1596 - val_accuracy: 0.9408\n",
      "Epoch 235/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1346 - accuracy: 0.9501 - val_loss: 0.1593 - val_accuracy: 0.9412\n",
      "Epoch 236/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1345 - accuracy: 0.9501 - val_loss: 0.1595 - val_accuracy: 0.9411\n",
      "Epoch 237/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1343 - accuracy: 0.9502 - val_loss: 0.1594 - val_accuracy: 0.9413\n",
      "Epoch 238/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1342 - accuracy: 0.9502 - val_loss: 0.1593 - val_accuracy: 0.9419\n",
      "Epoch 239/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1341 - accuracy: 0.9501 - val_loss: 0.1601 - val_accuracy: 0.9410\n",
      "Epoch 240/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1340 - accuracy: 0.9501 - val_loss: 0.1597 - val_accuracy: 0.9413\n",
      "Epoch 241/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1339 - accuracy: 0.9503 - val_loss: 0.1598 - val_accuracy: 0.9413\n",
      "Epoch 242/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1338 - accuracy: 0.9502 - val_loss: 0.1592 - val_accuracy: 0.9417\n",
      "Epoch 243/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1336 - accuracy: 0.9504 - val_loss: 0.1585 - val_accuracy: 0.9419\n",
      "Epoch 244/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1334 - accuracy: 0.9503 - val_loss: 0.1590 - val_accuracy: 0.9413\n",
      "Epoch 245/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1333 - accuracy: 0.9505 - val_loss: 0.1593 - val_accuracy: 0.9416\n",
      "Epoch 246/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1333 - accuracy: 0.9504 - val_loss: 0.1589 - val_accuracy: 0.9414\n",
      "Epoch 247/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1330 - accuracy: 0.9505 - val_loss: 0.1593 - val_accuracy: 0.9409\n",
      "Epoch 248/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1330 - accuracy: 0.9506 - val_loss: 0.1584 - val_accuracy: 0.9416\n",
      "Epoch 249/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1328 - accuracy: 0.9505 - val_loss: 0.1586 - val_accuracy: 0.9414\n",
      "Epoch 250/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1327 - accuracy: 0.9504 - val_loss: 0.1586 - val_accuracy: 0.9417\n",
      "Epoch 251/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1325 - accuracy: 0.9506 - val_loss: 0.1580 - val_accuracy: 0.9420\n",
      "Epoch 252/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1324 - accuracy: 0.9507 - val_loss: 0.1586 - val_accuracy: 0.9420\n",
      "Epoch 253/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1323 - accuracy: 0.9506 - val_loss: 0.1583 - val_accuracy: 0.9414\n",
      "Epoch 254/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1323 - accuracy: 0.9505 - val_loss: 0.1585 - val_accuracy: 0.9413\n",
      "Epoch 255/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1321 - accuracy: 0.9507 - val_loss: 0.1584 - val_accuracy: 0.9418\n",
      "Epoch 256/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1320 - accuracy: 0.9506 - val_loss: 0.1585 - val_accuracy: 0.9413\n",
      "Epoch 257/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1319 - accuracy: 0.9508 - val_loss: 0.1582 - val_accuracy: 0.9415\n",
      "Epoch 258/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1317 - accuracy: 0.9505 - val_loss: 0.1577 - val_accuracy: 0.9418\n",
      "Epoch 259/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1316 - accuracy: 0.9506 - val_loss: 0.1577 - val_accuracy: 0.9417\n",
      "Epoch 260/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1315 - accuracy: 0.9507 - val_loss: 0.1572 - val_accuracy: 0.9416\n",
      "Epoch 261/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1314 - accuracy: 0.9507 - val_loss: 0.1579 - val_accuracy: 0.9422\n",
      "Epoch 262/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1313 - accuracy: 0.9509 - val_loss: 0.1575 - val_accuracy: 0.9416\n",
      "Epoch 263/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1312 - accuracy: 0.9509 - val_loss: 0.1580 - val_accuracy: 0.9417\n",
      "Epoch 264/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1311 - accuracy: 0.9510 - val_loss: 0.1577 - val_accuracy: 0.9416\n",
      "Epoch 265/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1309 - accuracy: 0.9511 - val_loss: 0.1577 - val_accuracy: 0.9417\n",
      "Epoch 266/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1308 - accuracy: 0.9509 - val_loss: 0.1582 - val_accuracy: 0.9413\n",
      "Epoch 267/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1307 - accuracy: 0.9511 - val_loss: 0.1570 - val_accuracy: 0.9419\n",
      "Epoch 268/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1305 - accuracy: 0.9510 - val_loss: 0.1574 - val_accuracy: 0.9417\n",
      "Epoch 269/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1305 - accuracy: 0.9511 - val_loss: 0.1569 - val_accuracy: 0.9416\n",
      "Epoch 270/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1305 - accuracy: 0.9509 - val_loss: 0.1568 - val_accuracy: 0.9420\n",
      "Epoch 271/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1303 - accuracy: 0.9509 - val_loss: 0.1568 - val_accuracy: 0.9423\n",
      "Epoch 272/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1302 - accuracy: 0.9512 - val_loss: 0.1573 - val_accuracy: 0.9418\n",
      "Epoch 273/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1301 - accuracy: 0.9510 - val_loss: 0.1567 - val_accuracy: 0.9421\n",
      "Epoch 274/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1299 - accuracy: 0.9512 - val_loss: 0.1571 - val_accuracy: 0.9420\n",
      "Epoch 275/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1298 - accuracy: 0.9511 - val_loss: 0.1569 - val_accuracy: 0.9422\n",
      "Epoch 276/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1297 - accuracy: 0.9511 - val_loss: 0.1571 - val_accuracy: 0.9417\n",
      "Epoch 277/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1296 - accuracy: 0.9514 - val_loss: 0.1563 - val_accuracy: 0.9411\n",
      "Epoch 278/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1295 - accuracy: 0.9514 - val_loss: 0.1570 - val_accuracy: 0.9421\n",
      "Epoch 279/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1294 - accuracy: 0.9516 - val_loss: 0.1569 - val_accuracy: 0.9416\n",
      "Epoch 280/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1293 - accuracy: 0.9513 - val_loss: 0.1563 - val_accuracy: 0.9418\n",
      "Epoch 281/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1291 - accuracy: 0.9513 - val_loss: 0.1569 - val_accuracy: 0.9418\n",
      "Epoch 282/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1291 - accuracy: 0.9513 - val_loss: 0.1564 - val_accuracy: 0.9418\n",
      "Epoch 283/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.1289 - accuracy: 0.9514 - val_loss: 0.1564 - val_accuracy: 0.9417\n",
      "Epoch 284/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1288 - accuracy: 0.9513 - val_loss: 0.1563 - val_accuracy: 0.9416\n",
      "Epoch 285/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1287 - accuracy: 0.9516 - val_loss: 0.1562 - val_accuracy: 0.9419\n",
      "Epoch 286/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1286 - accuracy: 0.9514 - val_loss: 0.1564 - val_accuracy: 0.9423\n",
      "Epoch 287/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1287 - accuracy: 0.9514 - val_loss: 0.1561 - val_accuracy: 0.9421\n",
      "Epoch 288/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1284 - accuracy: 0.9517 - val_loss: 0.1564 - val_accuracy: 0.9420\n",
      "Epoch 289/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1283 - accuracy: 0.9517 - val_loss: 0.1555 - val_accuracy: 0.9419\n",
      "Epoch 290/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1282 - accuracy: 0.9513 - val_loss: 0.1554 - val_accuracy: 0.9421\n",
      "Epoch 291/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1282 - accuracy: 0.9517 - val_loss: 0.1556 - val_accuracy: 0.9424\n",
      "Epoch 292/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1280 - accuracy: 0.9517 - val_loss: 0.1562 - val_accuracy: 0.9423\n",
      "Epoch 293/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1279 - accuracy: 0.9516 - val_loss: 0.1560 - val_accuracy: 0.9421\n",
      "Epoch 294/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1278 - accuracy: 0.9517 - val_loss: 0.1563 - val_accuracy: 0.9420\n",
      "Epoch 295/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1277 - accuracy: 0.9519 - val_loss: 0.1558 - val_accuracy: 0.9416\n",
      "Epoch 296/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1276 - accuracy: 0.9516 - val_loss: 0.1565 - val_accuracy: 0.9419\n",
      "Epoch 297/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1275 - accuracy: 0.9517 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
      "Epoch 298/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1274 - accuracy: 0.9520 - val_loss: 0.1559 - val_accuracy: 0.9420\n",
      "Epoch 299/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1273 - accuracy: 0.9520 - val_loss: 0.1561 - val_accuracy: 0.9419\n",
      "Epoch 300/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1272 - accuracy: 0.9518 - val_loss: 0.1557 - val_accuracy: 0.9421\n",
      "Epoch 301/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1271 - accuracy: 0.9519 - val_loss: 0.1556 - val_accuracy: 0.9422\n",
      "Epoch 302/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1270 - accuracy: 0.9517 - val_loss: 0.1557 - val_accuracy: 0.9416\n",
      "Epoch 303/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1269 - accuracy: 0.9518 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 304/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1269 - accuracy: 0.9519 - val_loss: 0.1548 - val_accuracy: 0.9418\n",
      "Epoch 305/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1268 - accuracy: 0.9520 - val_loss: 0.1553 - val_accuracy: 0.9423\n",
      "Epoch 306/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1265 - accuracy: 0.9522 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 307/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1265 - accuracy: 0.9522 - val_loss: 0.1552 - val_accuracy: 0.9424\n",
      "Epoch 308/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1265 - accuracy: 0.9520 - val_loss: 0.1552 - val_accuracy: 0.9420\n",
      "Epoch 309/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1263 - accuracy: 0.9520 - val_loss: 0.1548 - val_accuracy: 0.9414\n",
      "Epoch 310/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1263 - accuracy: 0.9521 - val_loss: 0.1559 - val_accuracy: 0.9424\n",
      "Epoch 311/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1262 - accuracy: 0.9520 - val_loss: 0.1549 - val_accuracy: 0.9423\n",
      "Epoch 312/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1260 - accuracy: 0.9520 - val_loss: 0.1556 - val_accuracy: 0.9420\n",
      "Epoch 313/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1260 - accuracy: 0.9520 - val_loss: 0.1545 - val_accuracy: 0.9423\n",
      "Epoch 314/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1257 - accuracy: 0.9522 - val_loss: 0.1558 - val_accuracy: 0.9411\n",
      "Epoch 315/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1258 - accuracy: 0.9521 - val_loss: 0.1551 - val_accuracy: 0.9420\n",
      "Epoch 316/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1256 - accuracy: 0.9521 - val_loss: 0.1560 - val_accuracy: 0.9417\n",
      "Epoch 317/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1256 - accuracy: 0.9523 - val_loss: 0.1557 - val_accuracy: 0.9412\n",
      "Epoch 318/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1255 - accuracy: 0.9521 - val_loss: 0.1544 - val_accuracy: 0.9420\n",
      "Epoch 319/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1255 - accuracy: 0.9522 - val_loss: 0.1549 - val_accuracy: 0.9423\n",
      "Epoch 320/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1253 - accuracy: 0.9522 - val_loss: 0.1555 - val_accuracy: 0.9417\n",
      "Epoch 321/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1252 - accuracy: 0.9523 - val_loss: 0.1549 - val_accuracy: 0.9424\n",
      "Epoch 322/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1252 - accuracy: 0.9522 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 323/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1249 - accuracy: 0.9524 - val_loss: 0.1544 - val_accuracy: 0.9428\n",
      "Epoch 324/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1250 - accuracy: 0.9522 - val_loss: 0.1545 - val_accuracy: 0.9422\n",
      "Epoch 325/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1248 - accuracy: 0.9525 - val_loss: 0.1548 - val_accuracy: 0.9425\n",
      "Epoch 326/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1248 - accuracy: 0.9523 - val_loss: 0.1543 - val_accuracy: 0.9420\n",
      "Epoch 327/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1247 - accuracy: 0.9524 - val_loss: 0.1540 - val_accuracy: 0.9420\n",
      "Epoch 328/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1246 - accuracy: 0.9522 - val_loss: 0.1546 - val_accuracy: 0.9428\n",
      "Epoch 329/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1244 - accuracy: 0.9527 - val_loss: 0.1538 - val_accuracy: 0.9426\n",
      "Epoch 330/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1243 - accuracy: 0.9525 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 331/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1243 - accuracy: 0.9524 - val_loss: 0.1546 - val_accuracy: 0.9418\n",
      "Epoch 332/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1242 - accuracy: 0.9525 - val_loss: 0.1545 - val_accuracy: 0.9428\n",
      "Epoch 333/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1240 - accuracy: 0.9525 - val_loss: 0.1548 - val_accuracy: 0.9418\n",
      "Epoch 334/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1241 - accuracy: 0.9525 - val_loss: 0.1538 - val_accuracy: 0.9425\n",
      "Epoch 335/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1239 - accuracy: 0.9528 - val_loss: 0.1545 - val_accuracy: 0.9427\n",
      "Epoch 336/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1238 - accuracy: 0.9527 - val_loss: 0.1539 - val_accuracy: 0.9418\n",
      "Epoch 337/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1237 - accuracy: 0.9527 - val_loss: 0.1541 - val_accuracy: 0.9425\n",
      "Epoch 338/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1237 - accuracy: 0.9526 - val_loss: 0.1538 - val_accuracy: 0.9429\n",
      "Epoch 339/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1236 - accuracy: 0.9528 - val_loss: 0.1544 - val_accuracy: 0.9426\n",
      "Epoch 340/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1235 - accuracy: 0.9526 - val_loss: 0.1538 - val_accuracy: 0.9420\n",
      "Epoch 341/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1235 - accuracy: 0.9527 - val_loss: 0.1537 - val_accuracy: 0.9425\n",
      "Epoch 342/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1234 - accuracy: 0.9528 - val_loss: 0.1541 - val_accuracy: 0.9423\n",
      "Epoch 343/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1233 - accuracy: 0.9528 - val_loss: 0.1540 - val_accuracy: 0.9425\n",
      "Epoch 344/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1231 - accuracy: 0.9530 - val_loss: 0.1539 - val_accuracy: 0.9423\n",
      "Epoch 345/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1231 - accuracy: 0.9529 - val_loss: 0.1541 - val_accuracy: 0.9420\n",
      "Epoch 346/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1230 - accuracy: 0.9528 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 347/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1229 - accuracy: 0.9529 - val_loss: 0.1556 - val_accuracy: 0.9411\n",
      "Epoch 348/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1228 - accuracy: 0.9530 - val_loss: 0.1535 - val_accuracy: 0.9430\n",
      "Epoch 349/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1227 - accuracy: 0.9530 - val_loss: 0.1534 - val_accuracy: 0.9426\n",
      "Epoch 350/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1227 - accuracy: 0.9529 - val_loss: 0.1544 - val_accuracy: 0.9424\n",
      "Epoch 351/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1226 - accuracy: 0.9530 - val_loss: 0.1540 - val_accuracy: 0.9423\n",
      "Epoch 352/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1225 - accuracy: 0.9527 - val_loss: 0.1540 - val_accuracy: 0.9424\n",
      "Epoch 353/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1225 - accuracy: 0.9529 - val_loss: 0.1539 - val_accuracy: 0.9422\n",
      "Epoch 354/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1223 - accuracy: 0.9530 - val_loss: 0.1534 - val_accuracy: 0.9425\n",
      "Epoch 355/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1222 - accuracy: 0.9531 - val_loss: 0.1540 - val_accuracy: 0.9424\n",
      "Epoch 356/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1221 - accuracy: 0.9531 - val_loss: 0.1540 - val_accuracy: 0.9424\n",
      "Epoch 357/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1221 - accuracy: 0.9532 - val_loss: 0.1536 - val_accuracy: 0.9423\n",
      "Epoch 358/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1219 - accuracy: 0.9531 - val_loss: 0.1539 - val_accuracy: 0.9424\n",
      "Epoch 359/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1219 - accuracy: 0.9532 - val_loss: 0.1534 - val_accuracy: 0.9427\n",
      "Epoch 360/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1218 - accuracy: 0.9532 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 361/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1217 - accuracy: 0.9530 - val_loss: 0.1535 - val_accuracy: 0.9423\n",
      "Epoch 362/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1216 - accuracy: 0.9530 - val_loss: 0.1537 - val_accuracy: 0.9420\n",
      "Epoch 363/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1215 - accuracy: 0.9533 - val_loss: 0.1535 - val_accuracy: 0.9415\n",
      "Epoch 364/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1215 - accuracy: 0.9530 - val_loss: 0.1545 - val_accuracy: 0.9424\n",
      "Epoch 365/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1215 - accuracy: 0.9533 - val_loss: 0.1537 - val_accuracy: 0.9420\n",
      "Epoch 366/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1213 - accuracy: 0.9534 - val_loss: 0.1532 - val_accuracy: 0.9425\n",
      "Epoch 367/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1213 - accuracy: 0.9534 - val_loss: 0.1536 - val_accuracy: 0.9416\n",
      "Epoch 368/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1211 - accuracy: 0.9533 - val_loss: 0.1536 - val_accuracy: 0.9426\n",
      "Epoch 369/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1211 - accuracy: 0.9532 - val_loss: 0.1533 - val_accuracy: 0.9428\n",
      "Epoch 370/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1210 - accuracy: 0.9533 - val_loss: 0.1543 - val_accuracy: 0.9419\n",
      "Epoch 371/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1209 - accuracy: 0.9536 - val_loss: 0.1533 - val_accuracy: 0.9421\n",
      "Epoch 372/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1208 - accuracy: 0.9535 - val_loss: 0.1533 - val_accuracy: 0.9424\n",
      "Epoch 373/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1207 - accuracy: 0.9534 - val_loss: 0.1529 - val_accuracy: 0.9424\n",
      "Epoch 374/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1207 - accuracy: 0.9534 - val_loss: 0.1532 - val_accuracy: 0.9428\n",
      "Epoch 375/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1206 - accuracy: 0.9534 - val_loss: 0.1534 - val_accuracy: 0.9423\n",
      "Epoch 376/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1206 - accuracy: 0.9534 - val_loss: 0.1532 - val_accuracy: 0.9424\n",
      "Epoch 377/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1205 - accuracy: 0.9536 - val_loss: 0.1533 - val_accuracy: 0.9417\n",
      "Epoch 378/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1204 - accuracy: 0.9534 - val_loss: 0.1531 - val_accuracy: 0.9429\n",
      "Epoch 379/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1204 - accuracy: 0.9535 - val_loss: 0.1529 - val_accuracy: 0.9427\n",
      "Epoch 380/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1203 - accuracy: 0.9538 - val_loss: 0.1532 - val_accuracy: 0.9427\n",
      "Epoch 381/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1202 - accuracy: 0.9533 - val_loss: 0.1527 - val_accuracy: 0.9425\n",
      "Epoch 382/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1201 - accuracy: 0.9534 - val_loss: 0.1534 - val_accuracy: 0.9421\n",
      "Epoch 383/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1200 - accuracy: 0.9535 - val_loss: 0.1539 - val_accuracy: 0.9431\n",
      "Epoch 384/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1199 - accuracy: 0.9538 - val_loss: 0.1526 - val_accuracy: 0.9430\n",
      "Epoch 385/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1198 - accuracy: 0.9536 - val_loss: 0.1531 - val_accuracy: 0.9418\n",
      "Epoch 386/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1197 - accuracy: 0.9539 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 387/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1197 - accuracy: 0.9536 - val_loss: 0.1534 - val_accuracy: 0.9421\n",
      "Epoch 388/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1197 - accuracy: 0.9538 - val_loss: 0.1531 - val_accuracy: 0.9421\n",
      "Epoch 389/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1194 - accuracy: 0.9535 - val_loss: 0.1532 - val_accuracy: 0.9424\n",
      "Epoch 390/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1195 - accuracy: 0.9538 - val_loss: 0.1527 - val_accuracy: 0.9425\n",
      "Epoch 391/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1194 - accuracy: 0.9539 - val_loss: 0.1530 - val_accuracy: 0.9428\n",
      "Epoch 392/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1193 - accuracy: 0.9537 - val_loss: 0.1538 - val_accuracy: 0.9429\n",
      "Epoch 393/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1192 - accuracy: 0.9537 - val_loss: 0.1529 - val_accuracy: 0.9419\n",
      "Epoch 394/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1192 - accuracy: 0.9536 - val_loss: 0.1535 - val_accuracy: 0.9420\n",
      "Epoch 395/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1191 - accuracy: 0.9539 - val_loss: 0.1531 - val_accuracy: 0.9426\n",
      "Epoch 396/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1191 - accuracy: 0.9539 - val_loss: 0.1524 - val_accuracy: 0.9423\n",
      "Epoch 397/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1190 - accuracy: 0.9541 - val_loss: 0.1528 - val_accuracy: 0.9428\n",
      "Epoch 398/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1189 - accuracy: 0.9539 - val_loss: 0.1531 - val_accuracy: 0.9422\n",
      "Epoch 399/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1188 - accuracy: 0.9540 - val_loss: 0.1534 - val_accuracy: 0.9420\n",
      "Epoch 400/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1188 - accuracy: 0.9539 - val_loss: 0.1530 - val_accuracy: 0.9424\n",
      "Epoch 401/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1187 - accuracy: 0.9541 - val_loss: 0.1529 - val_accuracy: 0.9425\n",
      "Epoch 402/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1186 - accuracy: 0.9543 - val_loss: 0.1531 - val_accuracy: 0.9425\n",
      "Epoch 403/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1185 - accuracy: 0.9540 - val_loss: 0.1528 - val_accuracy: 0.9426\n",
      "Epoch 404/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1185 - accuracy: 0.9540 - val_loss: 0.1525 - val_accuracy: 0.9422\n",
      "Epoch 405/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1184 - accuracy: 0.9541 - val_loss: 0.1539 - val_accuracy: 0.9422\n",
      "Epoch 406/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1183 - accuracy: 0.9543 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 407/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1182 - accuracy: 0.9541 - val_loss: 0.1527 - val_accuracy: 0.9428\n",
      "Epoch 408/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1182 - accuracy: 0.9542 - val_loss: 0.1533 - val_accuracy: 0.9422\n",
      "Epoch 409/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1181 - accuracy: 0.9541 - val_loss: 0.1529 - val_accuracy: 0.9430\n",
      "Epoch 410/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1180 - accuracy: 0.9542 - val_loss: 0.1531 - val_accuracy: 0.9421\n",
      "Epoch 411/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1178 - accuracy: 0.9544 - val_loss: 0.1542 - val_accuracy: 0.9422\n",
      "Epoch 412/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1179 - accuracy: 0.9543 - val_loss: 0.1534 - val_accuracy: 0.9422\n",
      "Epoch 413/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1178 - accuracy: 0.9544 - val_loss: 0.1532 - val_accuracy: 0.9423\n",
      "Epoch 414/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1178 - accuracy: 0.9542 - val_loss: 0.1532 - val_accuracy: 0.9425\n",
      "Epoch 415/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1176 - accuracy: 0.9544 - val_loss: 0.1534 - val_accuracy: 0.9422\n",
      "Epoch 416/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1176 - accuracy: 0.9543 - val_loss: 0.1529 - val_accuracy: 0.9419\n",
      "Epoch 417/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1175 - accuracy: 0.9545 - val_loss: 0.1526 - val_accuracy: 0.9422\n",
      "Epoch 418/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1175 - accuracy: 0.9542 - val_loss: 0.1530 - val_accuracy: 0.9424\n",
      "Epoch 419/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1174 - accuracy: 0.9546 - val_loss: 0.1542 - val_accuracy: 0.9425\n",
      "Epoch 420/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1175 - accuracy: 0.9545 - val_loss: 0.1531 - val_accuracy: 0.9423\n",
      "Epoch 421/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1173 - accuracy: 0.9543 - val_loss: 0.1532 - val_accuracy: 0.9425\n",
      "Epoch 422/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1172 - accuracy: 0.9543 - val_loss: 0.1535 - val_accuracy: 0.9419\n",
      "Epoch 423/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1172 - accuracy: 0.9545 - val_loss: 0.1532 - val_accuracy: 0.9419\n",
      "Epoch 424/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1170 - accuracy: 0.9544 - val_loss: 0.1536 - val_accuracy: 0.9424\n",
      "Epoch 425/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1169 - accuracy: 0.9546 - val_loss: 0.1529 - val_accuracy: 0.9419\n",
      "Epoch 426/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1169 - accuracy: 0.9546 - val_loss: 0.1531 - val_accuracy: 0.9421\n",
      "Epoch 427/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1169 - accuracy: 0.9546 - val_loss: 0.1530 - val_accuracy: 0.9421\n",
      "Epoch 428/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1167 - accuracy: 0.9546 - val_loss: 0.1532 - val_accuracy: 0.9425\n",
      "Epoch 429/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1166 - accuracy: 0.9547 - val_loss: 0.1538 - val_accuracy: 0.9426\n",
      "Epoch 430/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1166 - accuracy: 0.9546 - val_loss: 0.1538 - val_accuracy: 0.9420\n",
      "Epoch 431/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1167 - accuracy: 0.9549 - val_loss: 0.1528 - val_accuracy: 0.9423\n",
      "Epoch 432/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1165 - accuracy: 0.9549 - val_loss: 0.1538 - val_accuracy: 0.9421\n",
      "Epoch 433/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1164 - accuracy: 0.9547 - val_loss: 0.1529 - val_accuracy: 0.9426\n",
      "Epoch 434/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1163 - accuracy: 0.9547 - val_loss: 0.1540 - val_accuracy: 0.9417\n",
      "Epoch 435/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1162 - accuracy: 0.9548 - val_loss: 0.1530 - val_accuracy: 0.9428\n",
      "Epoch 436/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1162 - accuracy: 0.9548 - val_loss: 0.1536 - val_accuracy: 0.9426\n",
      "Epoch 437/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1161 - accuracy: 0.9548 - val_loss: 0.1535 - val_accuracy: 0.9428\n",
      "Epoch 438/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1160 - accuracy: 0.9549 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 439/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1160 - accuracy: 0.9547 - val_loss: 0.1536 - val_accuracy: 0.9423\n",
      "Epoch 440/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1161 - accuracy: 0.9546 - val_loss: 0.1538 - val_accuracy: 0.9423\n",
      "Epoch 441/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1159 - accuracy: 0.9548 - val_loss: 0.1538 - val_accuracy: 0.9420\n",
      "Epoch 442/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1158 - accuracy: 0.9551 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 443/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1157 - accuracy: 0.9550 - val_loss: 0.1539 - val_accuracy: 0.9425\n",
      "Epoch 444/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1157 - accuracy: 0.9551 - val_loss: 0.1532 - val_accuracy: 0.9422\n",
      "Epoch 445/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1157 - accuracy: 0.9549 - val_loss: 0.1532 - val_accuracy: 0.9413\n",
      "Epoch 446/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1156 - accuracy: 0.9548 - val_loss: 0.1538 - val_accuracy: 0.9426\n",
      "Epoch 447/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1155 - accuracy: 0.9550 - val_loss: 0.1531 - val_accuracy: 0.9421\n",
      "Epoch 448/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1155 - accuracy: 0.9549 - val_loss: 0.1537 - val_accuracy: 0.9422\n",
      "Epoch 449/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1154 - accuracy: 0.9552 - val_loss: 0.1539 - val_accuracy: 0.9417\n",
      "Epoch 450/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1153 - accuracy: 0.9550 - val_loss: 0.1532 - val_accuracy: 0.9420\n",
      "Epoch 451/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1152 - accuracy: 0.9552 - val_loss: 0.1530 - val_accuracy: 0.9415\n",
      "Epoch 452/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1152 - accuracy: 0.9552 - val_loss: 0.1560 - val_accuracy: 0.9412\n",
      "Epoch 453/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1151 - accuracy: 0.9549 - val_loss: 0.1539 - val_accuracy: 0.9416\n",
      "Epoch 454/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1150 - accuracy: 0.9552 - val_loss: 0.1537 - val_accuracy: 0.9417\n",
      "Epoch 455/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1150 - accuracy: 0.9552 - val_loss: 0.1537 - val_accuracy: 0.9416\n",
      "Epoch 456/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1149 - accuracy: 0.9553 - val_loss: 0.1533 - val_accuracy: 0.9412\n",
      "Epoch 457/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1149 - accuracy: 0.9552 - val_loss: 0.1532 - val_accuracy: 0.9423\n",
      "Epoch 458/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1149 - accuracy: 0.9551 - val_loss: 0.1539 - val_accuracy: 0.9411\n",
      "Epoch 459/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1147 - accuracy: 0.9553 - val_loss: 0.1536 - val_accuracy: 0.9422\n",
      "Epoch 460/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1147 - accuracy: 0.9552 - val_loss: 0.1539 - val_accuracy: 0.9415\n",
      "Epoch 461/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1146 - accuracy: 0.9556 - val_loss: 0.1539 - val_accuracy: 0.9420\n",
      "Epoch 462/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1145 - accuracy: 0.9554 - val_loss: 0.1539 - val_accuracy: 0.9412\n",
      "Epoch 463/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1145 - accuracy: 0.9553 - val_loss: 0.1537 - val_accuracy: 0.9423\n",
      "Epoch 464/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1144 - accuracy: 0.9554 - val_loss: 0.1541 - val_accuracy: 0.9419\n",
      "Epoch 465/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1143 - accuracy: 0.9554 - val_loss: 0.1538 - val_accuracy: 0.9417\n",
      "Epoch 466/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1143 - accuracy: 0.9556 - val_loss: 0.1539 - val_accuracy: 0.9423\n",
      "Epoch 467/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1142 - accuracy: 0.9555 - val_loss: 0.1540 - val_accuracy: 0.9419\n",
      "Epoch 468/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1141 - accuracy: 0.9556 - val_loss: 0.1532 - val_accuracy: 0.9420\n",
      "Epoch 469/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1140 - accuracy: 0.9553 - val_loss: 0.1536 - val_accuracy: 0.9419\n",
      "Epoch 470/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1140 - accuracy: 0.9555 - val_loss: 0.1539 - val_accuracy: 0.9425\n",
      "Epoch 471/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1139 - accuracy: 0.9554 - val_loss: 0.1539 - val_accuracy: 0.9421\n",
      "Epoch 472/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1138 - accuracy: 0.9556 - val_loss: 0.1532 - val_accuracy: 0.9421\n",
      "Epoch 473/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1138 - accuracy: 0.9558 - val_loss: 0.1536 - val_accuracy: 0.9423\n",
      "Epoch 474/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1137 - accuracy: 0.9557 - val_loss: 0.1539 - val_accuracy: 0.9420\n",
      "Epoch 475/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1136 - accuracy: 0.9556 - val_loss: 0.1539 - val_accuracy: 0.9425\n",
      "Epoch 476/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1136 - accuracy: 0.9556 - val_loss: 0.1549 - val_accuracy: 0.9404\n",
      "Epoch 477/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1135 - accuracy: 0.9556 - val_loss: 0.1542 - val_accuracy: 0.9418\n",
      "Epoch 478/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1134 - accuracy: 0.9556 - val_loss: 0.1551 - val_accuracy: 0.9412\n",
      "Epoch 479/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1135 - accuracy: 0.9556 - val_loss: 0.1543 - val_accuracy: 0.9417\n",
      "Epoch 480/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1134 - accuracy: 0.9557 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 481/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1133 - accuracy: 0.9559 - val_loss: 0.1537 - val_accuracy: 0.9411\n",
      "Epoch 482/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1131 - accuracy: 0.9559 - val_loss: 0.1548 - val_accuracy: 0.9414\n",
      "Epoch 483/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1133 - accuracy: 0.9558 - val_loss: 0.1538 - val_accuracy: 0.9419\n",
      "Epoch 484/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1131 - accuracy: 0.9560 - val_loss: 0.1543 - val_accuracy: 0.9418\n",
      "Epoch 485/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1130 - accuracy: 0.9558 - val_loss: 0.1543 - val_accuracy: 0.9417\n",
      "Epoch 486/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1130 - accuracy: 0.9559 - val_loss: 0.1536 - val_accuracy: 0.9420\n",
      "Epoch 487/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1129 - accuracy: 0.9558 - val_loss: 0.1544 - val_accuracy: 0.9417\n",
      "Epoch 488/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1128 - accuracy: 0.9559 - val_loss: 0.1542 - val_accuracy: 0.9423\n",
      "Epoch 489/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1129 - accuracy: 0.9560 - val_loss: 0.1541 - val_accuracy: 0.9422\n",
      "Epoch 490/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1128 - accuracy: 0.9558 - val_loss: 0.1544 - val_accuracy: 0.9405\n",
      "Epoch 491/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1127 - accuracy: 0.9562 - val_loss: 0.1544 - val_accuracy: 0.9413\n",
      "Epoch 492/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1127 - accuracy: 0.9561 - val_loss: 0.1542 - val_accuracy: 0.9420\n",
      "Epoch 493/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1125 - accuracy: 0.9562 - val_loss: 0.1542 - val_accuracy: 0.9418\n",
      "Epoch 494/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1124 - accuracy: 0.9563 - val_loss: 0.1533 - val_accuracy: 0.9418\n",
      "Epoch 495/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1124 - accuracy: 0.9563 - val_loss: 0.1541 - val_accuracy: 0.9419\n",
      "Epoch 496/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1123 - accuracy: 0.9563 - val_loss: 0.1553 - val_accuracy: 0.9416\n",
      "Epoch 497/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1122 - accuracy: 0.9564 - val_loss: 0.1554 - val_accuracy: 0.9415\n",
      "Epoch 498/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1123 - accuracy: 0.9564 - val_loss: 0.1544 - val_accuracy: 0.9411\n",
      "Epoch 499/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1122 - accuracy: 0.9562 - val_loss: 0.1542 - val_accuracy: 0.9418\n",
      "Epoch 500/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1121 - accuracy: 0.9563 - val_loss: 0.1543 - val_accuracy: 0.9415\n",
      "Epoch 501/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1120 - accuracy: 0.9563 - val_loss: 0.1546 - val_accuracy: 0.9414\n",
      "Epoch 502/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1119 - accuracy: 0.9566 - val_loss: 0.1542 - val_accuracy: 0.9414\n",
      "Epoch 503/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1118 - accuracy: 0.9564 - val_loss: 0.1560 - val_accuracy: 0.9408\n",
      "Epoch 504/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1118 - accuracy: 0.9564 - val_loss: 0.1548 - val_accuracy: 0.9415\n",
      "Epoch 505/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1117 - accuracy: 0.9565 - val_loss: 0.1548 - val_accuracy: 0.9415\n",
      "Epoch 506/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1117 - accuracy: 0.9566 - val_loss: 0.1554 - val_accuracy: 0.9405\n",
      "Epoch 507/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1117 - accuracy: 0.9564 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 508/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1116 - accuracy: 0.9564 - val_loss: 0.1547 - val_accuracy: 0.9414\n",
      "Epoch 509/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1116 - accuracy: 0.9563 - val_loss: 0.1540 - val_accuracy: 0.9420\n",
      "Epoch 510/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1114 - accuracy: 0.9566 - val_loss: 0.1555 - val_accuracy: 0.9407\n",
      "Epoch 511/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1114 - accuracy: 0.9565 - val_loss: 0.1547 - val_accuracy: 0.9418\n",
      "Epoch 512/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1114 - accuracy: 0.9565 - val_loss: 0.1546 - val_accuracy: 0.9413\n",
      "Epoch 513/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1112 - accuracy: 0.9567 - val_loss: 0.1549 - val_accuracy: 0.9412\n",
      "Epoch 514/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1112 - accuracy: 0.9564 - val_loss: 0.1563 - val_accuracy: 0.9406\n",
      "Epoch 515/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1112 - accuracy: 0.9567 - val_loss: 0.1556 - val_accuracy: 0.9402\n",
      "Epoch 516/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1111 - accuracy: 0.9564 - val_loss: 0.1547 - val_accuracy: 0.9418\n",
      "Epoch 517/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1110 - accuracy: 0.9565 - val_loss: 0.1547 - val_accuracy: 0.9416\n",
      "Epoch 518/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1109 - accuracy: 0.9567 - val_loss: 0.1549 - val_accuracy: 0.9409\n",
      "Epoch 519/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1109 - accuracy: 0.9568 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 520/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1108 - accuracy: 0.9569 - val_loss: 0.1558 - val_accuracy: 0.9411\n",
      "Epoch 521/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1108 - accuracy: 0.9566 - val_loss: 0.1549 - val_accuracy: 0.9414\n",
      "Epoch 522/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1106 - accuracy: 0.9570 - val_loss: 0.1551 - val_accuracy: 0.9415\n",
      "Epoch 523/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1106 - accuracy: 0.9569 - val_loss: 0.1556 - val_accuracy: 0.9417\n",
      "Epoch 524/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1105 - accuracy: 0.9568 - val_loss: 0.1549 - val_accuracy: 0.9412\n",
      "Epoch 525/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1106 - accuracy: 0.9567 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 526/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1105 - accuracy: 0.9570 - val_loss: 0.1550 - val_accuracy: 0.9414\n",
      "Epoch 527/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1104 - accuracy: 0.9565 - val_loss: 0.1555 - val_accuracy: 0.9411\n",
      "Epoch 528/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1103 - accuracy: 0.9569 - val_loss: 0.1554 - val_accuracy: 0.9408\n",
      "Epoch 529/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1102 - accuracy: 0.9571 - val_loss: 0.1564 - val_accuracy: 0.9411\n",
      "Epoch 530/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1102 - accuracy: 0.9569 - val_loss: 0.1543 - val_accuracy: 0.9412\n",
      "Epoch 531/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1101 - accuracy: 0.9570 - val_loss: 0.1556 - val_accuracy: 0.9415\n",
      "Epoch 532/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1101 - accuracy: 0.9569 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 533/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1100 - accuracy: 0.9568 - val_loss: 0.1567 - val_accuracy: 0.9416\n",
      "Epoch 534/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1099 - accuracy: 0.9571 - val_loss: 0.1554 - val_accuracy: 0.9413\n",
      "Epoch 535/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1099 - accuracy: 0.9569 - val_loss: 0.1556 - val_accuracy: 0.9414\n",
      "Epoch 536/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1098 - accuracy: 0.9572 - val_loss: 0.1556 - val_accuracy: 0.9413\n",
      "Epoch 537/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1098 - accuracy: 0.9571 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 538/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1096 - accuracy: 0.9572 - val_loss: 0.1566 - val_accuracy: 0.9405\n",
      "Epoch 539/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1096 - accuracy: 0.9573 - val_loss: 0.1557 - val_accuracy: 0.9410\n",
      "Epoch 540/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1095 - accuracy: 0.9572 - val_loss: 0.1560 - val_accuracy: 0.9413\n",
      "Epoch 541/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1095 - accuracy: 0.9572 - val_loss: 0.1553 - val_accuracy: 0.9405\n",
      "Epoch 542/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1094 - accuracy: 0.9572 - val_loss: 0.1559 - val_accuracy: 0.9416\n",
      "Epoch 543/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1094 - accuracy: 0.9572 - val_loss: 0.1565 - val_accuracy: 0.9413\n",
      "Epoch 544/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1093 - accuracy: 0.9573 - val_loss: 0.1563 - val_accuracy: 0.9410\n",
      "Epoch 545/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1092 - accuracy: 0.9576 - val_loss: 0.1562 - val_accuracy: 0.9418\n",
      "Epoch 546/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1093 - accuracy: 0.9574 - val_loss: 0.1550 - val_accuracy: 0.9413\n",
      "Epoch 547/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1091 - accuracy: 0.9573 - val_loss: 0.1554 - val_accuracy: 0.9420\n",
      "Epoch 548/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1091 - accuracy: 0.9571 - val_loss: 0.1554 - val_accuracy: 0.9406\n",
      "Epoch 549/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1091 - accuracy: 0.9573 - val_loss: 0.1565 - val_accuracy: 0.9408\n",
      "Epoch 550/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1090 - accuracy: 0.9574 - val_loss: 0.1568 - val_accuracy: 0.9400\n",
      "Epoch 551/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1089 - accuracy: 0.9574 - val_loss: 0.1560 - val_accuracy: 0.9415\n",
      "Epoch 552/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1089 - accuracy: 0.9574 - val_loss: 0.1554 - val_accuracy: 0.9413\n",
      "Epoch 553/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1087 - accuracy: 0.9575 - val_loss: 0.1559 - val_accuracy: 0.9409\n",
      "Epoch 554/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1087 - accuracy: 0.9574 - val_loss: 0.1561 - val_accuracy: 0.9407\n",
      "Epoch 555/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1086 - accuracy: 0.9578 - val_loss: 0.1560 - val_accuracy: 0.9400\n",
      "Epoch 556/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1085 - accuracy: 0.9576 - val_loss: 0.1562 - val_accuracy: 0.9409\n",
      "Epoch 557/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1085 - accuracy: 0.9575 - val_loss: 0.1560 - val_accuracy: 0.9414\n",
      "Epoch 558/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1084 - accuracy: 0.9575 - val_loss: 0.1558 - val_accuracy: 0.9404\n",
      "Epoch 559/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1083 - accuracy: 0.9575 - val_loss: 0.1572 - val_accuracy: 0.9406\n",
      "Epoch 560/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1082 - accuracy: 0.9575 - val_loss: 0.1562 - val_accuracy: 0.9408\n",
      "Epoch 561/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1082 - accuracy: 0.9577 - val_loss: 0.1579 - val_accuracy: 0.9406\n",
      "Epoch 562/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1082 - accuracy: 0.9576 - val_loss: 0.1560 - val_accuracy: 0.9413\n",
      "Epoch 563/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1081 - accuracy: 0.9577 - val_loss: 0.1563 - val_accuracy: 0.9409\n",
      "Epoch 564/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1082 - accuracy: 0.9578 - val_loss: 0.1559 - val_accuracy: 0.9416\n",
      "Epoch 565/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1079 - accuracy: 0.9578 - val_loss: 0.1563 - val_accuracy: 0.9415\n",
      "Epoch 566/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1079 - accuracy: 0.9579 - val_loss: 0.1560 - val_accuracy: 0.9412\n",
      "Epoch 567/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1079 - accuracy: 0.9578 - val_loss: 0.1564 - val_accuracy: 0.9405\n",
      "Epoch 568/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1078 - accuracy: 0.9577 - val_loss: 0.1564 - val_accuracy: 0.9412\n",
      "Epoch 569/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1078 - accuracy: 0.9578 - val_loss: 0.1568 - val_accuracy: 0.9406\n",
      "Epoch 570/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1077 - accuracy: 0.9579 - val_loss: 0.1570 - val_accuracy: 0.9404\n",
      "Epoch 571/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1076 - accuracy: 0.9579 - val_loss: 0.1562 - val_accuracy: 0.9413\n",
      "Epoch 572/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1075 - accuracy: 0.9579 - val_loss: 0.1564 - val_accuracy: 0.9410\n",
      "Epoch 573/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1075 - accuracy: 0.9580 - val_loss: 0.1564 - val_accuracy: 0.9412\n",
      "Epoch 574/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1075 - accuracy: 0.9578 - val_loss: 0.1571 - val_accuracy: 0.9405\n",
      "Epoch 575/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1074 - accuracy: 0.9580 - val_loss: 0.1564 - val_accuracy: 0.9412\n",
      "Epoch 576/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1073 - accuracy: 0.9579 - val_loss: 0.1567 - val_accuracy: 0.9404\n",
      "Epoch 577/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1072 - accuracy: 0.9581 - val_loss: 0.1565 - val_accuracy: 0.9409\n",
      "Epoch 578/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1072 - accuracy: 0.9580 - val_loss: 0.1572 - val_accuracy: 0.9415\n",
      "Epoch 579/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1071 - accuracy: 0.9583 - val_loss: 0.1573 - val_accuracy: 0.9407\n",
      "Epoch 580/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1070 - accuracy: 0.9581 - val_loss: 0.1574 - val_accuracy: 0.9408\n",
      "Epoch 581/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1071 - accuracy: 0.9581 - val_loss: 0.1575 - val_accuracy: 0.9406\n",
      "Epoch 582/800\n",
      "6000/6000 [==============================] - 0s 67us/step - loss: 0.1070 - accuracy: 0.9579 - val_loss: 0.1574 - val_accuracy: 0.9408\n",
      "Epoch 583/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1067 - accuracy: 0.9584 - val_loss: 0.1566 - val_accuracy: 0.9408\n",
      "Epoch 584/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1067 - accuracy: 0.9582 - val_loss: 0.1573 - val_accuracy: 0.9409\n",
      "Epoch 585/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1068 - accuracy: 0.9581 - val_loss: 0.1580 - val_accuracy: 0.9399\n",
      "Epoch 586/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1066 - accuracy: 0.9584 - val_loss: 0.1580 - val_accuracy: 0.9404\n",
      "Epoch 587/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1067 - accuracy: 0.9584 - val_loss: 0.1570 - val_accuracy: 0.9410\n",
      "Epoch 588/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1065 - accuracy: 0.9583 - val_loss: 0.1569 - val_accuracy: 0.9404\n",
      "Epoch 589/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1065 - accuracy: 0.9585 - val_loss: 0.1572 - val_accuracy: 0.9407\n",
      "Epoch 590/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1064 - accuracy: 0.9584 - val_loss: 0.1575 - val_accuracy: 0.9415\n",
      "Epoch 591/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.1583 - val_accuracy: 0.9405\n",
      "Epoch 592/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1062 - accuracy: 0.9584 - val_loss: 0.1579 - val_accuracy: 0.9402\n",
      "Epoch 593/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1061 - accuracy: 0.9584 - val_loss: 0.1584 - val_accuracy: 0.9399\n",
      "Epoch 594/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1062 - accuracy: 0.9581 - val_loss: 0.1589 - val_accuracy: 0.9404\n",
      "Epoch 595/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1060 - accuracy: 0.9583 - val_loss: 0.1590 - val_accuracy: 0.9413\n",
      "Epoch 596/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1061 - accuracy: 0.9584 - val_loss: 0.1578 - val_accuracy: 0.9404\n",
      "Epoch 597/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1060 - accuracy: 0.9585 - val_loss: 0.1578 - val_accuracy: 0.9398\n",
      "Epoch 598/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1058 - accuracy: 0.9585 - val_loss: 0.1588 - val_accuracy: 0.9408\n",
      "Epoch 599/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1058 - accuracy: 0.9589 - val_loss: 0.1578 - val_accuracy: 0.9397\n",
      "Epoch 600/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1057 - accuracy: 0.9586 - val_loss: 0.1574 - val_accuracy: 0.9411\n",
      "Epoch 601/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1057 - accuracy: 0.9585 - val_loss: 0.1580 - val_accuracy: 0.9411\n",
      "Epoch 602/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1058 - accuracy: 0.9586 - val_loss: 0.1586 - val_accuracy: 0.9399\n",
      "Epoch 603/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1055 - accuracy: 0.9587 - val_loss: 0.1575 - val_accuracy: 0.9408\n",
      "Epoch 604/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1054 - accuracy: 0.9587 - val_loss: 0.1575 - val_accuracy: 0.9404\n",
      "Epoch 605/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1056 - accuracy: 0.9587 - val_loss: 0.1579 - val_accuracy: 0.9398\n",
      "Epoch 606/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1053 - accuracy: 0.9585 - val_loss: 0.1579 - val_accuracy: 0.9406\n",
      "Epoch 607/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1053 - accuracy: 0.9587 - val_loss: 0.1584 - val_accuracy: 0.9405\n",
      "Epoch 608/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1052 - accuracy: 0.9587 - val_loss: 0.1575 - val_accuracy: 0.9402\n",
      "Epoch 609/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1052 - accuracy: 0.9589 - val_loss: 0.1585 - val_accuracy: 0.9399\n",
      "Epoch 610/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1051 - accuracy: 0.9588 - val_loss: 0.1590 - val_accuracy: 0.9403\n",
      "Epoch 611/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1051 - accuracy: 0.9589 - val_loss: 0.1591 - val_accuracy: 0.9402\n",
      "Epoch 612/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1049 - accuracy: 0.9590 - val_loss: 0.1584 - val_accuracy: 0.9407\n",
      "Epoch 613/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1049 - accuracy: 0.9585 - val_loss: 0.1590 - val_accuracy: 0.9393\n",
      "Epoch 614/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1048 - accuracy: 0.9590 - val_loss: 0.1582 - val_accuracy: 0.9410\n",
      "Epoch 615/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1047 - accuracy: 0.9589 - val_loss: 0.1593 - val_accuracy: 0.9411\n",
      "Epoch 616/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1047 - accuracy: 0.9589 - val_loss: 0.1586 - val_accuracy: 0.9406\n",
      "Epoch 617/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1046 - accuracy: 0.9591 - val_loss: 0.1608 - val_accuracy: 0.9402\n",
      "Epoch 618/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1046 - accuracy: 0.9588 - val_loss: 0.1589 - val_accuracy: 0.9404\n",
      "Epoch 619/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1046 - accuracy: 0.9591 - val_loss: 0.1602 - val_accuracy: 0.9402\n",
      "Epoch 620/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1045 - accuracy: 0.9588 - val_loss: 0.1603 - val_accuracy: 0.9402\n",
      "Epoch 621/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1045 - accuracy: 0.9591 - val_loss: 0.1585 - val_accuracy: 0.9409\n",
      "Epoch 622/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1042 - accuracy: 0.9593 - val_loss: 0.1588 - val_accuracy: 0.9410\n",
      "Epoch 623/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1043 - accuracy: 0.9590 - val_loss: 0.1592 - val_accuracy: 0.9405\n",
      "Epoch 624/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1042 - accuracy: 0.9590 - val_loss: 0.1595 - val_accuracy: 0.9407\n",
      "Epoch 625/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1040 - accuracy: 0.9592 - val_loss: 0.1587 - val_accuracy: 0.9405\n",
      "Epoch 626/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1042 - accuracy: 0.9592 - val_loss: 0.1592 - val_accuracy: 0.9404\n",
      "Epoch 627/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1040 - accuracy: 0.9593 - val_loss: 0.1598 - val_accuracy: 0.9406\n",
      "Epoch 628/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1040 - accuracy: 0.9596 - val_loss: 0.1591 - val_accuracy: 0.9399\n",
      "Epoch 629/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1039 - accuracy: 0.9590 - val_loss: 0.1587 - val_accuracy: 0.9409\n",
      "Epoch 630/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1038 - accuracy: 0.9592 - val_loss: 0.1595 - val_accuracy: 0.9405\n",
      "Epoch 631/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1038 - accuracy: 0.9595 - val_loss: 0.1590 - val_accuracy: 0.9404\n",
      "Epoch 632/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1037 - accuracy: 0.9594 - val_loss: 0.1590 - val_accuracy: 0.9410\n",
      "Epoch 633/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1036 - accuracy: 0.9592 - val_loss: 0.1586 - val_accuracy: 0.9404\n",
      "Epoch 634/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1036 - accuracy: 0.9594 - val_loss: 0.1593 - val_accuracy: 0.9407\n",
      "Epoch 635/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1035 - accuracy: 0.9595 - val_loss: 0.1594 - val_accuracy: 0.9402\n",
      "Epoch 636/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1034 - accuracy: 0.9593 - val_loss: 0.1589 - val_accuracy: 0.9402\n",
      "Epoch 637/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1035 - accuracy: 0.9592 - val_loss: 0.1601 - val_accuracy: 0.9404\n",
      "Epoch 638/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1033 - accuracy: 0.9595 - val_loss: 0.1609 - val_accuracy: 0.9393\n",
      "Epoch 639/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1033 - accuracy: 0.9595 - val_loss: 0.1591 - val_accuracy: 0.9408\n",
      "Epoch 640/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1032 - accuracy: 0.9597 - val_loss: 0.1597 - val_accuracy: 0.9406\n",
      "Epoch 641/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1031 - accuracy: 0.9594 - val_loss: 0.1600 - val_accuracy: 0.9402\n",
      "Epoch 642/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1031 - accuracy: 0.9595 - val_loss: 0.1592 - val_accuracy: 0.9406\n",
      "Epoch 643/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1030 - accuracy: 0.9595 - val_loss: 0.1599 - val_accuracy: 0.9397\n",
      "Epoch 644/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1030 - accuracy: 0.9596 - val_loss: 0.1606 - val_accuracy: 0.9397\n",
      "Epoch 645/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1029 - accuracy: 0.9597 - val_loss: 0.1600 - val_accuracy: 0.9400\n",
      "Epoch 646/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1028 - accuracy: 0.9600 - val_loss: 0.1605 - val_accuracy: 0.9402\n",
      "Epoch 647/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1028 - accuracy: 0.9596 - val_loss: 0.1597 - val_accuracy: 0.9398\n",
      "Epoch 648/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1026 - accuracy: 0.9598 - val_loss: 0.1609 - val_accuracy: 0.9400\n",
      "Epoch 649/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1026 - accuracy: 0.9596 - val_loss: 0.1599 - val_accuracy: 0.9405\n",
      "Epoch 650/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1025 - accuracy: 0.9598 - val_loss: 0.1606 - val_accuracy: 0.9398\n",
      "Epoch 651/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1025 - accuracy: 0.9599 - val_loss: 0.1619 - val_accuracy: 0.9402\n",
      "Epoch 652/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1023 - accuracy: 0.9599 - val_loss: 0.1603 - val_accuracy: 0.9405\n",
      "Epoch 653/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1023 - accuracy: 0.9598 - val_loss: 0.1611 - val_accuracy: 0.9396\n",
      "Epoch 654/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1023 - accuracy: 0.9600 - val_loss: 0.1602 - val_accuracy: 0.9400\n",
      "Epoch 655/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1022 - accuracy: 0.9600 - val_loss: 0.1605 - val_accuracy: 0.9403\n",
      "Epoch 656/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1021 - accuracy: 0.9600 - val_loss: 0.1607 - val_accuracy: 0.9405\n",
      "Epoch 657/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1022 - accuracy: 0.9600 - val_loss: 0.1599 - val_accuracy: 0.9401\n",
      "Epoch 658/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1020 - accuracy: 0.9601 - val_loss: 0.1616 - val_accuracy: 0.9395\n",
      "Epoch 659/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1019 - accuracy: 0.9600 - val_loss: 0.1619 - val_accuracy: 0.9396\n",
      "Epoch 660/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1020 - accuracy: 0.9600 - val_loss: 0.1609 - val_accuracy: 0.9404\n",
      "Epoch 661/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1018 - accuracy: 0.9600 - val_loss: 0.1611 - val_accuracy: 0.9405\n",
      "Epoch 662/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1017 - accuracy: 0.9602 - val_loss: 0.1607 - val_accuracy: 0.9404\n",
      "Epoch 663/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1018 - accuracy: 0.9601 - val_loss: 0.1606 - val_accuracy: 0.9406\n",
      "Epoch 664/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1017 - accuracy: 0.9599 - val_loss: 0.1607 - val_accuracy: 0.9398\n",
      "Epoch 665/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1015 - accuracy: 0.9602 - val_loss: 0.1612 - val_accuracy: 0.9398\n",
      "Epoch 666/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1014 - accuracy: 0.9602 - val_loss: 0.1620 - val_accuracy: 0.9399\n",
      "Epoch 667/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1014 - accuracy: 0.9602 - val_loss: 0.1602 - val_accuracy: 0.9394\n",
      "Epoch 668/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1014 - accuracy: 0.9603 - val_loss: 0.1613 - val_accuracy: 0.9395\n",
      "Epoch 669/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1013 - accuracy: 0.9603 - val_loss: 0.1619 - val_accuracy: 0.9401\n",
      "Epoch 670/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1014 - accuracy: 0.9604 - val_loss: 0.1620 - val_accuracy: 0.9406\n",
      "Epoch 671/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1011 - accuracy: 0.9607 - val_loss: 0.1618 - val_accuracy: 0.9390\n",
      "Epoch 672/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1011 - accuracy: 0.9603 - val_loss: 0.1614 - val_accuracy: 0.9399\n",
      "Epoch 673/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1010 - accuracy: 0.9606 - val_loss: 0.1628 - val_accuracy: 0.9397\n",
      "Epoch 674/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1010 - accuracy: 0.9601 - val_loss: 0.1619 - val_accuracy: 0.9402\n",
      "Epoch 675/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1009 - accuracy: 0.9604 - val_loss: 0.1607 - val_accuracy: 0.9404\n",
      "Epoch 676/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1009 - accuracy: 0.9605 - val_loss: 0.1619 - val_accuracy: 0.9397\n",
      "Epoch 677/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1008 - accuracy: 0.9603 - val_loss: 0.1617 - val_accuracy: 0.9402\n",
      "Epoch 678/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1007 - accuracy: 0.9606 - val_loss: 0.1634 - val_accuracy: 0.9391\n",
      "Epoch 679/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1006 - accuracy: 0.9607 - val_loss: 0.1624 - val_accuracy: 0.9403\n",
      "Epoch 680/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1005 - accuracy: 0.9605 - val_loss: 0.1618 - val_accuracy: 0.9395\n",
      "Epoch 681/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1004 - accuracy: 0.9605 - val_loss: 0.1622 - val_accuracy: 0.9404\n",
      "Epoch 682/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1004 - accuracy: 0.9607 - val_loss: 0.1632 - val_accuracy: 0.9405\n",
      "Epoch 683/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1004 - accuracy: 0.9607 - val_loss: 0.1635 - val_accuracy: 0.9394\n",
      "Epoch 684/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1003 - accuracy: 0.9607 - val_loss: 0.1623 - val_accuracy: 0.9403\n",
      "Epoch 685/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1002 - accuracy: 0.9608 - val_loss: 0.1625 - val_accuracy: 0.9399\n",
      "Epoch 686/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1002 - accuracy: 0.9608 - val_loss: 0.1634 - val_accuracy: 0.9398\n",
      "Epoch 687/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1001 - accuracy: 0.9607 - val_loss: 0.1634 - val_accuracy: 0.9394\n",
      "Epoch 688/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.1001 - accuracy: 0.9607 - val_loss: 0.1628 - val_accuracy: 0.9397\n",
      "Epoch 689/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1000 - accuracy: 0.9608 - val_loss: 0.1616 - val_accuracy: 0.9398\n",
      "Epoch 690/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.1000 - accuracy: 0.9608 - val_loss: 0.1618 - val_accuracy: 0.9402\n",
      "Epoch 691/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0999 - accuracy: 0.9607 - val_loss: 0.1619 - val_accuracy: 0.9397\n",
      "Epoch 692/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0997 - accuracy: 0.9609 - val_loss: 0.1631 - val_accuracy: 0.9403\n",
      "Epoch 693/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.0997 - accuracy: 0.9609 - val_loss: 0.1632 - val_accuracy: 0.9392\n",
      "Epoch 694/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0998 - accuracy: 0.9609 - val_loss: 0.1627 - val_accuracy: 0.9394\n",
      "Epoch 695/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.0996 - accuracy: 0.9608 - val_loss: 0.1625 - val_accuracy: 0.9395\n",
      "Epoch 696/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0994 - accuracy: 0.9611 - val_loss: 0.1644 - val_accuracy: 0.9388\n",
      "Epoch 697/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0995 - accuracy: 0.9609 - val_loss: 0.1644 - val_accuracy: 0.9389\n",
      "Epoch 698/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0994 - accuracy: 0.9610 - val_loss: 0.1636 - val_accuracy: 0.9390\n",
      "Epoch 699/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0992 - accuracy: 0.9610 - val_loss: 0.1633 - val_accuracy: 0.9396\n",
      "Epoch 700/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0992 - accuracy: 0.9610 - val_loss: 0.1647 - val_accuracy: 0.9393\n",
      "Epoch 701/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0993 - accuracy: 0.9611 - val_loss: 0.1653 - val_accuracy: 0.9393\n",
      "Epoch 702/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0992 - accuracy: 0.9609 - val_loss: 0.1640 - val_accuracy: 0.9398\n",
      "Epoch 703/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0991 - accuracy: 0.9611 - val_loss: 0.1646 - val_accuracy: 0.9392\n",
      "Epoch 704/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0991 - accuracy: 0.9612 - val_loss: 0.1645 - val_accuracy: 0.9397\n",
      "Epoch 705/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0988 - accuracy: 0.9612 - val_loss: 0.1651 - val_accuracy: 0.9397\n",
      "Epoch 706/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0989 - accuracy: 0.9611 - val_loss: 0.1646 - val_accuracy: 0.9401\n",
      "Epoch 707/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0988 - accuracy: 0.9615 - val_loss: 0.1642 - val_accuracy: 0.9384\n",
      "Epoch 708/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0987 - accuracy: 0.9614 - val_loss: 0.1642 - val_accuracy: 0.9394\n",
      "Epoch 709/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0988 - accuracy: 0.9611 - val_loss: 0.1650 - val_accuracy: 0.9398\n",
      "Epoch 710/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0985 - accuracy: 0.9614 - val_loss: 0.1637 - val_accuracy: 0.9396\n",
      "Epoch 711/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.0986 - accuracy: 0.9612 - val_loss: 0.1636 - val_accuracy: 0.9402\n",
      "Epoch 712/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0985 - accuracy: 0.9614 - val_loss: 0.1645 - val_accuracy: 0.9394\n",
      "Epoch 713/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0984 - accuracy: 0.9614 - val_loss: 0.1642 - val_accuracy: 0.9390\n",
      "Epoch 714/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1649 - val_accuracy: 0.9399\n",
      "Epoch 715/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0984 - accuracy: 0.9613 - val_loss: 0.1651 - val_accuracy: 0.9393\n",
      "Epoch 716/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.0982 - accuracy: 0.9616 - val_loss: 0.1639 - val_accuracy: 0.9393\n",
      "Epoch 717/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0982 - accuracy: 0.9614 - val_loss: 0.1653 - val_accuracy: 0.9383\n",
      "Epoch 718/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.0980 - accuracy: 0.9616 - val_loss: 0.1647 - val_accuracy: 0.9390\n",
      "Epoch 719/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0980 - accuracy: 0.9615 - val_loss: 0.1652 - val_accuracy: 0.9395\n",
      "Epoch 720/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0980 - accuracy: 0.9616 - val_loss: 0.1645 - val_accuracy: 0.9398\n",
      "Epoch 721/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.0978 - accuracy: 0.9614 - val_loss: 0.1650 - val_accuracy: 0.9398\n",
      "Epoch 722/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0977 - accuracy: 0.9616 - val_loss: 0.1665 - val_accuracy: 0.9385\n",
      "Epoch 723/800\n",
      "6000/6000 [==============================] - 0s 68us/step - loss: 0.0977 - accuracy: 0.9618 - val_loss: 0.1663 - val_accuracy: 0.9394\n",
      "Epoch 724/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0975 - accuracy: 0.9617 - val_loss: 0.1649 - val_accuracy: 0.9393\n",
      "Epoch 725/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0974 - accuracy: 0.9617 - val_loss: 0.1651 - val_accuracy: 0.9391\n",
      "Epoch 726/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.0974 - accuracy: 0.9618 - val_loss: 0.1667 - val_accuracy: 0.9405\n",
      "Epoch 727/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0975 - accuracy: 0.9615 - val_loss: 0.1657 - val_accuracy: 0.9390\n",
      "Epoch 728/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.0975 - accuracy: 0.9617 - val_loss: 0.1652 - val_accuracy: 0.9397\n",
      "Epoch 729/800\n",
      "6000/6000 [==============================] - 0s 69us/step - loss: 0.0974 - accuracy: 0.9618 - val_loss: 0.1668 - val_accuracy: 0.9393\n",
      "Epoch 730/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0974 - accuracy: 0.9617 - val_loss: 0.1657 - val_accuracy: 0.9395\n",
      "Epoch 731/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.0972 - accuracy: 0.9619 - val_loss: 0.1676 - val_accuracy: 0.9384\n",
      "Epoch 732/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0971 - accuracy: 0.9620 - val_loss: 0.1660 - val_accuracy: 0.9398\n",
      "Epoch 733/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0970 - accuracy: 0.9622 - val_loss: 0.1663 - val_accuracy: 0.9390\n",
      "Epoch 734/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0970 - accuracy: 0.9619 - val_loss: 0.1678 - val_accuracy: 0.9395\n",
      "Epoch 735/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.0969 - accuracy: 0.9620 - val_loss: 0.1656 - val_accuracy: 0.9392\n",
      "Epoch 736/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0970 - accuracy: 0.9619 - val_loss: 0.1659 - val_accuracy: 0.9391\n",
      "Epoch 737/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.0969 - accuracy: 0.9618 - val_loss: 0.1678 - val_accuracy: 0.9395\n",
      "Epoch 738/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.0967 - accuracy: 0.9621 - val_loss: 0.1678 - val_accuracy: 0.9376\n",
      "Epoch 739/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0966 - accuracy: 0.9620 - val_loss: 0.1664 - val_accuracy: 0.9392\n",
      "Epoch 740/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.0966 - accuracy: 0.9621 - val_loss: 0.1670 - val_accuracy: 0.9388\n",
      "Epoch 741/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0966 - accuracy: 0.9618 - val_loss: 0.1690 - val_accuracy: 0.9377\n",
      "Epoch 742/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0964 - accuracy: 0.9624 - val_loss: 0.1664 - val_accuracy: 0.9398\n",
      "Epoch 743/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.0964 - accuracy: 0.9623 - val_loss: 0.1688 - val_accuracy: 0.9376\n",
      "Epoch 744/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0963 - accuracy: 0.9621 - val_loss: 0.1665 - val_accuracy: 0.9392\n",
      "Epoch 745/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0961 - accuracy: 0.9623 - val_loss: 0.1670 - val_accuracy: 0.9390\n",
      "Epoch 746/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0961 - accuracy: 0.9624 - val_loss: 0.1677 - val_accuracy: 0.9387\n",
      "Epoch 747/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0960 - accuracy: 0.9623 - val_loss: 0.1669 - val_accuracy: 0.9391\n",
      "Epoch 748/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0959 - accuracy: 0.9623 - val_loss: 0.1671 - val_accuracy: 0.9389\n",
      "Epoch 749/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.0959 - accuracy: 0.9627 - val_loss: 0.1677 - val_accuracy: 0.9395\n",
      "Epoch 750/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0959 - accuracy: 0.9626 - val_loss: 0.1693 - val_accuracy: 0.9386\n",
      "Epoch 751/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0960 - accuracy: 0.9623 - val_loss: 0.1685 - val_accuracy: 0.9392\n",
      "Epoch 752/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.0957 - accuracy: 0.9625 - val_loss: 0.1673 - val_accuracy: 0.9391\n",
      "Epoch 753/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0955 - accuracy: 0.9625 - val_loss: 0.1672 - val_accuracy: 0.9397\n",
      "Epoch 754/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0957 - accuracy: 0.9624 - val_loss: 0.1680 - val_accuracy: 0.9391\n",
      "Epoch 755/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0955 - accuracy: 0.9626 - val_loss: 0.1723 - val_accuracy: 0.9380\n",
      "Epoch 756/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0954 - accuracy: 0.9626 - val_loss: 0.1680 - val_accuracy: 0.9386\n",
      "Epoch 757/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0952 - accuracy: 0.9627 - val_loss: 0.1694 - val_accuracy: 0.9384\n",
      "Epoch 758/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0953 - accuracy: 0.9626 - val_loss: 0.1688 - val_accuracy: 0.9390\n",
      "Epoch 759/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.0954 - accuracy: 0.9625 - val_loss: 0.1679 - val_accuracy: 0.9388\n",
      "Epoch 760/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.0951 - accuracy: 0.9624 - val_loss: 0.1693 - val_accuracy: 0.9391\n",
      "Epoch 761/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.0952 - accuracy: 0.9626 - val_loss: 0.1683 - val_accuracy: 0.9389\n",
      "Epoch 762/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.0950 - accuracy: 0.9630 - val_loss: 0.1702 - val_accuracy: 0.9387\n",
      "Epoch 763/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0951 - accuracy: 0.9627 - val_loss: 0.1688 - val_accuracy: 0.9383\n",
      "Epoch 764/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0949 - accuracy: 0.9627 - val_loss: 0.1684 - val_accuracy: 0.9381\n",
      "Epoch 765/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0947 - accuracy: 0.9625 - val_loss: 0.1677 - val_accuracy: 0.9395\n",
      "Epoch 766/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0949 - accuracy: 0.9628 - val_loss: 0.1685 - val_accuracy: 0.9399\n",
      "Epoch 767/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.0947 - accuracy: 0.9628 - val_loss: 0.1699 - val_accuracy: 0.9393\n",
      "Epoch 768/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.0946 - accuracy: 0.9631 - val_loss: 0.1698 - val_accuracy: 0.9371\n",
      "Epoch 769/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.0944 - accuracy: 0.9630 - val_loss: 0.1678 - val_accuracy: 0.9385\n",
      "Epoch 770/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.0946 - accuracy: 0.9630 - val_loss: 0.1700 - val_accuracy: 0.9382\n",
      "Epoch 771/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.0945 - accuracy: 0.9628 - val_loss: 0.1708 - val_accuracy: 0.9391\n",
      "Epoch 772/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.0944 - accuracy: 0.9632 - val_loss: 0.1683 - val_accuracy: 0.9385\n",
      "Epoch 773/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0944 - accuracy: 0.9630 - val_loss: 0.1687 - val_accuracy: 0.9393\n",
      "Epoch 774/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.0941 - accuracy: 0.9630 - val_loss: 0.1694 - val_accuracy: 0.9374\n",
      "Epoch 775/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.0943 - accuracy: 0.9630 - val_loss: 0.1688 - val_accuracy: 0.9399\n",
      "Epoch 776/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0941 - accuracy: 0.9629 - val_loss: 0.1709 - val_accuracy: 0.9377\n",
      "Epoch 777/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0940 - accuracy: 0.9633 - val_loss: 0.1706 - val_accuracy: 0.9378\n",
      "Epoch 778/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0939 - accuracy: 0.9633 - val_loss: 0.1703 - val_accuracy: 0.9386\n",
      "Epoch 779/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0939 - accuracy: 0.9628 - val_loss: 0.1685 - val_accuracy: 0.9387\n",
      "Epoch 780/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0937 - accuracy: 0.9630 - val_loss: 0.1711 - val_accuracy: 0.9383\n",
      "Epoch 781/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0937 - accuracy: 0.9632 - val_loss: 0.1699 - val_accuracy: 0.9387\n",
      "Epoch 782/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0937 - accuracy: 0.9632 - val_loss: 0.1700 - val_accuracy: 0.9389\n",
      "Epoch 783/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0934 - accuracy: 0.9634 - val_loss: 0.1707 - val_accuracy: 0.9379\n",
      "Epoch 784/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0934 - accuracy: 0.9634 - val_loss: 0.1698 - val_accuracy: 0.9388\n",
      "Epoch 785/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0934 - accuracy: 0.9633 - val_loss: 0.1711 - val_accuracy: 0.9388\n",
      "Epoch 786/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0933 - accuracy: 0.9633 - val_loss: 0.1696 - val_accuracy: 0.9388\n",
      "Epoch 787/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0933 - accuracy: 0.9636 - val_loss: 0.1719 - val_accuracy: 0.9386\n",
      "Epoch 788/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0933 - accuracy: 0.9634 - val_loss: 0.1724 - val_accuracy: 0.9381\n",
      "Epoch 789/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0932 - accuracy: 0.9635 - val_loss: 0.1715 - val_accuracy: 0.9374\n",
      "Epoch 790/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0931 - accuracy: 0.9635 - val_loss: 0.1716 - val_accuracy: 0.9391\n",
      "Epoch 791/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0931 - accuracy: 0.9633 - val_loss: 0.1717 - val_accuracy: 0.9382\n",
      "Epoch 792/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0930 - accuracy: 0.9633 - val_loss: 0.1716 - val_accuracy: 0.9379\n",
      "Epoch 793/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0928 - accuracy: 0.9636 - val_loss: 0.1739 - val_accuracy: 0.9371\n",
      "Epoch 794/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0928 - accuracy: 0.9634 - val_loss: 0.1711 - val_accuracy: 0.9390\n",
      "Epoch 795/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0927 - accuracy: 0.9635 - val_loss: 0.1710 - val_accuracy: 0.9388\n",
      "Epoch 796/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0926 - accuracy: 0.9637 - val_loss: 0.1717 - val_accuracy: 0.9377\n",
      "Epoch 797/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0925 - accuracy: 0.9638 - val_loss: 0.1716 - val_accuracy: 0.9385\n",
      "Epoch 798/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0926 - accuracy: 0.9638 - val_loss: 0.1715 - val_accuracy: 0.9387\n",
      "Epoch 799/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0925 - accuracy: 0.9636 - val_loss: 0.1720 - val_accuracy: 0.9383\n",
      "Epoch 800/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0924 - accuracy: 0.9638 - val_loss: 0.1714 - val_accuracy: 0.9385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:122: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 6000 samples, validate on 2001 samples\n",
      "Epoch 1/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.4401 - accuracy: 0.8509 - val_loss: 0.2687 - val_accuracy: 0.9250\n",
      "Epoch 2/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2520 - accuracy: 0.9304 - val_loss: 0.2667 - val_accuracy: 0.9250\n",
      "Epoch 3/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2510 - accuracy: 0.9304 - val_loss: 0.2660 - val_accuracy: 0.9250\n",
      "Epoch 4/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2504 - accuracy: 0.9304 - val_loss: 0.2653 - val_accuracy: 0.9250\n",
      "Epoch 5/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2499 - accuracy: 0.9304 - val_loss: 0.2648 - val_accuracy: 0.9250\n",
      "Epoch 6/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2494 - accuracy: 0.9304 - val_loss: 0.2644 - val_accuracy: 0.9250\n",
      "Epoch 7/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2490 - accuracy: 0.9304 - val_loss: 0.2637 - val_accuracy: 0.9250\n",
      "Epoch 8/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2486 - accuracy: 0.9304 - val_loss: 0.2635 - val_accuracy: 0.9250\n",
      "Epoch 9/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2482 - accuracy: 0.9304 - val_loss: 0.2631 - val_accuracy: 0.9250\n",
      "Epoch 10/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2478 - accuracy: 0.9304 - val_loss: 0.2628 - val_accuracy: 0.9250\n",
      "Epoch 11/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2474 - accuracy: 0.9304 - val_loss: 0.2623 - val_accuracy: 0.9250\n",
      "Epoch 12/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2470 - accuracy: 0.9304 - val_loss: 0.2616 - val_accuracy: 0.9250\n",
      "Epoch 13/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2465 - accuracy: 0.9304 - val_loss: 0.2611 - val_accuracy: 0.9250\n",
      "Epoch 14/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2461 - accuracy: 0.9304 - val_loss: 0.2606 - val_accuracy: 0.9250\n",
      "Epoch 15/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2455 - accuracy: 0.9304 - val_loss: 0.2600 - val_accuracy: 0.9250\n",
      "Epoch 16/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2449 - accuracy: 0.9304 - val_loss: 0.2596 - val_accuracy: 0.9250\n",
      "Epoch 17/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2444 - accuracy: 0.9304 - val_loss: 0.2590 - val_accuracy: 0.9250\n",
      "Epoch 18/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2437 - accuracy: 0.9304 - val_loss: 0.2581 - val_accuracy: 0.9250\n",
      "Epoch 19/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2429 - accuracy: 0.9304 - val_loss: 0.2572 - val_accuracy: 0.9250\n",
      "Epoch 20/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2420 - accuracy: 0.9304 - val_loss: 0.2563 - val_accuracy: 0.9250\n",
      "Epoch 21/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.2411 - accuracy: 0.9304 - val_loss: 0.2553 - val_accuracy: 0.9250\n",
      "Epoch 22/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.2400 - accuracy: 0.9304 - val_loss: 0.2541 - val_accuracy: 0.9250\n",
      "Epoch 23/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2389 - accuracy: 0.9304 - val_loss: 0.2530 - val_accuracy: 0.9250\n",
      "Epoch 24/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2376 - accuracy: 0.9304 - val_loss: 0.2515 - val_accuracy: 0.9250\n",
      "Epoch 25/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.2362 - accuracy: 0.9304 - val_loss: 0.2500 - val_accuracy: 0.9250\n",
      "Epoch 26/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2347 - accuracy: 0.9304 - val_loss: 0.2484 - val_accuracy: 0.9250\n",
      "Epoch 27/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2331 - accuracy: 0.9304 - val_loss: 0.2467 - val_accuracy: 0.9250\n",
      "Epoch 28/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2313 - accuracy: 0.9304 - val_loss: 0.2449 - val_accuracy: 0.9250\n",
      "Epoch 29/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2294 - accuracy: 0.9304 - val_loss: 0.2430 - val_accuracy: 0.9250\n",
      "Epoch 30/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2275 - accuracy: 0.9304 - val_loss: 0.2412 - val_accuracy: 0.9250\n",
      "Epoch 31/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2256 - accuracy: 0.9304 - val_loss: 0.2392 - val_accuracy: 0.9250\n",
      "Epoch 32/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2236 - accuracy: 0.9304 - val_loss: 0.2370 - val_accuracy: 0.9250\n",
      "Epoch 33/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2217 - accuracy: 0.9305 - val_loss: 0.2352 - val_accuracy: 0.9250\n",
      "Epoch 34/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2198 - accuracy: 0.9306 - val_loss: 0.2333 - val_accuracy: 0.9252\n",
      "Epoch 35/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2180 - accuracy: 0.9307 - val_loss: 0.2316 - val_accuracy: 0.9252\n",
      "Epoch 36/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.2162 - accuracy: 0.9307 - val_loss: 0.2298 - val_accuracy: 0.9252\n",
      "Epoch 37/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.2145 - accuracy: 0.9308 - val_loss: 0.2284 - val_accuracy: 0.9254\n",
      "Epoch 38/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2128 - accuracy: 0.9309 - val_loss: 0.2265 - val_accuracy: 0.9254\n",
      "Epoch 39/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2111 - accuracy: 0.9309 - val_loss: 0.2251 - val_accuracy: 0.9257\n",
      "Epoch 40/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.2096 - accuracy: 0.9311 - val_loss: 0.2236 - val_accuracy: 0.9258\n",
      "Epoch 41/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2081 - accuracy: 0.9312 - val_loss: 0.2223 - val_accuracy: 0.9258\n",
      "Epoch 42/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.2066 - accuracy: 0.9313 - val_loss: 0.2207 - val_accuracy: 0.9261\n",
      "Epoch 43/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.2051 - accuracy: 0.9314 - val_loss: 0.2193 - val_accuracy: 0.9261\n",
      "Epoch 44/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2037 - accuracy: 0.9314 - val_loss: 0.2184 - val_accuracy: 0.9260\n",
      "Epoch 45/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.2023 - accuracy: 0.9316 - val_loss: 0.2170 - val_accuracy: 0.9266\n",
      "Epoch 46/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2010 - accuracy: 0.9318 - val_loss: 0.2159 - val_accuracy: 0.9268\n",
      "Epoch 47/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1997 - accuracy: 0.9318 - val_loss: 0.2145 - val_accuracy: 0.9267\n",
      "Epoch 48/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1985 - accuracy: 0.9319 - val_loss: 0.2132 - val_accuracy: 0.9270\n",
      "Epoch 49/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1973 - accuracy: 0.9321 - val_loss: 0.2124 - val_accuracy: 0.9269\n",
      "Epoch 50/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1961 - accuracy: 0.9324 - val_loss: 0.2113 - val_accuracy: 0.9273\n",
      "Epoch 51/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1950 - accuracy: 0.9325 - val_loss: 0.2104 - val_accuracy: 0.9274\n",
      "Epoch 52/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1940 - accuracy: 0.9327 - val_loss: 0.2097 - val_accuracy: 0.9276\n",
      "Epoch 53/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1930 - accuracy: 0.9329 - val_loss: 0.2084 - val_accuracy: 0.9272\n",
      "Epoch 54/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1920 - accuracy: 0.9329 - val_loss: 0.2076 - val_accuracy: 0.9277\n",
      "Epoch 55/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1910 - accuracy: 0.9333 - val_loss: 0.2073 - val_accuracy: 0.9283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1901 - accuracy: 0.9335 - val_loss: 0.2062 - val_accuracy: 0.9279\n",
      "Epoch 57/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1892 - accuracy: 0.9336 - val_loss: 0.2051 - val_accuracy: 0.9283\n",
      "Epoch 58/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1883 - accuracy: 0.9338 - val_loss: 0.2047 - val_accuracy: 0.9286\n",
      "Epoch 59/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1874 - accuracy: 0.9340 - val_loss: 0.2039 - val_accuracy: 0.9289\n",
      "Epoch 60/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1865 - accuracy: 0.9343 - val_loss: 0.2032 - val_accuracy: 0.9287\n",
      "Epoch 61/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1857 - accuracy: 0.9345 - val_loss: 0.2023 - val_accuracy: 0.9286\n",
      "Epoch 62/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1849 - accuracy: 0.9345 - val_loss: 0.2013 - val_accuracy: 0.9291\n",
      "Epoch 63/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1840 - accuracy: 0.9348 - val_loss: 0.2010 - val_accuracy: 0.9291\n",
      "Epoch 64/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1833 - accuracy: 0.9350 - val_loss: 0.2000 - val_accuracy: 0.9290\n",
      "Epoch 65/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1824 - accuracy: 0.9352 - val_loss: 0.1996 - val_accuracy: 0.9293\n",
      "Epoch 66/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1817 - accuracy: 0.9354 - val_loss: 0.1984 - val_accuracy: 0.9295\n",
      "Epoch 67/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1809 - accuracy: 0.9356 - val_loss: 0.1977 - val_accuracy: 0.9293\n",
      "Epoch 68/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1802 - accuracy: 0.9358 - val_loss: 0.1972 - val_accuracy: 0.9294\n",
      "Epoch 69/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1794 - accuracy: 0.9358 - val_loss: 0.1963 - val_accuracy: 0.9299\n",
      "Epoch 70/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1787 - accuracy: 0.9361 - val_loss: 0.1959 - val_accuracy: 0.9300\n",
      "Epoch 71/800\n",
      "6000/6000 [==============================] - 1s 83us/step - loss: 0.1780 - accuracy: 0.9364 - val_loss: 0.1954 - val_accuracy: 0.9300\n",
      "Epoch 72/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1773 - accuracy: 0.9366 - val_loss: 0.1948 - val_accuracy: 0.9303\n",
      "Epoch 73/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1766 - accuracy: 0.9368 - val_loss: 0.1941 - val_accuracy: 0.9304\n",
      "Epoch 74/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1760 - accuracy: 0.9369 - val_loss: 0.1935 - val_accuracy: 0.9305\n",
      "Epoch 75/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1753 - accuracy: 0.9369 - val_loss: 0.1929 - val_accuracy: 0.9313\n",
      "Epoch 76/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1746 - accuracy: 0.9372 - val_loss: 0.1922 - val_accuracy: 0.9309\n",
      "Epoch 77/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1740 - accuracy: 0.9376 - val_loss: 0.1918 - val_accuracy: 0.9312\n",
      "Epoch 78/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1734 - accuracy: 0.9375 - val_loss: 0.1913 - val_accuracy: 0.9313\n",
      "Epoch 79/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1727 - accuracy: 0.9380 - val_loss: 0.1905 - val_accuracy: 0.9316\n",
      "Epoch 80/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1722 - accuracy: 0.9382 - val_loss: 0.1901 - val_accuracy: 0.9318\n",
      "Epoch 81/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1716 - accuracy: 0.9382 - val_loss: 0.1897 - val_accuracy: 0.9323\n",
      "Epoch 82/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1710 - accuracy: 0.9385 - val_loss: 0.1888 - val_accuracy: 0.9320\n",
      "Epoch 83/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1705 - accuracy: 0.9388 - val_loss: 0.1884 - val_accuracy: 0.9330\n",
      "Epoch 84/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1699 - accuracy: 0.9390 - val_loss: 0.1880 - val_accuracy: 0.9321\n",
      "Epoch 85/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1694 - accuracy: 0.9390 - val_loss: 0.1873 - val_accuracy: 0.9325\n",
      "Epoch 86/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1688 - accuracy: 0.9392 - val_loss: 0.1875 - val_accuracy: 0.9330\n",
      "Epoch 87/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1683 - accuracy: 0.9392 - val_loss: 0.1861 - val_accuracy: 0.9330\n",
      "Epoch 88/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1679 - accuracy: 0.9394 - val_loss: 0.1861 - val_accuracy: 0.9329\n",
      "Epoch 89/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1673 - accuracy: 0.9396 - val_loss: 0.1853 - val_accuracy: 0.9332\n",
      "Epoch 90/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1669 - accuracy: 0.9398 - val_loss: 0.1857 - val_accuracy: 0.9334\n",
      "Epoch 91/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1664 - accuracy: 0.9400 - val_loss: 0.1851 - val_accuracy: 0.9336\n",
      "Epoch 92/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1660 - accuracy: 0.9400 - val_loss: 0.1843 - val_accuracy: 0.9339\n",
      "Epoch 93/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1655 - accuracy: 0.9403 - val_loss: 0.1835 - val_accuracy: 0.9339\n",
      "Epoch 94/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1651 - accuracy: 0.9402 - val_loss: 0.1833 - val_accuracy: 0.9341\n",
      "Epoch 95/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1646 - accuracy: 0.9404 - val_loss: 0.1830 - val_accuracy: 0.9341\n",
      "Epoch 96/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1642 - accuracy: 0.9406 - val_loss: 0.1827 - val_accuracy: 0.9344\n",
      "Epoch 97/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1639 - accuracy: 0.9406 - val_loss: 0.1825 - val_accuracy: 0.9340\n",
      "Epoch 98/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1635 - accuracy: 0.9408 - val_loss: 0.1818 - val_accuracy: 0.9344\n",
      "Epoch 99/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1631 - accuracy: 0.9410 - val_loss: 0.1814 - val_accuracy: 0.9343\n",
      "Epoch 100/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1627 - accuracy: 0.9410 - val_loss: 0.1813 - val_accuracy: 0.9345\n",
      "Epoch 101/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1623 - accuracy: 0.9410 - val_loss: 0.1807 - val_accuracy: 0.9351\n",
      "Epoch 102/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1620 - accuracy: 0.9412 - val_loss: 0.1804 - val_accuracy: 0.9347\n",
      "Epoch 103/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1616 - accuracy: 0.9414 - val_loss: 0.1802 - val_accuracy: 0.9349\n",
      "Epoch 104/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1612 - accuracy: 0.9416 - val_loss: 0.1796 - val_accuracy: 0.9351\n",
      "Epoch 105/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1609 - accuracy: 0.9415 - val_loss: 0.1794 - val_accuracy: 0.9348\n",
      "Epoch 106/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1606 - accuracy: 0.9417 - val_loss: 0.1789 - val_accuracy: 0.9354\n",
      "Epoch 107/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1603 - accuracy: 0.9418 - val_loss: 0.1790 - val_accuracy: 0.9348\n",
      "Epoch 108/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1599 - accuracy: 0.9418 - val_loss: 0.1788 - val_accuracy: 0.9350\n",
      "Epoch 109/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1595 - accuracy: 0.9422 - val_loss: 0.1782 - val_accuracy: 0.9354\n",
      "Epoch 110/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1592 - accuracy: 0.9422 - val_loss: 0.1783 - val_accuracy: 0.9353\n",
      "Epoch 111/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1590 - accuracy: 0.9422 - val_loss: 0.1777 - val_accuracy: 0.9357\n",
      "Epoch 112/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1586 - accuracy: 0.9424 - val_loss: 0.1775 - val_accuracy: 0.9353\n",
      "Epoch 113/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1584 - accuracy: 0.9424 - val_loss: 0.1772 - val_accuracy: 0.9361\n",
      "Epoch 114/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1581 - accuracy: 0.9425 - val_loss: 0.1775 - val_accuracy: 0.9355\n",
      "Epoch 115/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1578 - accuracy: 0.9426 - val_loss: 0.1766 - val_accuracy: 0.9362\n",
      "Epoch 116/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1575 - accuracy: 0.9428 - val_loss: 0.1762 - val_accuracy: 0.9362\n",
      "Epoch 117/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1573 - accuracy: 0.9426 - val_loss: 0.1768 - val_accuracy: 0.9356\n",
      "Epoch 118/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1571 - accuracy: 0.9429 - val_loss: 0.1762 - val_accuracy: 0.9360\n",
      "Epoch 119/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1567 - accuracy: 0.9431 - val_loss: 0.1771 - val_accuracy: 0.9360\n",
      "Epoch 120/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1565 - accuracy: 0.9430 - val_loss: 0.1756 - val_accuracy: 0.9361\n",
      "Epoch 121/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1562 - accuracy: 0.9431 - val_loss: 0.1755 - val_accuracy: 0.9359\n",
      "Epoch 122/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1560 - accuracy: 0.9431 - val_loss: 0.1753 - val_accuracy: 0.9363\n",
      "Epoch 123/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1558 - accuracy: 0.9434 - val_loss: 0.1748 - val_accuracy: 0.9362\n",
      "Epoch 124/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1556 - accuracy: 0.9434 - val_loss: 0.1747 - val_accuracy: 0.9366\n",
      "Epoch 125/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1553 - accuracy: 0.9434 - val_loss: 0.1748 - val_accuracy: 0.9366\n",
      "Epoch 126/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1551 - accuracy: 0.9436 - val_loss: 0.1750 - val_accuracy: 0.9362\n",
      "Epoch 127/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1549 - accuracy: 0.9437 - val_loss: 0.1748 - val_accuracy: 0.9370\n",
      "Epoch 128/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1546 - accuracy: 0.9436 - val_loss: 0.1740 - val_accuracy: 0.9366\n",
      "Epoch 129/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1545 - accuracy: 0.9438 - val_loss: 0.1742 - val_accuracy: 0.9367\n",
      "Epoch 130/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1542 - accuracy: 0.9439 - val_loss: 0.1735 - val_accuracy: 0.9365\n",
      "Epoch 131/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1540 - accuracy: 0.9439 - val_loss: 0.1733 - val_accuracy: 0.9368\n",
      "Epoch 132/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1538 - accuracy: 0.9441 - val_loss: 0.1733 - val_accuracy: 0.9371\n",
      "Epoch 133/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1536 - accuracy: 0.9442 - val_loss: 0.1733 - val_accuracy: 0.9368\n",
      "Epoch 134/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1534 - accuracy: 0.9440 - val_loss: 0.1732 - val_accuracy: 0.9370\n",
      "Epoch 135/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1533 - accuracy: 0.9441 - val_loss: 0.1729 - val_accuracy: 0.9371\n",
      "Epoch 136/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1530 - accuracy: 0.9443 - val_loss: 0.1725 - val_accuracy: 0.9370\n",
      "Epoch 137/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1529 - accuracy: 0.9443 - val_loss: 0.1724 - val_accuracy: 0.9370\n",
      "Epoch 138/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1526 - accuracy: 0.9445 - val_loss: 0.1724 - val_accuracy: 0.9372\n",
      "Epoch 139/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1525 - accuracy: 0.9445 - val_loss: 0.1723 - val_accuracy: 0.9374\n",
      "Epoch 140/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1523 - accuracy: 0.9446 - val_loss: 0.1719 - val_accuracy: 0.9374\n",
      "Epoch 141/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1521 - accuracy: 0.9446 - val_loss: 0.1726 - val_accuracy: 0.9372\n",
      "Epoch 142/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1519 - accuracy: 0.9445 - val_loss: 0.1728 - val_accuracy: 0.9372\n",
      "Epoch 143/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1517 - accuracy: 0.9446 - val_loss: 0.1722 - val_accuracy: 0.9374\n",
      "Epoch 144/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1516 - accuracy: 0.9447 - val_loss: 0.1723 - val_accuracy: 0.9373\n",
      "Epoch 145/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1513 - accuracy: 0.9448 - val_loss: 0.1716 - val_accuracy: 0.9375\n",
      "Epoch 146/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1512 - accuracy: 0.9450 - val_loss: 0.1714 - val_accuracy: 0.9378\n",
      "Epoch 147/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1510 - accuracy: 0.9449 - val_loss: 0.1709 - val_accuracy: 0.9371\n",
      "Epoch 148/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1509 - accuracy: 0.9449 - val_loss: 0.1713 - val_accuracy: 0.9374\n",
      "Epoch 149/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1507 - accuracy: 0.9451 - val_loss: 0.1710 - val_accuracy: 0.9379\n",
      "Epoch 150/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1506 - accuracy: 0.9451 - val_loss: 0.1707 - val_accuracy: 0.9378\n",
      "Epoch 151/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1503 - accuracy: 0.9451 - val_loss: 0.1707 - val_accuracy: 0.9376\n",
      "Epoch 152/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1501 - accuracy: 0.9452 - val_loss: 0.1715 - val_accuracy: 0.9379\n",
      "Epoch 153/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1501 - accuracy: 0.9453 - val_loss: 0.1707 - val_accuracy: 0.9375\n",
      "Epoch 154/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1499 - accuracy: 0.9452 - val_loss: 0.1700 - val_accuracy: 0.9378\n",
      "Epoch 155/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1498 - accuracy: 0.9453 - val_loss: 0.1701 - val_accuracy: 0.9379\n",
      "Epoch 156/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1495 - accuracy: 0.9453 - val_loss: 0.1701 - val_accuracy: 0.9381\n",
      "Epoch 157/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1494 - accuracy: 0.9455 - val_loss: 0.1703 - val_accuracy: 0.9382\n",
      "Epoch 158/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1491 - accuracy: 0.9455 - val_loss: 0.1703 - val_accuracy: 0.9381\n",
      "Epoch 159/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1490 - accuracy: 0.9456 - val_loss: 0.1696 - val_accuracy: 0.9382\n",
      "Epoch 160/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1488 - accuracy: 0.9458 - val_loss: 0.1697 - val_accuracy: 0.9380\n",
      "Epoch 161/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1487 - accuracy: 0.9457 - val_loss: 0.1690 - val_accuracy: 0.9385\n",
      "Epoch 162/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1486 - accuracy: 0.9458 - val_loss: 0.1693 - val_accuracy: 0.9383\n",
      "Epoch 163/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1483 - accuracy: 0.9458 - val_loss: 0.1694 - val_accuracy: 0.9380\n",
      "Epoch 164/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1482 - accuracy: 0.9458 - val_loss: 0.1690 - val_accuracy: 0.9382\n",
      "Epoch 165/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1481 - accuracy: 0.9457 - val_loss: 0.1687 - val_accuracy: 0.9383\n",
      "Epoch 166/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1479 - accuracy: 0.9462 - val_loss: 0.1682 - val_accuracy: 0.9384\n",
      "Epoch 167/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1478 - accuracy: 0.9459 - val_loss: 0.1689 - val_accuracy: 0.9383\n",
      "Epoch 168/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1476 - accuracy: 0.9460 - val_loss: 0.1681 - val_accuracy: 0.9386\n",
      "Epoch 169/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1474 - accuracy: 0.9461 - val_loss: 0.1679 - val_accuracy: 0.9386\n",
      "Epoch 170/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1473 - accuracy: 0.9462 - val_loss: 0.1681 - val_accuracy: 0.9382\n",
      "Epoch 171/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1471 - accuracy: 0.9461 - val_loss: 0.1680 - val_accuracy: 0.9382\n",
      "Epoch 172/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1469 - accuracy: 0.9462 - val_loss: 0.1678 - val_accuracy: 0.9385\n",
      "Epoch 173/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1467 - accuracy: 0.9463 - val_loss: 0.1682 - val_accuracy: 0.9386\n",
      "Epoch 174/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1466 - accuracy: 0.9464 - val_loss: 0.1683 - val_accuracy: 0.9385\n",
      "Epoch 175/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1464 - accuracy: 0.9463 - val_loss: 0.1678 - val_accuracy: 0.9384\n",
      "Epoch 176/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1463 - accuracy: 0.9466 - val_loss: 0.1675 - val_accuracy: 0.9385\n",
      "Epoch 177/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1462 - accuracy: 0.9464 - val_loss: 0.1671 - val_accuracy: 0.9386\n",
      "Epoch 178/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1460 - accuracy: 0.9465 - val_loss: 0.1675 - val_accuracy: 0.9388\n",
      "Epoch 179/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1459 - accuracy: 0.9464 - val_loss: 0.1669 - val_accuracy: 0.9391\n",
      "Epoch 180/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1456 - accuracy: 0.9468 - val_loss: 0.1671 - val_accuracy: 0.9384\n",
      "Epoch 181/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1455 - accuracy: 0.9467 - val_loss: 0.1666 - val_accuracy: 0.9389\n",
      "Epoch 182/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1453 - accuracy: 0.9465 - val_loss: 0.1666 - val_accuracy: 0.9391\n",
      "Epoch 183/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1452 - accuracy: 0.9467 - val_loss: 0.1666 - val_accuracy: 0.9389\n",
      "Epoch 184/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1450 - accuracy: 0.9469 - val_loss: 0.1668 - val_accuracy: 0.9387\n",
      "Epoch 185/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1449 - accuracy: 0.9468 - val_loss: 0.1660 - val_accuracy: 0.9392\n",
      "Epoch 186/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1447 - accuracy: 0.9469 - val_loss: 0.1655 - val_accuracy: 0.9392\n",
      "Epoch 187/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1445 - accuracy: 0.9471 - val_loss: 0.1661 - val_accuracy: 0.9393\n",
      "Epoch 188/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1444 - accuracy: 0.9470 - val_loss: 0.1657 - val_accuracy: 0.9395\n",
      "Epoch 189/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1443 - accuracy: 0.9471 - val_loss: 0.1653 - val_accuracy: 0.9393\n",
      "Epoch 190/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1440 - accuracy: 0.9473 - val_loss: 0.1663 - val_accuracy: 0.9394\n",
      "Epoch 191/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1439 - accuracy: 0.9471 - val_loss: 0.1657 - val_accuracy: 0.9392\n",
      "Epoch 192/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1437 - accuracy: 0.9472 - val_loss: 0.1653 - val_accuracy: 0.9393\n",
      "Epoch 193/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1436 - accuracy: 0.9471 - val_loss: 0.1651 - val_accuracy: 0.9393\n",
      "Epoch 194/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1434 - accuracy: 0.9474 - val_loss: 0.1652 - val_accuracy: 0.9391\n",
      "Epoch 195/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1432 - accuracy: 0.9474 - val_loss: 0.1655 - val_accuracy: 0.9395\n",
      "Epoch 196/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1431 - accuracy: 0.9475 - val_loss: 0.1645 - val_accuracy: 0.9394\n",
      "Epoch 197/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1429 - accuracy: 0.9475 - val_loss: 0.1646 - val_accuracy: 0.9393\n",
      "Epoch 198/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1427 - accuracy: 0.9476 - val_loss: 0.1650 - val_accuracy: 0.9398\n",
      "Epoch 199/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1425 - accuracy: 0.9475 - val_loss: 0.1644 - val_accuracy: 0.9394\n",
      "Epoch 200/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1424 - accuracy: 0.9477 - val_loss: 0.1640 - val_accuracy: 0.9396\n",
      "Epoch 201/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1422 - accuracy: 0.9477 - val_loss: 0.1640 - val_accuracy: 0.9398\n",
      "Epoch 202/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1421 - accuracy: 0.9477 - val_loss: 0.1646 - val_accuracy: 0.9398\n",
      "Epoch 203/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1419 - accuracy: 0.9477 - val_loss: 0.1637 - val_accuracy: 0.9398\n",
      "Epoch 204/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1417 - accuracy: 0.9477 - val_loss: 0.1638 - val_accuracy: 0.9399\n",
      "Epoch 205/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1416 - accuracy: 0.9478 - val_loss: 0.1634 - val_accuracy: 0.9403\n",
      "Epoch 206/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1414 - accuracy: 0.9479 - val_loss: 0.1633 - val_accuracy: 0.9399\n",
      "Epoch 207/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1412 - accuracy: 0.9480 - val_loss: 0.1638 - val_accuracy: 0.9398\n",
      "Epoch 208/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1411 - accuracy: 0.9480 - val_loss: 0.1635 - val_accuracy: 0.9397\n",
      "Epoch 209/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1409 - accuracy: 0.9479 - val_loss: 0.1635 - val_accuracy: 0.9400\n",
      "Epoch 210/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1406 - accuracy: 0.9482 - val_loss: 0.1624 - val_accuracy: 0.9401\n",
      "Epoch 211/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1406 - accuracy: 0.9482 - val_loss: 0.1624 - val_accuracy: 0.9400\n",
      "Epoch 212/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1404 - accuracy: 0.9482 - val_loss: 0.1624 - val_accuracy: 0.9398\n",
      "Epoch 213/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1402 - accuracy: 0.9483 - val_loss: 0.1629 - val_accuracy: 0.9403\n",
      "Epoch 214/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1401 - accuracy: 0.9484 - val_loss: 0.1621 - val_accuracy: 0.9405\n",
      "Epoch 215/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1399 - accuracy: 0.9486 - val_loss: 0.1624 - val_accuracy: 0.9398\n",
      "Epoch 216/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1397 - accuracy: 0.9484 - val_loss: 0.1621 - val_accuracy: 0.9407\n",
      "Epoch 217/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1396 - accuracy: 0.9487 - val_loss: 0.1615 - val_accuracy: 0.9403\n",
      "Epoch 218/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1394 - accuracy: 0.9486 - val_loss: 0.1616 - val_accuracy: 0.9405\n",
      "Epoch 219/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1391 - accuracy: 0.9486 - val_loss: 0.1616 - val_accuracy: 0.9403\n",
      "Epoch 220/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1391 - accuracy: 0.9487 - val_loss: 0.1614 - val_accuracy: 0.9405\n",
      "Epoch 221/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1389 - accuracy: 0.9488 - val_loss: 0.1613 - val_accuracy: 0.9405\n",
      "Epoch 222/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1387 - accuracy: 0.9489 - val_loss: 0.1617 - val_accuracy: 0.9405\n",
      "Epoch 223/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1386 - accuracy: 0.9488 - val_loss: 0.1611 - val_accuracy: 0.9409\n",
      "Epoch 224/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1384 - accuracy: 0.9489 - val_loss: 0.1610 - val_accuracy: 0.9406\n",
      "Epoch 225/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1382 - accuracy: 0.9491 - val_loss: 0.1614 - val_accuracy: 0.9406\n",
      "Epoch 226/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1381 - accuracy: 0.9490 - val_loss: 0.1606 - val_accuracy: 0.9406\n",
      "Epoch 227/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1379 - accuracy: 0.9489 - val_loss: 0.1610 - val_accuracy: 0.9408\n",
      "Epoch 228/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1378 - accuracy: 0.9491 - val_loss: 0.1607 - val_accuracy: 0.9406\n",
      "Epoch 229/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1377 - accuracy: 0.9492 - val_loss: 0.1604 - val_accuracy: 0.9410\n",
      "Epoch 230/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1374 - accuracy: 0.9493 - val_loss: 0.1600 - val_accuracy: 0.9408\n",
      "Epoch 231/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1373 - accuracy: 0.9493 - val_loss: 0.1597 - val_accuracy: 0.9413\n",
      "Epoch 232/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1370 - accuracy: 0.9494 - val_loss: 0.1601 - val_accuracy: 0.9407\n",
      "Epoch 233/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1369 - accuracy: 0.9493 - val_loss: 0.1599 - val_accuracy: 0.9406\n",
      "Epoch 234/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1368 - accuracy: 0.9493 - val_loss: 0.1603 - val_accuracy: 0.9409\n",
      "Epoch 235/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1366 - accuracy: 0.9493 - val_loss: 0.1597 - val_accuracy: 0.9414\n",
      "Epoch 236/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1364 - accuracy: 0.9497 - val_loss: 0.1600 - val_accuracy: 0.9410\n",
      "Epoch 237/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1364 - accuracy: 0.9497 - val_loss: 0.1596 - val_accuracy: 0.9412\n",
      "Epoch 238/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1363 - accuracy: 0.9497 - val_loss: 0.1597 - val_accuracy: 0.9411\n",
      "Epoch 239/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1360 - accuracy: 0.9496 - val_loss: 0.1594 - val_accuracy: 0.9410\n",
      "Epoch 240/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1359 - accuracy: 0.9496 - val_loss: 0.1591 - val_accuracy: 0.9414\n",
      "Epoch 241/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1358 - accuracy: 0.9496 - val_loss: 0.1592 - val_accuracy: 0.9413\n",
      "Epoch 242/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1356 - accuracy: 0.9496 - val_loss: 0.1588 - val_accuracy: 0.9411\n",
      "Epoch 243/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1355 - accuracy: 0.9496 - val_loss: 0.1590 - val_accuracy: 0.9413\n",
      "Epoch 244/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1353 - accuracy: 0.9499 - val_loss: 0.1595 - val_accuracy: 0.9408\n",
      "Epoch 245/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1352 - accuracy: 0.9497 - val_loss: 0.1588 - val_accuracy: 0.9415\n",
      "Epoch 246/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1351 - accuracy: 0.9501 - val_loss: 0.1582 - val_accuracy: 0.9415\n",
      "Epoch 247/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1348 - accuracy: 0.9501 - val_loss: 0.1584 - val_accuracy: 0.9413\n",
      "Epoch 248/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1348 - accuracy: 0.9497 - val_loss: 0.1585 - val_accuracy: 0.9410\n",
      "Epoch 249/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1346 - accuracy: 0.9501 - val_loss: 0.1584 - val_accuracy: 0.9413\n",
      "Epoch 250/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1345 - accuracy: 0.9499 - val_loss: 0.1578 - val_accuracy: 0.9413\n",
      "Epoch 251/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1343 - accuracy: 0.9501 - val_loss: 0.1583 - val_accuracy: 0.9413\n",
      "Epoch 252/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1342 - accuracy: 0.9499 - val_loss: 0.1579 - val_accuracy: 0.9417\n",
      "Epoch 253/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1341 - accuracy: 0.9502 - val_loss: 0.1588 - val_accuracy: 0.9413\n",
      "Epoch 254/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1340 - accuracy: 0.9503 - val_loss: 0.1577 - val_accuracy: 0.9413\n",
      "Epoch 255/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1338 - accuracy: 0.9502 - val_loss: 0.1575 - val_accuracy: 0.9420\n",
      "Epoch 256/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1336 - accuracy: 0.9502 - val_loss: 0.1578 - val_accuracy: 0.9416\n",
      "Epoch 257/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1335 - accuracy: 0.9502 - val_loss: 0.1577 - val_accuracy: 0.9418\n",
      "Epoch 258/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1334 - accuracy: 0.9503 - val_loss: 0.1570 - val_accuracy: 0.9416\n",
      "Epoch 259/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1332 - accuracy: 0.9503 - val_loss: 0.1574 - val_accuracy: 0.9423\n",
      "Epoch 260/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1331 - accuracy: 0.9504 - val_loss: 0.1569 - val_accuracy: 0.9416\n",
      "Epoch 261/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1331 - accuracy: 0.9503 - val_loss: 0.1570 - val_accuracy: 0.9419\n",
      "Epoch 262/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1329 - accuracy: 0.9503 - val_loss: 0.1569 - val_accuracy: 0.9417\n",
      "Epoch 263/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1328 - accuracy: 0.9504 - val_loss: 0.1566 - val_accuracy: 0.9418\n",
      "Epoch 264/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1326 - accuracy: 0.9504 - val_loss: 0.1570 - val_accuracy: 0.9415\n",
      "Epoch 265/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1324 - accuracy: 0.9504 - val_loss: 0.1571 - val_accuracy: 0.9419\n",
      "Epoch 266/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1323 - accuracy: 0.9506 - val_loss: 0.1568 - val_accuracy: 0.9418\n",
      "Epoch 267/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1323 - accuracy: 0.9504 - val_loss: 0.1567 - val_accuracy: 0.9417\n",
      "Epoch 268/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1322 - accuracy: 0.9506 - val_loss: 0.1566 - val_accuracy: 0.9420\n",
      "Epoch 269/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1320 - accuracy: 0.9507 - val_loss: 0.1564 - val_accuracy: 0.9420\n",
      "Epoch 270/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1320 - accuracy: 0.9503 - val_loss: 0.1567 - val_accuracy: 0.9416\n",
      "Epoch 271/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1318 - accuracy: 0.9506 - val_loss: 0.1568 - val_accuracy: 0.9416\n",
      "Epoch 272/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1317 - accuracy: 0.9508 - val_loss: 0.1563 - val_accuracy: 0.9418\n",
      "Epoch 273/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1316 - accuracy: 0.9508 - val_loss: 0.1562 - val_accuracy: 0.9422\n",
      "Epoch 274/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1314 - accuracy: 0.9507 - val_loss: 0.1563 - val_accuracy: 0.9422\n",
      "Epoch 275/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1313 - accuracy: 0.9507 - val_loss: 0.1564 - val_accuracy: 0.9422\n",
      "Epoch 276/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1312 - accuracy: 0.9509 - val_loss: 0.1562 - val_accuracy: 0.9419\n",
      "Epoch 277/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1311 - accuracy: 0.9510 - val_loss: 0.1562 - val_accuracy: 0.9415\n",
      "Epoch 278/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1310 - accuracy: 0.9509 - val_loss: 0.1567 - val_accuracy: 0.9417\n",
      "Epoch 279/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1309 - accuracy: 0.9509 - val_loss: 0.1562 - val_accuracy: 0.9420\n",
      "Epoch 280/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1308 - accuracy: 0.9510 - val_loss: 0.1555 - val_accuracy: 0.9422\n",
      "Epoch 281/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.1561 - val_accuracy: 0.9418\n",
      "Epoch 282/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1305 - accuracy: 0.9511 - val_loss: 0.1552 - val_accuracy: 0.9422\n",
      "Epoch 283/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1304 - accuracy: 0.9509 - val_loss: 0.1554 - val_accuracy: 0.9424\n",
      "Epoch 284/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1304 - accuracy: 0.9511 - val_loss: 0.1557 - val_accuracy: 0.9422\n",
      "Epoch 285/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1302 - accuracy: 0.9510 - val_loss: 0.1560 - val_accuracy: 0.9423\n",
      "Epoch 286/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1301 - accuracy: 0.9511 - val_loss: 0.1556 - val_accuracy: 0.9421\n",
      "Epoch 287/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1299 - accuracy: 0.9511 - val_loss: 0.1561 - val_accuracy: 0.9422\n",
      "Epoch 288/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1298 - accuracy: 0.9512 - val_loss: 0.1555 - val_accuracy: 0.9423\n",
      "Epoch 289/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1298 - accuracy: 0.9511 - val_loss: 0.1554 - val_accuracy: 0.9420\n",
      "Epoch 290/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1297 - accuracy: 0.9513 - val_loss: 0.1552 - val_accuracy: 0.9424\n",
      "Epoch 291/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1296 - accuracy: 0.9514 - val_loss: 0.1553 - val_accuracy: 0.9423\n",
      "Epoch 292/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1294 - accuracy: 0.9514 - val_loss: 0.1551 - val_accuracy: 0.9422\n",
      "Epoch 293/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1294 - accuracy: 0.9513 - val_loss: 0.1555 - val_accuracy: 0.9421\n",
      "Epoch 294/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1293 - accuracy: 0.9513 - val_loss: 0.1556 - val_accuracy: 0.9421\n",
      "Epoch 295/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1292 - accuracy: 0.9514 - val_loss: 0.1552 - val_accuracy: 0.9422\n",
      "Epoch 296/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1290 - accuracy: 0.9514 - val_loss: 0.1546 - val_accuracy: 0.9423\n",
      "Epoch 297/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1290 - accuracy: 0.9515 - val_loss: 0.1553 - val_accuracy: 0.9421\n",
      "Epoch 298/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1288 - accuracy: 0.9516 - val_loss: 0.1548 - val_accuracy: 0.9422\n",
      "Epoch 299/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1287 - accuracy: 0.9514 - val_loss: 0.1547 - val_accuracy: 0.9426\n",
      "Epoch 300/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1287 - accuracy: 0.9515 - val_loss: 0.1552 - val_accuracy: 0.9428\n",
      "Epoch 301/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1284 - accuracy: 0.9517 - val_loss: 0.1545 - val_accuracy: 0.9420\n",
      "Epoch 302/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1283 - accuracy: 0.9517 - val_loss: 0.1552 - val_accuracy: 0.9425\n",
      "Epoch 303/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1283 - accuracy: 0.9518 - val_loss: 0.1550 - val_accuracy: 0.9427\n",
      "Epoch 304/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1282 - accuracy: 0.9517 - val_loss: 0.1548 - val_accuracy: 0.9422\n",
      "Epoch 305/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1281 - accuracy: 0.9518 - val_loss: 0.1548 - val_accuracy: 0.9427\n",
      "Epoch 306/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1281 - accuracy: 0.9516 - val_loss: 0.1547 - val_accuracy: 0.9425\n",
      "Epoch 307/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1279 - accuracy: 0.9519 - val_loss: 0.1541 - val_accuracy: 0.9426\n",
      "Epoch 308/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1279 - accuracy: 0.9518 - val_loss: 0.1543 - val_accuracy: 0.9423\n",
      "Epoch 309/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1277 - accuracy: 0.9519 - val_loss: 0.1553 - val_accuracy: 0.9424\n",
      "Epoch 310/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1276 - accuracy: 0.9517 - val_loss: 0.1545 - val_accuracy: 0.9425\n",
      "Epoch 311/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1276 - accuracy: 0.9518 - val_loss: 0.1544 - val_accuracy: 0.9424\n",
      "Epoch 312/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1275 - accuracy: 0.9517 - val_loss: 0.1548 - val_accuracy: 0.9423\n",
      "Epoch 313/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1274 - accuracy: 0.9519 - val_loss: 0.1541 - val_accuracy: 0.9425\n",
      "Epoch 314/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1273 - accuracy: 0.9519 - val_loss: 0.1540 - val_accuracy: 0.9425\n",
      "Epoch 315/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1272 - accuracy: 0.9518 - val_loss: 0.1542 - val_accuracy: 0.9424\n",
      "Epoch 316/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1271 - accuracy: 0.9520 - val_loss: 0.1562 - val_accuracy: 0.9421\n",
      "Epoch 317/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1270 - accuracy: 0.9519 - val_loss: 0.1551 - val_accuracy: 0.9420\n",
      "Epoch 318/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1268 - accuracy: 0.9521 - val_loss: 0.1544 - val_accuracy: 0.9420\n",
      "Epoch 319/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1269 - accuracy: 0.9519 - val_loss: 0.1542 - val_accuracy: 0.9425\n",
      "Epoch 320/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1267 - accuracy: 0.9520 - val_loss: 0.1543 - val_accuracy: 0.9427\n",
      "Epoch 321/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1266 - accuracy: 0.9521 - val_loss: 0.1543 - val_accuracy: 0.9426\n",
      "Epoch 322/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1266 - accuracy: 0.9522 - val_loss: 0.1539 - val_accuracy: 0.9427\n",
      "Epoch 323/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1264 - accuracy: 0.9521 - val_loss: 0.1538 - val_accuracy: 0.9428\n",
      "Epoch 324/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1263 - accuracy: 0.9524 - val_loss: 0.1542 - val_accuracy: 0.9426\n",
      "Epoch 325/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1262 - accuracy: 0.9521 - val_loss: 0.1539 - val_accuracy: 0.9429\n",
      "Epoch 326/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1261 - accuracy: 0.9522 - val_loss: 0.1542 - val_accuracy: 0.9424\n",
      "Epoch 327/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1261 - accuracy: 0.9520 - val_loss: 0.1538 - val_accuracy: 0.9424\n",
      "Epoch 328/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1260 - accuracy: 0.9522 - val_loss: 0.1537 - val_accuracy: 0.9426\n",
      "Epoch 329/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1259 - accuracy: 0.9523 - val_loss: 0.1539 - val_accuracy: 0.9429\n",
      "Epoch 330/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1258 - accuracy: 0.9521 - val_loss: 0.1542 - val_accuracy: 0.9429\n",
      "Epoch 331/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1257 - accuracy: 0.9524 - val_loss: 0.1535 - val_accuracy: 0.9426\n",
      "Epoch 332/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1255 - accuracy: 0.9525 - val_loss: 0.1545 - val_accuracy: 0.9427\n",
      "Epoch 333/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.95 - 0s 72us/step - loss: 0.1256 - accuracy: 0.9523 - val_loss: 0.1545 - val_accuracy: 0.9426\n",
      "Epoch 334/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1255 - accuracy: 0.9524 - val_loss: 0.1539 - val_accuracy: 0.9430\n",
      "Epoch 335/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1255 - accuracy: 0.9524 - val_loss: 0.1535 - val_accuracy: 0.9423\n",
      "Epoch 336/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1252 - accuracy: 0.9522 - val_loss: 0.1539 - val_accuracy: 0.9427\n",
      "Epoch 337/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1252 - accuracy: 0.9524 - val_loss: 0.1536 - val_accuracy: 0.9427\n",
      "Epoch 338/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1251 - accuracy: 0.9523 - val_loss: 0.1542 - val_accuracy: 0.9432\n",
      "Epoch 339/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1250 - accuracy: 0.9525 - val_loss: 0.1537 - val_accuracy: 0.9427\n",
      "Epoch 340/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1250 - accuracy: 0.9525 - val_loss: 0.1535 - val_accuracy: 0.9427\n",
      "Epoch 341/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1248 - accuracy: 0.9525 - val_loss: 0.1544 - val_accuracy: 0.9420\n",
      "Epoch 342/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1248 - accuracy: 0.9526 - val_loss: 0.1538 - val_accuracy: 0.9426\n",
      "Epoch 343/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1246 - accuracy: 0.9525 - val_loss: 0.1530 - val_accuracy: 0.9429\n",
      "Epoch 344/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1246 - accuracy: 0.9524 - val_loss: 0.1535 - val_accuracy: 0.9429\n",
      "Epoch 345/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1246 - accuracy: 0.9527 - val_loss: 0.1534 - val_accuracy: 0.9425\n",
      "Epoch 346/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1244 - accuracy: 0.9528 - val_loss: 0.1538 - val_accuracy: 0.9425\n",
      "Epoch 347/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1244 - accuracy: 0.9527 - val_loss: 0.1533 - val_accuracy: 0.9423\n",
      "Epoch 348/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1242 - accuracy: 0.9526 - val_loss: 0.1539 - val_accuracy: 0.9431\n",
      "Epoch 349/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1241 - accuracy: 0.9527 - val_loss: 0.1536 - val_accuracy: 0.9429\n",
      "Epoch 350/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1241 - accuracy: 0.9527 - val_loss: 0.1532 - val_accuracy: 0.9430\n",
      "Epoch 351/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1240 - accuracy: 0.9527 - val_loss: 0.1538 - val_accuracy: 0.9431\n",
      "Epoch 352/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1239 - accuracy: 0.9527 - val_loss: 0.1537 - val_accuracy: 0.9420\n",
      "Epoch 353/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1238 - accuracy: 0.9528 - val_loss: 0.1532 - val_accuracy: 0.9427\n",
      "Epoch 354/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1238 - accuracy: 0.9529 - val_loss: 0.1538 - val_accuracy: 0.9431\n",
      "Epoch 355/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1237 - accuracy: 0.9529 - val_loss: 0.1531 - val_accuracy: 0.9426\n",
      "Epoch 356/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1235 - accuracy: 0.9530 - val_loss: 0.1538 - val_accuracy: 0.9431\n",
      "Epoch 357/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1235 - accuracy: 0.9529 - val_loss: 0.1535 - val_accuracy: 0.9427\n",
      "Epoch 358/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1234 - accuracy: 0.9529 - val_loss: 0.1534 - val_accuracy: 0.9425\n",
      "Epoch 359/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1233 - accuracy: 0.9531 - val_loss: 0.1535 - val_accuracy: 0.9428\n",
      "Epoch 360/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1233 - accuracy: 0.9532 - val_loss: 0.1528 - val_accuracy: 0.9428\n",
      "Epoch 361/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1232 - accuracy: 0.9530 - val_loss: 0.1532 - val_accuracy: 0.9429\n",
      "Epoch 362/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1231 - accuracy: 0.9529 - val_loss: 0.1529 - val_accuracy: 0.9430\n",
      "Epoch 363/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1230 - accuracy: 0.9531 - val_loss: 0.1532 - val_accuracy: 0.9424\n",
      "Epoch 364/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1230 - accuracy: 0.9530 - val_loss: 0.1528 - val_accuracy: 0.9428\n",
      "Epoch 365/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1229 - accuracy: 0.9530 - val_loss: 0.1534 - val_accuracy: 0.9424\n",
      "Epoch 366/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1227 - accuracy: 0.9531 - val_loss: 0.1532 - val_accuracy: 0.9426\n",
      "Epoch 367/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1227 - accuracy: 0.9530 - val_loss: 0.1527 - val_accuracy: 0.9426\n",
      "Epoch 368/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1227 - accuracy: 0.9531 - val_loss: 0.1525 - val_accuracy: 0.9426\n",
      "Epoch 369/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1225 - accuracy: 0.9531 - val_loss: 0.1535 - val_accuracy: 0.9423\n",
      "Epoch 370/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1225 - accuracy: 0.9531 - val_loss: 0.1526 - val_accuracy: 0.9427\n",
      "Epoch 371/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1224 - accuracy: 0.9532 - val_loss: 0.1525 - val_accuracy: 0.9431\n",
      "Epoch 372/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1223 - accuracy: 0.9532 - val_loss: 0.1530 - val_accuracy: 0.9425\n",
      "Epoch 373/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1222 - accuracy: 0.9533 - val_loss: 0.1527 - val_accuracy: 0.9427\n",
      "Epoch 374/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1221 - accuracy: 0.9531 - val_loss: 0.1532 - val_accuracy: 0.9423\n",
      "Epoch 375/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1220 - accuracy: 0.9533 - val_loss: 0.1527 - val_accuracy: 0.9431\n",
      "Epoch 376/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1220 - accuracy: 0.9533 - val_loss: 0.1528 - val_accuracy: 0.9428\n",
      "Epoch 377/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1219 - accuracy: 0.9535 - val_loss: 0.1529 - val_accuracy: 0.9428\n",
      "Epoch 378/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1219 - accuracy: 0.9533 - val_loss: 0.1537 - val_accuracy: 0.9431\n",
      "Epoch 379/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1218 - accuracy: 0.9534 - val_loss: 0.1528 - val_accuracy: 0.9425\n",
      "Epoch 380/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1217 - accuracy: 0.9533 - val_loss: 0.1527 - val_accuracy: 0.9425\n",
      "Epoch 381/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1216 - accuracy: 0.9533 - val_loss: 0.1529 - val_accuracy: 0.9430\n",
      "Epoch 382/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1216 - accuracy: 0.9534 - val_loss: 0.1526 - val_accuracy: 0.9426\n",
      "Epoch 383/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1214 - accuracy: 0.9534 - val_loss: 0.1536 - val_accuracy: 0.9428\n",
      "Epoch 384/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.95 - 0s 72us/step - loss: 0.1215 - accuracy: 0.9533 - val_loss: 0.1533 - val_accuracy: 0.9428\n",
      "Epoch 385/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1213 - accuracy: 0.9535 - val_loss: 0.1538 - val_accuracy: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1212 - accuracy: 0.9534 - val_loss: 0.1526 - val_accuracy: 0.9423\n",
      "Epoch 387/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1212 - accuracy: 0.9535 - val_loss: 0.1529 - val_accuracy: 0.9428\n",
      "Epoch 388/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1210 - accuracy: 0.9536 - val_loss: 0.1525 - val_accuracy: 0.9432\n",
      "Epoch 389/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1210 - accuracy: 0.9538 - val_loss: 0.1532 - val_accuracy: 0.9432\n",
      "Epoch 390/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1209 - accuracy: 0.9536 - val_loss: 0.1528 - val_accuracy: 0.9426\n",
      "Epoch 391/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1208 - accuracy: 0.9536 - val_loss: 0.1533 - val_accuracy: 0.9424\n",
      "Epoch 392/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1208 - accuracy: 0.9536 - val_loss: 0.1530 - val_accuracy: 0.9430\n",
      "Epoch 393/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1206 - accuracy: 0.9538 - val_loss: 0.1531 - val_accuracy: 0.9424\n",
      "Epoch 394/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1206 - accuracy: 0.9538 - val_loss: 0.1531 - val_accuracy: 0.9424\n",
      "Epoch 395/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1205 - accuracy: 0.9535 - val_loss: 0.1526 - val_accuracy: 0.9425\n",
      "Epoch 396/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1204 - accuracy: 0.9536 - val_loss: 0.1530 - val_accuracy: 0.9427\n",
      "Epoch 397/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1205 - accuracy: 0.9536 - val_loss: 0.1528 - val_accuracy: 0.9423\n",
      "Epoch 398/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1203 - accuracy: 0.9539 - val_loss: 0.1524 - val_accuracy: 0.9426\n",
      "Epoch 399/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1203 - accuracy: 0.9536 - val_loss: 0.1525 - val_accuracy: 0.9424\n",
      "Epoch 400/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1202 - accuracy: 0.9538 - val_loss: 0.1529 - val_accuracy: 0.9428\n",
      "Epoch 401/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1201 - accuracy: 0.9537 - val_loss: 0.1529 - val_accuracy: 0.9424\n",
      "Epoch 402/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1200 - accuracy: 0.9540 - val_loss: 0.1526 - val_accuracy: 0.9423\n",
      "Epoch 403/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1199 - accuracy: 0.9539 - val_loss: 0.1532 - val_accuracy: 0.9430\n",
      "Epoch 404/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1199 - accuracy: 0.9538 - val_loss: 0.1527 - val_accuracy: 0.9428\n",
      "Epoch 405/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1197 - accuracy: 0.9538 - val_loss: 0.1529 - val_accuracy: 0.9428\n",
      "Epoch 406/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1198 - accuracy: 0.9538 - val_loss: 0.1525 - val_accuracy: 0.9427\n",
      "Epoch 407/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1197 - accuracy: 0.9540 - val_loss: 0.1529 - val_accuracy: 0.9426\n",
      "Epoch 408/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1197 - accuracy: 0.9538 - val_loss: 0.1526 - val_accuracy: 0.9423\n",
      "Epoch 409/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1195 - accuracy: 0.9539 - val_loss: 0.1526 - val_accuracy: 0.9430\n",
      "Epoch 410/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1194 - accuracy: 0.9542 - val_loss: 0.1534 - val_accuracy: 0.9426\n",
      "Epoch 411/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1194 - accuracy: 0.9542 - val_loss: 0.1526 - val_accuracy: 0.9427\n",
      "Epoch 412/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1194 - accuracy: 0.9542 - val_loss: 0.1532 - val_accuracy: 0.9431\n",
      "Epoch 413/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1192 - accuracy: 0.9541 - val_loss: 0.1528 - val_accuracy: 0.9425\n",
      "Epoch 414/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1192 - accuracy: 0.9542 - val_loss: 0.1528 - val_accuracy: 0.9427\n",
      "Epoch 415/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1191 - accuracy: 0.9540 - val_loss: 0.1530 - val_accuracy: 0.9422\n",
      "Epoch 416/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1191 - accuracy: 0.9540 - val_loss: 0.1529 - val_accuracy: 0.9425\n",
      "Epoch 417/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1190 - accuracy: 0.9541 - val_loss: 0.1527 - val_accuracy: 0.9426\n",
      "Epoch 418/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1189 - accuracy: 0.9543 - val_loss: 0.1522 - val_accuracy: 0.9429\n",
      "Epoch 419/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1189 - accuracy: 0.9541 - val_loss: 0.1526 - val_accuracy: 0.9429\n",
      "Epoch 420/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1188 - accuracy: 0.9544 - val_loss: 0.1527 - val_accuracy: 0.9426\n",
      "Epoch 421/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1188 - accuracy: 0.9542 - val_loss: 0.1528 - val_accuracy: 0.9423\n",
      "Epoch 422/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1186 - accuracy: 0.9543 - val_loss: 0.1530 - val_accuracy: 0.9428\n",
      "Epoch 423/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1184 - accuracy: 0.9543 - val_loss: 0.1525 - val_accuracy: 0.9425\n",
      "Epoch 424/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1184 - accuracy: 0.9544 - val_loss: 0.1526 - val_accuracy: 0.9419\n",
      "Epoch 425/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1185 - accuracy: 0.9544 - val_loss: 0.1528 - val_accuracy: 0.9428\n",
      "Epoch 426/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1184 - accuracy: 0.9543 - val_loss: 0.1528 - val_accuracy: 0.9423\n",
      "Epoch 427/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1182 - accuracy: 0.9544 - val_loss: 0.1527 - val_accuracy: 0.9429\n",
      "Epoch 428/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1182 - accuracy: 0.9544 - val_loss: 0.1535 - val_accuracy: 0.9425\n",
      "Epoch 429/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1182 - accuracy: 0.9543 - val_loss: 0.1531 - val_accuracy: 0.9427\n",
      "Epoch 430/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1181 - accuracy: 0.9544 - val_loss: 0.1528 - val_accuracy: 0.9421\n",
      "Epoch 431/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1180 - accuracy: 0.9547 - val_loss: 0.1534 - val_accuracy: 0.9428\n",
      "Epoch 432/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1179 - accuracy: 0.9544 - val_loss: 0.1529 - val_accuracy: 0.9424\n",
      "Epoch 433/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1178 - accuracy: 0.9543 - val_loss: 0.1526 - val_accuracy: 0.9432\n",
      "Epoch 434/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1178 - accuracy: 0.9546 - val_loss: 0.1529 - val_accuracy: 0.9428\n",
      "Epoch 435/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1178 - accuracy: 0.9546 - val_loss: 0.1527 - val_accuracy: 0.9424\n",
      "Epoch 436/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1177 - accuracy: 0.9545 - val_loss: 0.1527 - val_accuracy: 0.9423\n",
      "Epoch 437/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1176 - accuracy: 0.9543 - val_loss: 0.1525 - val_accuracy: 0.9427\n",
      "Epoch 438/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1174 - accuracy: 0.9547 - val_loss: 0.1534 - val_accuracy: 0.9425\n",
      "Epoch 439/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1175 - accuracy: 0.9547 - val_loss: 0.1527 - val_accuracy: 0.9431\n",
      "Epoch 440/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1174 - accuracy: 0.9546 - val_loss: 0.1527 - val_accuracy: 0.9428\n",
      "Epoch 441/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1174 - accuracy: 0.9546 - val_loss: 0.1524 - val_accuracy: 0.9426\n",
      "Epoch 442/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1172 - accuracy: 0.9546 - val_loss: 0.1530 - val_accuracy: 0.9424\n",
      "Epoch 443/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1172 - accuracy: 0.9547 - val_loss: 0.1532 - val_accuracy: 0.9420\n",
      "Epoch 444/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1171 - accuracy: 0.9548 - val_loss: 0.1529 - val_accuracy: 0.9424\n",
      "Epoch 445/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1170 - accuracy: 0.9547 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 446/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1170 - accuracy: 0.9548 - val_loss: 0.1525 - val_accuracy: 0.9423\n",
      "Epoch 447/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1170 - accuracy: 0.9547 - val_loss: 0.1535 - val_accuracy: 0.9419\n",
      "Epoch 448/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1169 - accuracy: 0.9547 - val_loss: 0.1529 - val_accuracy: 0.9427\n",
      "Epoch 449/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.95 - 0s 74us/step - loss: 0.1167 - accuracy: 0.9549 - val_loss: 0.1532 - val_accuracy: 0.9424\n",
      "Epoch 450/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1167 - accuracy: 0.9550 - val_loss: 0.1532 - val_accuracy: 0.9422\n",
      "Epoch 451/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1167 - accuracy: 0.9550 - val_loss: 0.1532 - val_accuracy: 0.9424\n",
      "Epoch 452/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1166 - accuracy: 0.9550 - val_loss: 0.1527 - val_accuracy: 0.9426\n",
      "Epoch 453/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1164 - accuracy: 0.9548 - val_loss: 0.1532 - val_accuracy: 0.9422\n",
      "Epoch 454/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1165 - accuracy: 0.9551 - val_loss: 0.1530 - val_accuracy: 0.9425\n",
      "Epoch 455/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1163 - accuracy: 0.9549 - val_loss: 0.1522 - val_accuracy: 0.9422\n",
      "Epoch 456/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1163 - accuracy: 0.9549 - val_loss: 0.1528 - val_accuracy: 0.9425\n",
      "Epoch 457/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1163 - accuracy: 0.9550 - val_loss: 0.1528 - val_accuracy: 0.9422\n",
      "Epoch 458/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1162 - accuracy: 0.9550 - val_loss: 0.1533 - val_accuracy: 0.9426\n",
      "Epoch 459/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1161 - accuracy: 0.9550 - val_loss: 0.1531 - val_accuracy: 0.9429\n",
      "Epoch 460/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1160 - accuracy: 0.9549 - val_loss: 0.1534 - val_accuracy: 0.9427\n",
      "Epoch 461/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.95 - 0s 73us/step - loss: 0.1160 - accuracy: 0.9549 - val_loss: 0.1534 - val_accuracy: 0.9426\n",
      "Epoch 462/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1159 - accuracy: 0.9553 - val_loss: 0.1533 - val_accuracy: 0.9416\n",
      "Epoch 463/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1159 - accuracy: 0.9551 - val_loss: 0.1531 - val_accuracy: 0.9421\n",
      "Epoch 464/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1158 - accuracy: 0.9552 - val_loss: 0.1529 - val_accuracy: 0.9422\n",
      "Epoch 465/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1158 - accuracy: 0.9551 - val_loss: 0.1528 - val_accuracy: 0.9428\n",
      "Epoch 466/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1157 - accuracy: 0.9550 - val_loss: 0.1534 - val_accuracy: 0.9422\n",
      "Epoch 467/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1156 - accuracy: 0.9553 - val_loss: 0.1538 - val_accuracy: 0.9421\n",
      "Epoch 468/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1156 - accuracy: 0.9555 - val_loss: 0.1529 - val_accuracy: 0.9426\n",
      "Epoch 469/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1155 - accuracy: 0.9553 - val_loss: 0.1532 - val_accuracy: 0.9429\n",
      "Epoch 470/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1154 - accuracy: 0.9551 - val_loss: 0.1527 - val_accuracy: 0.9426\n",
      "Epoch 471/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1154 - accuracy: 0.9553 - val_loss: 0.1532 - val_accuracy: 0.9425\n",
      "Epoch 472/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1153 - accuracy: 0.9554 - val_loss: 0.1536 - val_accuracy: 0.9426\n",
      "Epoch 473/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1151 - accuracy: 0.9554 - val_loss: 0.1531 - val_accuracy: 0.9419\n",
      "Epoch 474/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1151 - accuracy: 0.9552 - val_loss: 0.1528 - val_accuracy: 0.9425\n",
      "Epoch 475/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1150 - accuracy: 0.9557 - val_loss: 0.1537 - val_accuracy: 0.9418\n",
      "Epoch 476/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1150 - accuracy: 0.9554 - val_loss: 0.1540 - val_accuracy: 0.9422\n",
      "Epoch 477/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1150 - accuracy: 0.9552 - val_loss: 0.1534 - val_accuracy: 0.9422\n",
      "Epoch 478/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1150 - accuracy: 0.9555 - val_loss: 0.1529 - val_accuracy: 0.9427\n",
      "Epoch 479/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1148 - accuracy: 0.9555 - val_loss: 0.1529 - val_accuracy: 0.9424\n",
      "Epoch 480/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1148 - accuracy: 0.9554 - val_loss: 0.1528 - val_accuracy: 0.9422\n",
      "Epoch 481/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1147 - accuracy: 0.9553 - val_loss: 0.1530 - val_accuracy: 0.9425\n",
      "Epoch 482/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1146 - accuracy: 0.9555 - val_loss: 0.1535 - val_accuracy: 0.9430\n",
      "Epoch 483/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.95 - 0s 73us/step - loss: 0.1146 - accuracy: 0.9554 - val_loss: 0.1528 - val_accuracy: 0.9425\n",
      "Epoch 484/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1146 - accuracy: 0.9556 - val_loss: 0.1534 - val_accuracy: 0.9425\n",
      "Epoch 485/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1144 - accuracy: 0.9557 - val_loss: 0.1533 - val_accuracy: 0.9426\n",
      "Epoch 486/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1142 - accuracy: 0.9558 - val_loss: 0.1535 - val_accuracy: 0.9427\n",
      "Epoch 487/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1142 - accuracy: 0.9555 - val_loss: 0.1532 - val_accuracy: 0.9421\n",
      "Epoch 488/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1142 - accuracy: 0.9557 - val_loss: 0.1533 - val_accuracy: 0.9422\n",
      "Epoch 489/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1141 - accuracy: 0.9558 - val_loss: 0.1533 - val_accuracy: 0.9426\n",
      "Epoch 490/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1140 - accuracy: 0.9558 - val_loss: 0.1534 - val_accuracy: 0.9421\n",
      "Epoch 491/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1141 - accuracy: 0.9556 - val_loss: 0.1537 - val_accuracy: 0.9422\n",
      "Epoch 492/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1140 - accuracy: 0.9559 - val_loss: 0.1544 - val_accuracy: 0.9422\n",
      "Epoch 493/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1139 - accuracy: 0.9560 - val_loss: 0.1530 - val_accuracy: 0.9421\n",
      "Epoch 494/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1139 - accuracy: 0.9558 - val_loss: 0.1534 - val_accuracy: 0.9430\n",
      "Epoch 495/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1138 - accuracy: 0.9558 - val_loss: 0.1535 - val_accuracy: 0.9418\n",
      "Epoch 496/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1138 - accuracy: 0.9558 - val_loss: 0.1537 - val_accuracy: 0.9428\n",
      "Epoch 497/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1137 - accuracy: 0.9560 - val_loss: 0.1533 - val_accuracy: 0.9423\n",
      "Epoch 498/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1136 - accuracy: 0.9558 - val_loss: 0.1536 - val_accuracy: 0.9425\n",
      "Epoch 499/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1135 - accuracy: 0.9560 - val_loss: 0.1535 - val_accuracy: 0.9419\n",
      "Epoch 500/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1135 - accuracy: 0.9560 - val_loss: 0.1535 - val_accuracy: 0.9421\n",
      "Epoch 501/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1135 - accuracy: 0.9557 - val_loss: 0.1537 - val_accuracy: 0.9427\n",
      "Epoch 502/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1134 - accuracy: 0.9560 - val_loss: 0.1534 - val_accuracy: 0.9422\n",
      "Epoch 503/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1133 - accuracy: 0.9560 - val_loss: 0.1539 - val_accuracy: 0.9421\n",
      "Epoch 504/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1132 - accuracy: 0.9559 - val_loss: 0.1556 - val_accuracy: 0.9412\n",
      "Epoch 505/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1132 - accuracy: 0.9559 - val_loss: 0.1536 - val_accuracy: 0.9420\n",
      "Epoch 506/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1130 - accuracy: 0.9562 - val_loss: 0.1536 - val_accuracy: 0.9414\n",
      "Epoch 507/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1130 - accuracy: 0.9560 - val_loss: 0.1541 - val_accuracy: 0.9424\n",
      "Epoch 508/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1131 - accuracy: 0.9561 - val_loss: 0.1548 - val_accuracy: 0.9415\n",
      "Epoch 509/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1129 - accuracy: 0.9560 - val_loss: 0.1533 - val_accuracy: 0.9417\n",
      "Epoch 510/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1129 - accuracy: 0.9561 - val_loss: 0.1536 - val_accuracy: 0.9418\n",
      "Epoch 511/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1128 - accuracy: 0.9561 - val_loss: 0.1535 - val_accuracy: 0.9420\n",
      "Epoch 512/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1128 - accuracy: 0.9561 - val_loss: 0.1536 - val_accuracy: 0.9427\n",
      "Epoch 513/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1128 - accuracy: 0.9560 - val_loss: 0.1535 - val_accuracy: 0.9419\n",
      "Epoch 514/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1126 - accuracy: 0.9563 - val_loss: 0.1543 - val_accuracy: 0.9419\n",
      "Epoch 515/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1126 - accuracy: 0.9562 - val_loss: 0.1537 - val_accuracy: 0.9421\n",
      "Epoch 516/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1126 - accuracy: 0.9561 - val_loss: 0.1537 - val_accuracy: 0.9427\n",
      "Epoch 517/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1124 - accuracy: 0.9564 - val_loss: 0.1537 - val_accuracy: 0.9423\n",
      "Epoch 518/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1124 - accuracy: 0.9564 - val_loss: 0.1542 - val_accuracy: 0.9416\n",
      "Epoch 519/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1123 - accuracy: 0.9563 - val_loss: 0.1542 - val_accuracy: 0.9417\n",
      "Epoch 520/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1122 - accuracy: 0.9564 - val_loss: 0.1534 - val_accuracy: 0.9418\n",
      "Epoch 521/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1122 - accuracy: 0.9562 - val_loss: 0.1538 - val_accuracy: 0.9420\n",
      "Epoch 522/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1121 - accuracy: 0.9564 - val_loss: 0.1535 - val_accuracy: 0.9419\n",
      "Epoch 523/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1121 - accuracy: 0.9564 - val_loss: 0.1534 - val_accuracy: 0.9425\n",
      "Epoch 524/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1121 - accuracy: 0.9563 - val_loss: 0.1544 - val_accuracy: 0.9420\n",
      "Epoch 525/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1119 - accuracy: 0.9562 - val_loss: 0.1541 - val_accuracy: 0.9418\n",
      "Epoch 526/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1119 - accuracy: 0.9565 - val_loss: 0.1541 - val_accuracy: 0.9417\n",
      "Epoch 527/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1118 - accuracy: 0.9565 - val_loss: 0.1541 - val_accuracy: 0.9416\n",
      "Epoch 528/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1119 - accuracy: 0.9565 - val_loss: 0.1544 - val_accuracy: 0.9423\n",
      "Epoch 529/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1116 - accuracy: 0.9564 - val_loss: 0.1544 - val_accuracy: 0.9423\n",
      "Epoch 530/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1116 - accuracy: 0.9565 - val_loss: 0.1539 - val_accuracy: 0.9424\n",
      "Epoch 531/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1115 - accuracy: 0.9565 - val_loss: 0.1547 - val_accuracy: 0.9417\n",
      "Epoch 532/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1115 - accuracy: 0.9564 - val_loss: 0.1547 - val_accuracy: 0.9409\n",
      "Epoch 533/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1115 - accuracy: 0.9568 - val_loss: 0.1538 - val_accuracy: 0.9422\n",
      "Epoch 534/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1113 - accuracy: 0.9567 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 535/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1113 - accuracy: 0.9567 - val_loss: 0.1540 - val_accuracy: 0.9420\n",
      "Epoch 536/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1112 - accuracy: 0.9568 - val_loss: 0.1550 - val_accuracy: 0.9417\n",
      "Epoch 537/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1112 - accuracy: 0.9568 - val_loss: 0.1544 - val_accuracy: 0.9419\n",
      "Epoch 538/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1111 - accuracy: 0.9567 - val_loss: 0.1540 - val_accuracy: 0.9419\n",
      "Epoch 539/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1112 - accuracy: 0.9567 - val_loss: 0.1538 - val_accuracy: 0.9423\n",
      "Epoch 540/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1110 - accuracy: 0.9569 - val_loss: 0.1550 - val_accuracy: 0.9424\n",
      "Epoch 541/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1110 - accuracy: 0.9568 - val_loss: 0.1542 - val_accuracy: 0.9412\n",
      "Epoch 542/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1109 - accuracy: 0.9566 - val_loss: 0.1545 - val_accuracy: 0.9417\n",
      "Epoch 543/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1107 - accuracy: 0.9568 - val_loss: 0.1547 - val_accuracy: 0.9416\n",
      "Epoch 544/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1107 - accuracy: 0.9567 - val_loss: 0.1546 - val_accuracy: 0.9418\n",
      "Epoch 545/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1106 - accuracy: 0.9570 - val_loss: 0.1556 - val_accuracy: 0.9416\n",
      "Epoch 546/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1106 - accuracy: 0.9568 - val_loss: 0.1540 - val_accuracy: 0.9422\n",
      "Epoch 547/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1105 - accuracy: 0.9570 - val_loss: 0.1539 - val_accuracy: 0.9427\n",
      "Epoch 548/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1105 - accuracy: 0.9569 - val_loss: 0.1545 - val_accuracy: 0.9420\n",
      "Epoch 549/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1103 - accuracy: 0.9570 - val_loss: 0.1543 - val_accuracy: 0.9422\n",
      "Epoch 550/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1104 - accuracy: 0.9570 - val_loss: 0.1549 - val_accuracy: 0.9417\n",
      "Epoch 551/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1102 - accuracy: 0.9570 - val_loss: 0.1548 - val_accuracy: 0.9418\n",
      "Epoch 552/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1103 - accuracy: 0.9569 - val_loss: 0.1550 - val_accuracy: 0.9412\n",
      "Epoch 553/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1102 - accuracy: 0.9571 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
      "Epoch 554/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1101 - accuracy: 0.9572 - val_loss: 0.1543 - val_accuracy: 0.9422\n",
      "Epoch 555/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1101 - accuracy: 0.9571 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 556/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1100 - accuracy: 0.9570 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 557/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1098 - accuracy: 0.9571 - val_loss: 0.1545 - val_accuracy: 0.9413\n",
      "Epoch 558/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1099 - accuracy: 0.9570 - val_loss: 0.1545 - val_accuracy: 0.9419\n",
      "Epoch 559/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1097 - accuracy: 0.9573 - val_loss: 0.1550 - val_accuracy: 0.9418\n",
      "Epoch 560/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1097 - accuracy: 0.9572 - val_loss: 0.1555 - val_accuracy: 0.9419\n",
      "Epoch 561/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1097 - accuracy: 0.9572 - val_loss: 0.1559 - val_accuracy: 0.9417\n",
      "Epoch 562/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1095 - accuracy: 0.9572 - val_loss: 0.1544 - val_accuracy: 0.9421\n",
      "Epoch 563/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1095 - accuracy: 0.9574 - val_loss: 0.1559 - val_accuracy: 0.9417\n",
      "Epoch 564/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1094 - accuracy: 0.9572 - val_loss: 0.1547 - val_accuracy: 0.9414\n",
      "Epoch 565/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1094 - accuracy: 0.9573 - val_loss: 0.1550 - val_accuracy: 0.9416\n",
      "Epoch 566/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1094 - accuracy: 0.9572 - val_loss: 0.1545 - val_accuracy: 0.9420\n",
      "Epoch 567/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1093 - accuracy: 0.9575 - val_loss: 0.1556 - val_accuracy: 0.9416\n",
      "Epoch 568/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1093 - accuracy: 0.9575 - val_loss: 0.1555 - val_accuracy: 0.9417\n",
      "Epoch 569/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1091 - accuracy: 0.9575 - val_loss: 0.1557 - val_accuracy: 0.9417\n",
      "Epoch 570/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1089 - accuracy: 0.9573 - val_loss: 0.1558 - val_accuracy: 0.9409\n",
      "Epoch 571/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1090 - accuracy: 0.9573 - val_loss: 0.1551 - val_accuracy: 0.9423\n",
      "Epoch 572/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1089 - accuracy: 0.9574 - val_loss: 0.1562 - val_accuracy: 0.9423\n",
      "Epoch 573/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1089 - accuracy: 0.9576 - val_loss: 0.1551 - val_accuracy: 0.9417\n",
      "Epoch 574/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1089 - accuracy: 0.9577 - val_loss: 0.1549 - val_accuracy: 0.9418\n",
      "Epoch 575/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1088 - accuracy: 0.9577 - val_loss: 0.1553 - val_accuracy: 0.9416\n",
      "Epoch 576/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1087 - accuracy: 0.9574 - val_loss: 0.1559 - val_accuracy: 0.9420\n",
      "Epoch 577/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1087 - accuracy: 0.9576 - val_loss: 0.1561 - val_accuracy: 0.9420\n",
      "Epoch 578/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1085 - accuracy: 0.9578 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 579/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1085 - accuracy: 0.9577 - val_loss: 0.1551 - val_accuracy: 0.9418\n",
      "Epoch 580/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1084 - accuracy: 0.9577 - val_loss: 0.1571 - val_accuracy: 0.9415\n",
      "Epoch 581/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1085 - accuracy: 0.9575 - val_loss: 0.1560 - val_accuracy: 0.9421\n",
      "Epoch 582/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1083 - accuracy: 0.9576 - val_loss: 0.1549 - val_accuracy: 0.9417\n",
      "Epoch 583/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1083 - accuracy: 0.9577 - val_loss: 0.1555 - val_accuracy: 0.9406\n",
      "Epoch 584/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1082 - accuracy: 0.9576 - val_loss: 0.1567 - val_accuracy: 0.9410\n",
      "Epoch 585/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1082 - accuracy: 0.9577 - val_loss: 0.1562 - val_accuracy: 0.9413\n",
      "Epoch 586/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1081 - accuracy: 0.9578 - val_loss: 0.1570 - val_accuracy: 0.9408\n",
      "Epoch 587/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1080 - accuracy: 0.9579 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 588/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1079 - accuracy: 0.9579 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 589/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1078 - accuracy: 0.9579 - val_loss: 0.1566 - val_accuracy: 0.9412\n",
      "Epoch 590/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1078 - accuracy: 0.9579 - val_loss: 0.1576 - val_accuracy: 0.9406\n",
      "Epoch 591/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1077 - accuracy: 0.9580 - val_loss: 0.1559 - val_accuracy: 0.9421\n",
      "Epoch 592/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1078 - accuracy: 0.9578 - val_loss: 0.1561 - val_accuracy: 0.9411\n",
      "Epoch 593/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1076 - accuracy: 0.9581 - val_loss: 0.1560 - val_accuracy: 0.9413\n",
      "Epoch 594/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1076 - accuracy: 0.9577 - val_loss: 0.1562 - val_accuracy: 0.9418\n",
      "Epoch 595/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1075 - accuracy: 0.9581 - val_loss: 0.1567 - val_accuracy: 0.9419\n",
      "Epoch 596/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1073 - accuracy: 0.9580 - val_loss: 0.1569 - val_accuracy: 0.9410\n",
      "Epoch 597/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1075 - accuracy: 0.9583 - val_loss: 0.1561 - val_accuracy: 0.9411\n",
      "Epoch 598/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1073 - accuracy: 0.9582 - val_loss: 0.1559 - val_accuracy: 0.9413\n",
      "Epoch 599/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1073 - accuracy: 0.9582 - val_loss: 0.1556 - val_accuracy: 0.9413\n",
      "Epoch 600/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1072 - accuracy: 0.9582 - val_loss: 0.1575 - val_accuracy: 0.9412\n",
      "Epoch 601/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1072 - accuracy: 0.9582 - val_loss: 0.1564 - val_accuracy: 0.9417\n",
      "Epoch 602/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1070 - accuracy: 0.9581 - val_loss: 0.1557 - val_accuracy: 0.9409\n",
      "Epoch 603/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.1564 - val_accuracy: 0.9421\n",
      "Epoch 604/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1069 - accuracy: 0.9584 - val_loss: 0.1566 - val_accuracy: 0.9407\n",
      "Epoch 605/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1068 - accuracy: 0.9582 - val_loss: 0.1560 - val_accuracy: 0.9415\n",
      "Epoch 606/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1068 - accuracy: 0.9581 - val_loss: 0.1559 - val_accuracy: 0.9413\n",
      "Epoch 607/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.1574 - val_accuracy: 0.9420\n",
      "Epoch 608/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1067 - accuracy: 0.9582 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
      "Epoch 609/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1067 - accuracy: 0.9582 - val_loss: 0.1565 - val_accuracy: 0.9411\n",
      "Epoch 610/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1066 - accuracy: 0.9582 - val_loss: 0.1559 - val_accuracy: 0.9414\n",
      "Epoch 611/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1065 - accuracy: 0.9582 - val_loss: 0.1569 - val_accuracy: 0.9411\n",
      "Epoch 612/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1063 - accuracy: 0.9583 - val_loss: 0.1591 - val_accuracy: 0.9414\n",
      "Epoch 613/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1062 - accuracy: 0.9584 - val_loss: 0.1567 - val_accuracy: 0.9412\n",
      "Epoch 614/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1064 - accuracy: 0.9584 - val_loss: 0.1566 - val_accuracy: 0.9411\n",
      "Epoch 615/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1063 - accuracy: 0.9585 - val_loss: 0.1563 - val_accuracy: 0.9416\n",
      "Epoch 616/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1062 - accuracy: 0.9585 - val_loss: 0.1571 - val_accuracy: 0.9407\n",
      "Epoch 617/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1062 - accuracy: 0.9584 - val_loss: 0.1569 - val_accuracy: 0.9410\n",
      "Epoch 618/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1059 - accuracy: 0.9586 - val_loss: 0.1576 - val_accuracy: 0.9414\n",
      "Epoch 619/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1060 - accuracy: 0.9587 - val_loss: 0.1573 - val_accuracy: 0.9407\n",
      "Epoch 620/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1058 - accuracy: 0.9588 - val_loss: 0.1574 - val_accuracy: 0.9407\n",
      "Epoch 621/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1059 - accuracy: 0.9585 - val_loss: 0.1567 - val_accuracy: 0.9416\n",
      "Epoch 622/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1059 - accuracy: 0.9589 - val_loss: 0.1566 - val_accuracy: 0.9413\n",
      "Epoch 623/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1056 - accuracy: 0.9587 - val_loss: 0.1577 - val_accuracy: 0.9410\n",
      "Epoch 624/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1057 - accuracy: 0.9586 - val_loss: 0.1573 - val_accuracy: 0.9411\n",
      "Epoch 625/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1056 - accuracy: 0.9586 - val_loss: 0.1575 - val_accuracy: 0.9410\n",
      "Epoch 626/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1055 - accuracy: 0.9589 - val_loss: 0.1577 - val_accuracy: 0.9409\n",
      "Epoch 627/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1056 - accuracy: 0.9589 - val_loss: 0.1570 - val_accuracy: 0.9414\n",
      "Epoch 628/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1054 - accuracy: 0.9586 - val_loss: 0.1576 - val_accuracy: 0.9412\n",
      "Epoch 629/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1053 - accuracy: 0.9588 - val_loss: 0.1578 - val_accuracy: 0.9407\n",
      "Epoch 630/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1054 - accuracy: 0.9587 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
      "Epoch 631/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1052 - accuracy: 0.9590 - val_loss: 0.1573 - val_accuracy: 0.9412\n",
      "Epoch 632/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1052 - accuracy: 0.9588 - val_loss: 0.1579 - val_accuracy: 0.9415\n",
      "Epoch 633/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1050 - accuracy: 0.9591 - val_loss: 0.1577 - val_accuracy: 0.9414\n",
      "Epoch 634/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1051 - accuracy: 0.9589 - val_loss: 0.1572 - val_accuracy: 0.9415\n",
      "Epoch 635/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1049 - accuracy: 0.9592 - val_loss: 0.1585 - val_accuracy: 0.9408\n",
      "Epoch 636/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1050 - accuracy: 0.9589 - val_loss: 0.1590 - val_accuracy: 0.9420\n",
      "Epoch 637/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1049 - accuracy: 0.9590 - val_loss: 0.1570 - val_accuracy: 0.9410\n",
      "Epoch 638/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1046 - accuracy: 0.9591 - val_loss: 0.1586 - val_accuracy: 0.9408\n",
      "Epoch 639/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1048 - accuracy: 0.9588 - val_loss: 0.1574 - val_accuracy: 0.9415\n",
      "Epoch 640/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1046 - accuracy: 0.9590 - val_loss: 0.1578 - val_accuracy: 0.9406\n",
      "Epoch 641/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1047 - accuracy: 0.9590 - val_loss: 0.1585 - val_accuracy: 0.9410\n",
      "Epoch 642/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1045 - accuracy: 0.9591 - val_loss: 0.1583 - val_accuracy: 0.9410\n",
      "Epoch 643/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1044 - accuracy: 0.9591 - val_loss: 0.1583 - val_accuracy: 0.9408\n",
      "Epoch 644/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1044 - accuracy: 0.9592 - val_loss: 0.1581 - val_accuracy: 0.9406\n",
      "Epoch 645/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1044 - accuracy: 0.9591 - val_loss: 0.1584 - val_accuracy: 0.9413\n",
      "Epoch 646/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1043 - accuracy: 0.9592 - val_loss: 0.1590 - val_accuracy: 0.9412\n",
      "Epoch 647/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1043 - accuracy: 0.9592 - val_loss: 0.1590 - val_accuracy: 0.9406\n",
      "Epoch 648/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1042 - accuracy: 0.9592 - val_loss: 0.1588 - val_accuracy: 0.9419\n",
      "Epoch 649/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1040 - accuracy: 0.9593 - val_loss: 0.1584 - val_accuracy: 0.9407\n",
      "Epoch 650/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1039 - accuracy: 0.9592 - val_loss: 0.1581 - val_accuracy: 0.9408\n",
      "Epoch 651/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1040 - accuracy: 0.9594 - val_loss: 0.1584 - val_accuracy: 0.9413\n",
      "Epoch 652/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1039 - accuracy: 0.9595 - val_loss: 0.1588 - val_accuracy: 0.9413\n",
      "Epoch 653/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1039 - accuracy: 0.9595 - val_loss: 0.1595 - val_accuracy: 0.9405\n",
      "Epoch 654/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1038 - accuracy: 0.9592 - val_loss: 0.1585 - val_accuracy: 0.9410\n",
      "Epoch 655/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1036 - accuracy: 0.9597 - val_loss: 0.1592 - val_accuracy: 0.9411\n",
      "Epoch 656/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1036 - accuracy: 0.9594 - val_loss: 0.1583 - val_accuracy: 0.9413\n",
      "Epoch 657/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1035 - accuracy: 0.9595 - val_loss: 0.1594 - val_accuracy: 0.9404\n",
      "Epoch 658/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1034 - accuracy: 0.9593 - val_loss: 0.1590 - val_accuracy: 0.9407\n",
      "Epoch 659/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1035 - accuracy: 0.9596 - val_loss: 0.1580 - val_accuracy: 0.9415\n",
      "Epoch 660/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1033 - accuracy: 0.9594 - val_loss: 0.1592 - val_accuracy: 0.9405\n",
      "Epoch 661/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1032 - accuracy: 0.9598 - val_loss: 0.1587 - val_accuracy: 0.9415\n",
      "Epoch 662/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1032 - accuracy: 0.9594 - val_loss: 0.1599 - val_accuracy: 0.9408\n",
      "Epoch 663/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1031 - accuracy: 0.9596 - val_loss: 0.1590 - val_accuracy: 0.9419\n",
      "Epoch 664/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1030 - accuracy: 0.9598 - val_loss: 0.1597 - val_accuracy: 0.9396\n",
      "Epoch 665/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1031 - accuracy: 0.9594 - val_loss: 0.1599 - val_accuracy: 0.9407\n",
      "Epoch 666/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1029 - accuracy: 0.9598 - val_loss: 0.1609 - val_accuracy: 0.9405\n",
      "Epoch 667/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1030 - accuracy: 0.9597 - val_loss: 0.1590 - val_accuracy: 0.9411\n",
      "Epoch 668/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1028 - accuracy: 0.9598 - val_loss: 0.1603 - val_accuracy: 0.9411\n",
      "Epoch 669/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1028 - accuracy: 0.9600 - val_loss: 0.1596 - val_accuracy: 0.9406\n",
      "Epoch 670/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1027 - accuracy: 0.9598 - val_loss: 0.1596 - val_accuracy: 0.9403\n",
      "Epoch 671/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1026 - accuracy: 0.9598 - val_loss: 0.1596 - val_accuracy: 0.9402\n",
      "Epoch 672/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1026 - accuracy: 0.9601 - val_loss: 0.1586 - val_accuracy: 0.9408\n",
      "Epoch 673/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1025 - accuracy: 0.9598 - val_loss: 0.1591 - val_accuracy: 0.9406\n",
      "Epoch 674/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1026 - accuracy: 0.9597 - val_loss: 0.1593 - val_accuracy: 0.9402\n",
      "Epoch 675/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1023 - accuracy: 0.9599 - val_loss: 0.1600 - val_accuracy: 0.9407\n",
      "Epoch 676/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1024 - accuracy: 0.9598 - val_loss: 0.1594 - val_accuracy: 0.9405\n",
      "Epoch 677/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1023 - accuracy: 0.9600 - val_loss: 0.1593 - val_accuracy: 0.9411\n",
      "Epoch 678/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1022 - accuracy: 0.9600 - val_loss: 0.1610 - val_accuracy: 0.9409\n",
      "Epoch 679/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1020 - accuracy: 0.9600 - val_loss: 0.1595 - val_accuracy: 0.9405\n",
      "Epoch 680/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1020 - accuracy: 0.9600 - val_loss: 0.1597 - val_accuracy: 0.9409\n",
      "Epoch 681/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1021 - accuracy: 0.9599 - val_loss: 0.1595 - val_accuracy: 0.9411\n",
      "Epoch 682/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1019 - accuracy: 0.9601 - val_loss: 0.1600 - val_accuracy: 0.9412\n",
      "Epoch 683/800\n",
      "6000/6000 [==============================] - 1s 95us/step - loss: 0.1018 - accuracy: 0.9599 - val_loss: 0.1601 - val_accuracy: 0.9394\n",
      "Epoch 684/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1018 - accuracy: 0.9601 - val_loss: 0.1600 - val_accuracy: 0.9399\n",
      "Epoch 685/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1018 - accuracy: 0.9602 - val_loss: 0.1600 - val_accuracy: 0.9409\n",
      "Epoch 686/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1016 - accuracy: 0.9603 - val_loss: 0.1609 - val_accuracy: 0.9400\n",
      "Epoch 687/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1014 - accuracy: 0.9603 - val_loss: 0.1605 - val_accuracy: 0.9395\n",
      "Epoch 688/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1015 - accuracy: 0.9603 - val_loss: 0.1603 - val_accuracy: 0.9416\n",
      "Epoch 689/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1015 - accuracy: 0.9604 - val_loss: 0.1611 - val_accuracy: 0.9405\n",
      "Epoch 690/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1014 - accuracy: 0.9601 - val_loss: 0.1617 - val_accuracy: 0.9400\n",
      "Epoch 691/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1012 - accuracy: 0.9603 - val_loss: 0.1618 - val_accuracy: 0.9410\n",
      "Epoch 692/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1012 - accuracy: 0.9605 - val_loss: 0.1602 - val_accuracy: 0.9406\n",
      "Epoch 693/800\n",
      "6000/6000 [==============================] - 1s 83us/step - loss: 0.1012 - accuracy: 0.9604 - val_loss: 0.1609 - val_accuracy: 0.9406\n",
      "Epoch 694/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1011 - accuracy: 0.9603 - val_loss: 0.1603 - val_accuracy: 0.9402\n",
      "Epoch 695/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1009 - accuracy: 0.9607 - val_loss: 0.1604 - val_accuracy: 0.9413\n",
      "Epoch 696/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1009 - accuracy: 0.9608 - val_loss: 0.1612 - val_accuracy: 0.9416\n",
      "Epoch 697/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1009 - accuracy: 0.9605 - val_loss: 0.1611 - val_accuracy: 0.9387\n",
      "Epoch 698/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1008 - accuracy: 0.9604 - val_loss: 0.1620 - val_accuracy: 0.9401\n",
      "Epoch 699/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1008 - accuracy: 0.9604 - val_loss: 0.1625 - val_accuracy: 0.9401\n",
      "Epoch 700/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1006 - accuracy: 0.9606 - val_loss: 0.1610 - val_accuracy: 0.9406\n",
      "Epoch 701/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1006 - accuracy: 0.9605 - val_loss: 0.1622 - val_accuracy: 0.9402\n",
      "Epoch 702/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1006 - accuracy: 0.9609 - val_loss: 0.1612 - val_accuracy: 0.9406\n",
      "Epoch 703/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1005 - accuracy: 0.9607 - val_loss: 0.1603 - val_accuracy: 0.9404\n",
      "Epoch 704/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1005 - accuracy: 0.9608 - val_loss: 0.1619 - val_accuracy: 0.9400\n",
      "Epoch 705/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1004 - accuracy: 0.9607 - val_loss: 0.1614 - val_accuracy: 0.9398\n",
      "Epoch 706/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1003 - accuracy: 0.9606 - val_loss: 0.1610 - val_accuracy: 0.9403\n",
      "Epoch 707/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1003 - accuracy: 0.9606 - val_loss: 0.1625 - val_accuracy: 0.9404\n",
      "Epoch 708/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1003 - accuracy: 0.9607 - val_loss: 0.1623 - val_accuracy: 0.9415\n",
      "Epoch 709/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1001 - accuracy: 0.9610 - val_loss: 0.1617 - val_accuracy: 0.9404\n",
      "Epoch 710/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1618 - val_accuracy: 0.9402\n",
      "Epoch 711/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0999 - accuracy: 0.9609 - val_loss: 0.1629 - val_accuracy: 0.9402\n",
      "Epoch 712/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0999 - accuracy: 0.9610 - val_loss: 0.1616 - val_accuracy: 0.9407\n",
      "Epoch 713/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0998 - accuracy: 0.9609 - val_loss: 0.1625 - val_accuracy: 0.9403\n",
      "Epoch 714/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0998 - accuracy: 0.9609 - val_loss: 0.1625 - val_accuracy: 0.9404\n",
      "Epoch 715/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0997 - accuracy: 0.9609 - val_loss: 0.1622 - val_accuracy: 0.9402\n",
      "Epoch 716/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0997 - accuracy: 0.9610 - val_loss: 0.1619 - val_accuracy: 0.9410\n",
      "Epoch 717/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0995 - accuracy: 0.9611 - val_loss: 0.1619 - val_accuracy: 0.9395\n",
      "Epoch 718/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0996 - accuracy: 0.9609 - val_loss: 0.1628 - val_accuracy: 0.9406\n",
      "Epoch 719/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0995 - accuracy: 0.9614 - val_loss: 0.1626 - val_accuracy: 0.9410\n",
      "Epoch 720/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0994 - accuracy: 0.9612 - val_loss: 0.1621 - val_accuracy: 0.9396\n",
      "Epoch 721/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0992 - accuracy: 0.9613 - val_loss: 0.1633 - val_accuracy: 0.9408\n",
      "Epoch 722/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0994 - accuracy: 0.9610 - val_loss: 0.1622 - val_accuracy: 0.9404\n",
      "Epoch 723/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0993 - accuracy: 0.9614 - val_loss: 0.1628 - val_accuracy: 0.9400\n",
      "Epoch 724/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0990 - accuracy: 0.9612 - val_loss: 0.1626 - val_accuracy: 0.9410\n",
      "Epoch 725/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0990 - accuracy: 0.9611 - val_loss: 0.1640 - val_accuracy: 0.9406\n",
      "Epoch 726/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0990 - accuracy: 0.9613 - val_loss: 0.1640 - val_accuracy: 0.9400\n",
      "Epoch 727/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0988 - accuracy: 0.9612 - val_loss: 0.1638 - val_accuracy: 0.9400\n",
      "Epoch 728/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0989 - accuracy: 0.9611 - val_loss: 0.1642 - val_accuracy: 0.9403\n",
      "Epoch 729/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0989 - accuracy: 0.9614 - val_loss: 0.1637 - val_accuracy: 0.9405\n",
      "Epoch 730/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0989 - accuracy: 0.9611 - val_loss: 0.1641 - val_accuracy: 0.9401\n",
      "Epoch 731/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0986 - accuracy: 0.9614 - val_loss: 0.1636 - val_accuracy: 0.9394\n",
      "Epoch 732/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0985 - accuracy: 0.9615 - val_loss: 0.1641 - val_accuracy: 0.9396\n",
      "Epoch 733/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0983 - accuracy: 0.9617 - val_loss: 0.1642 - val_accuracy: 0.9401\n",
      "Epoch 734/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0985 - accuracy: 0.9613 - val_loss: 0.1632 - val_accuracy: 0.9401\n",
      "Epoch 735/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0984 - accuracy: 0.9616 - val_loss: 0.1646 - val_accuracy: 0.9405\n",
      "Epoch 736/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1632 - val_accuracy: 0.9405\n",
      "Epoch 737/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0981 - accuracy: 0.9615 - val_loss: 0.1650 - val_accuracy: 0.9401\n",
      "Epoch 738/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0981 - accuracy: 0.9617 - val_loss: 0.1631 - val_accuracy: 0.9407\n",
      "Epoch 739/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0982 - accuracy: 0.9618 - val_loss: 0.1646 - val_accuracy: 0.9388\n",
      "Epoch 740/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0981 - accuracy: 0.9618 - val_loss: 0.1641 - val_accuracy: 0.9406\n",
      "Epoch 741/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.96 - 0s 74us/step - loss: 0.0981 - accuracy: 0.9615 - val_loss: 0.1638 - val_accuracy: 0.9402\n",
      "Epoch 742/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0978 - accuracy: 0.9620 - val_loss: 0.1645 - val_accuracy: 0.9385\n",
      "Epoch 743/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0979 - accuracy: 0.9617 - val_loss: 0.1656 - val_accuracy: 0.9387\n",
      "Epoch 744/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0976 - accuracy: 0.9619 - val_loss: 0.1659 - val_accuracy: 0.9389\n",
      "Epoch 745/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0977 - accuracy: 0.9618 - val_loss: 0.1638 - val_accuracy: 0.9395\n",
      "Epoch 746/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0976 - accuracy: 0.9621 - val_loss: 0.1648 - val_accuracy: 0.9400\n",
      "Epoch 747/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0975 - accuracy: 0.9618 - val_loss: 0.1649 - val_accuracy: 0.9406\n",
      "Epoch 748/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0974 - accuracy: 0.9618 - val_loss: 0.1646 - val_accuracy: 0.9397\n",
      "Epoch 749/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0974 - accuracy: 0.9620 - val_loss: 0.1641 - val_accuracy: 0.9407\n",
      "Epoch 750/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0972 - accuracy: 0.9620 - val_loss: 0.1652 - val_accuracy: 0.9395\n",
      "Epoch 751/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0973 - accuracy: 0.9619 - val_loss: 0.1641 - val_accuracy: 0.9405\n",
      "Epoch 752/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0972 - accuracy: 0.9618 - val_loss: 0.1649 - val_accuracy: 0.9398\n",
      "Epoch 753/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0972 - accuracy: 0.9623 - val_loss: 0.1641 - val_accuracy: 0.9397\n",
      "Epoch 754/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0971 - accuracy: 0.9618 - val_loss: 0.1648 - val_accuracy: 0.9399\n",
      "Epoch 755/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0969 - accuracy: 0.9621 - val_loss: 0.1645 - val_accuracy: 0.9401\n",
      "Epoch 756/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0970 - accuracy: 0.9622 - val_loss: 0.1645 - val_accuracy: 0.9405\n",
      "Epoch 757/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0969 - accuracy: 0.9623 - val_loss: 0.1645 - val_accuracy: 0.9402\n",
      "Epoch 758/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0968 - accuracy: 0.9620 - val_loss: 0.1654 - val_accuracy: 0.9394\n",
      "Epoch 759/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0968 - accuracy: 0.9623 - val_loss: 0.1669 - val_accuracy: 0.9386\n",
      "Epoch 760/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0966 - accuracy: 0.9621 - val_loss: 0.1655 - val_accuracy: 0.9395\n",
      "Epoch 761/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0966 - accuracy: 0.9623 - val_loss: 0.1652 - val_accuracy: 0.9399\n",
      "Epoch 762/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0965 - accuracy: 0.9621 - val_loss: 0.1653 - val_accuracy: 0.9398\n",
      "Epoch 763/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0964 - accuracy: 0.9623 - val_loss: 0.1653 - val_accuracy: 0.9399\n",
      "Epoch 764/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0965 - accuracy: 0.9625 - val_loss: 0.1652 - val_accuracy: 0.9396\n",
      "Epoch 765/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0964 - accuracy: 0.9624 - val_loss: 0.1658 - val_accuracy: 0.9390\n",
      "Epoch 766/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0963 - accuracy: 0.9625 - val_loss: 0.1675 - val_accuracy: 0.9395\n",
      "Epoch 767/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0961 - accuracy: 0.9626 - val_loss: 0.1671 - val_accuracy: 0.9386\n",
      "Epoch 768/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0961 - accuracy: 0.9624 - val_loss: 0.1655 - val_accuracy: 0.9392\n",
      "Epoch 769/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.1658 - val_accuracy: 0.9394\n",
      "Epoch 770/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0961 - accuracy: 0.9625 - val_loss: 0.1660 - val_accuracy: 0.9396\n",
      "Epoch 771/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0958 - accuracy: 0.9625 - val_loss: 0.1659 - val_accuracy: 0.9403\n",
      "Epoch 772/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0957 - accuracy: 0.9629 - val_loss: 0.1669 - val_accuracy: 0.9391\n",
      "Epoch 773/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0959 - accuracy: 0.9625 - val_loss: 0.1666 - val_accuracy: 0.9392\n",
      "Epoch 774/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0955 - accuracy: 0.9630 - val_loss: 0.1660 - val_accuracy: 0.9399\n",
      "Epoch 775/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0956 - accuracy: 0.9626 - val_loss: 0.1669 - val_accuracy: 0.9398\n",
      "Epoch 776/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0956 - accuracy: 0.9628 - val_loss: 0.1668 - val_accuracy: 0.9397\n",
      "Epoch 777/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0954 - accuracy: 0.9627 - val_loss: 0.1665 - val_accuracy: 0.9403\n",
      "Epoch 778/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0953 - accuracy: 0.9629 - val_loss: 0.1668 - val_accuracy: 0.9407\n",
      "Epoch 779/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0952 - accuracy: 0.9628 - val_loss: 0.1670 - val_accuracy: 0.9399\n",
      "Epoch 780/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0953 - accuracy: 0.9628 - val_loss: 0.1656 - val_accuracy: 0.9399\n",
      "Epoch 781/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0951 - accuracy: 0.9629 - val_loss: 0.1670 - val_accuracy: 0.9387\n",
      "Epoch 782/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0951 - accuracy: 0.9628 - val_loss: 0.1685 - val_accuracy: 0.9390\n",
      "Epoch 783/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0950 - accuracy: 0.9632 - val_loss: 0.1661 - val_accuracy: 0.9402\n",
      "Epoch 784/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0950 - accuracy: 0.9630 - val_loss: 0.1664 - val_accuracy: 0.9397\n",
      "Epoch 785/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0950 - accuracy: 0.9628 - val_loss: 0.1674 - val_accuracy: 0.9389\n",
      "Epoch 786/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0947 - accuracy: 0.9629 - val_loss: 0.1667 - val_accuracy: 0.9392\n",
      "Epoch 787/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0946 - accuracy: 0.9632 - val_loss: 0.1680 - val_accuracy: 0.9403\n",
      "Epoch 788/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0948 - accuracy: 0.9631 - val_loss: 0.1679 - val_accuracy: 0.9389\n",
      "Epoch 789/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0946 - accuracy: 0.9631 - val_loss: 0.1677 - val_accuracy: 0.9400\n",
      "Epoch 790/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0945 - accuracy: 0.9633 - val_loss: 0.1691 - val_accuracy: 0.9394\n",
      "Epoch 791/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0946 - accuracy: 0.9632 - val_loss: 0.1676 - val_accuracy: 0.9397\n",
      "Epoch 792/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0943 - accuracy: 0.9633 - val_loss: 0.1691 - val_accuracy: 0.9402\n",
      "Epoch 793/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0944 - accuracy: 0.9632 - val_loss: 0.1690 - val_accuracy: 0.9382\n",
      "Epoch 794/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0941 - accuracy: 0.9633 - val_loss: 0.1701 - val_accuracy: 0.9389\n",
      "Epoch 795/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0940 - accuracy: 0.9633 - val_loss: 0.1680 - val_accuracy: 0.9391\n",
      "Epoch 796/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0940 - accuracy: 0.9633 - val_loss: 0.1681 - val_accuracy: 0.9391\n",
      "Epoch 797/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0939 - accuracy: 0.9636 - val_loss: 0.1709 - val_accuracy: 0.9391\n",
      "Epoch 798/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0940 - accuracy: 0.9632 - val_loss: 0.1676 - val_accuracy: 0.9391\n",
      "Epoch 799/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0938 - accuracy: 0.9635 - val_loss: 0.1676 - val_accuracy: 0.9396\n",
      "Epoch 800/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0937 - accuracy: 0.9634 - val_loss: 0.1711 - val_accuracy: 0.9382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:122: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 6000 samples, validate on 2001 samples\n",
      "Epoch 1/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.4272 - accuracy: 0.8553 - val_loss: 0.2680 - val_accuracy: 0.9248\n",
      "Epoch 2/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.2517 - accuracy: 0.9302 - val_loss: 0.2664 - val_accuracy: 0.9248\n",
      "Epoch 3/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2510 - accuracy: 0.9302 - val_loss: 0.2658 - val_accuracy: 0.9248\n",
      "Epoch 4/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.2505 - accuracy: 0.9302 - val_loss: 0.2655 - val_accuracy: 0.9248\n",
      "Epoch 5/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.2501 - accuracy: 0.9302 - val_loss: 0.2649 - val_accuracy: 0.9248\n",
      "Epoch 6/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.2497 - accuracy: 0.9302 - val_loss: 0.2644 - val_accuracy: 0.9248\n",
      "Epoch 7/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.2493 - accuracy: 0.9302 - val_loss: 0.2641 - val_accuracy: 0.9248\n",
      "Epoch 8/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.2489 - accuracy: 0.9302 - val_loss: 0.2638 - val_accuracy: 0.9248\n",
      "Epoch 9/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2486 - accuracy: 0.9302 - val_loss: 0.2634 - val_accuracy: 0.9248\n",
      "Epoch 10/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2482 - accuracy: 0.9302 - val_loss: 0.2630 - val_accuracy: 0.9248\n",
      "Epoch 11/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.2478 - accuracy: 0.9302 - val_loss: 0.2626 - val_accuracy: 0.9248\n",
      "Epoch 12/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.2474 - accuracy: 0.9302 - val_loss: 0.2623 - val_accuracy: 0.9248\n",
      "Epoch 13/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2470 - accuracy: 0.9302 - val_loss: 0.2615 - val_accuracy: 0.9248\n",
      "Epoch 14/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.2465 - accuracy: 0.9302 - val_loss: 0.2612 - val_accuracy: 0.9248\n",
      "Epoch 15/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.2459 - accuracy: 0.9302 - val_loss: 0.2606 - val_accuracy: 0.9248\n",
      "Epoch 16/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2454 - accuracy: 0.9302 - val_loss: 0.2600 - val_accuracy: 0.9248\n",
      "Epoch 17/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.2447 - accuracy: 0.9302 - val_loss: 0.2593 - val_accuracy: 0.9248\n",
      "Epoch 18/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2441 - accuracy: 0.9302 - val_loss: 0.2585 - val_accuracy: 0.9248\n",
      "Epoch 19/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.2433 - accuracy: 0.9302 - val_loss: 0.2579 - val_accuracy: 0.9248\n",
      "Epoch 20/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.2425 - accuracy: 0.9302 - val_loss: 0.2570 - val_accuracy: 0.9248\n",
      "Epoch 21/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2415 - accuracy: 0.9302 - val_loss: 0.2558 - val_accuracy: 0.9248\n",
      "Epoch 22/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.2405 - accuracy: 0.9302 - val_loss: 0.2548 - val_accuracy: 0.9248\n",
      "Epoch 23/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.2393 - accuracy: 0.9302 - val_loss: 0.2537 - val_accuracy: 0.9248\n",
      "Epoch 24/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2381 - accuracy: 0.9302 - val_loss: 0.2525 - val_accuracy: 0.9248\n",
      "Epoch 25/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2367 - accuracy: 0.9302 - val_loss: 0.2511 - val_accuracy: 0.9248\n",
      "Epoch 26/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2352 - accuracy: 0.9302 - val_loss: 0.2495 - val_accuracy: 0.9248\n",
      "Epoch 27/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2337 - accuracy: 0.9302 - val_loss: 0.2479 - val_accuracy: 0.9248\n",
      "Epoch 28/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.2322 - accuracy: 0.9302 - val_loss: 0.2466 - val_accuracy: 0.9248\n",
      "Epoch 29/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2306 - accuracy: 0.9303 - val_loss: 0.2450 - val_accuracy: 0.9248\n",
      "Epoch 30/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2290 - accuracy: 0.9303 - val_loss: 0.2434 - val_accuracy: 0.9249\n",
      "Epoch 31/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.2274 - accuracy: 0.9304 - val_loss: 0.2418 - val_accuracy: 0.9250\n",
      "Epoch 32/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2259 - accuracy: 0.9304 - val_loss: 0.2402 - val_accuracy: 0.9250\n",
      "Epoch 33/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2244 - accuracy: 0.9305 - val_loss: 0.2386 - val_accuracy: 0.9250\n",
      "Epoch 34/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2229 - accuracy: 0.9306 - val_loss: 0.2372 - val_accuracy: 0.9250\n",
      "Epoch 35/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2214 - accuracy: 0.9307 - val_loss: 0.2358 - val_accuracy: 0.9251\n",
      "Epoch 36/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2200 - accuracy: 0.9308 - val_loss: 0.2341 - val_accuracy: 0.9252\n",
      "Epoch 37/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2185 - accuracy: 0.9308 - val_loss: 0.2329 - val_accuracy: 0.9252\n",
      "Epoch 38/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2170 - accuracy: 0.9309 - val_loss: 0.2314 - val_accuracy: 0.9254\n",
      "Epoch 39/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2156 - accuracy: 0.9309 - val_loss: 0.2300 - val_accuracy: 0.9254\n",
      "Epoch 40/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2141 - accuracy: 0.9310 - val_loss: 0.2284 - val_accuracy: 0.9254\n",
      "Epoch 41/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2126 - accuracy: 0.9311 - val_loss: 0.2270 - val_accuracy: 0.9254\n",
      "Epoch 42/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.2111 - accuracy: 0.9311 - val_loss: 0.2252 - val_accuracy: 0.9254\n",
      "Epoch 43/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2096 - accuracy: 0.9313 - val_loss: 0.2239 - val_accuracy: 0.9256\n",
      "Epoch 44/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2081 - accuracy: 0.9313 - val_loss: 0.2224 - val_accuracy: 0.9257\n",
      "Epoch 45/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2066 - accuracy: 0.9314 - val_loss: 0.2209 - val_accuracy: 0.9256\n",
      "Epoch 46/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2052 - accuracy: 0.9316 - val_loss: 0.2193 - val_accuracy: 0.9259\n",
      "Epoch 47/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2037 - accuracy: 0.9318 - val_loss: 0.2179 - val_accuracy: 0.9260\n",
      "Epoch 48/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2023 - accuracy: 0.9318 - val_loss: 0.2167 - val_accuracy: 0.9263\n",
      "Epoch 49/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2008 - accuracy: 0.9319 - val_loss: 0.2149 - val_accuracy: 0.9264\n",
      "Epoch 50/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1993 - accuracy: 0.9320 - val_loss: 0.2135 - val_accuracy: 0.9266\n",
      "Epoch 51/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1979 - accuracy: 0.9322 - val_loss: 0.2123 - val_accuracy: 0.9265\n",
      "Epoch 52/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1965 - accuracy: 0.9323 - val_loss: 0.2105 - val_accuracy: 0.9268\n",
      "Epoch 53/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1951 - accuracy: 0.9324 - val_loss: 0.2096 - val_accuracy: 0.9266\n",
      "Epoch 54/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1938 - accuracy: 0.9325 - val_loss: 0.2083 - val_accuracy: 0.9269\n",
      "Epoch 55/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1926 - accuracy: 0.9326 - val_loss: 0.2068 - val_accuracy: 0.9269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1913 - accuracy: 0.9328 - val_loss: 0.2061 - val_accuracy: 0.9272\n",
      "Epoch 57/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1902 - accuracy: 0.9331 - val_loss: 0.2048 - val_accuracy: 0.9269\n",
      "Epoch 58/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1891 - accuracy: 0.9332 - val_loss: 0.2037 - val_accuracy: 0.9271\n",
      "Epoch 59/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1880 - accuracy: 0.9333 - val_loss: 0.2026 - val_accuracy: 0.9274\n",
      "Epoch 60/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1870 - accuracy: 0.9335 - val_loss: 0.2016 - val_accuracy: 0.9276\n",
      "Epoch 61/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1860 - accuracy: 0.9337 - val_loss: 0.2010 - val_accuracy: 0.9274\n",
      "Epoch 62/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1851 - accuracy: 0.9338 - val_loss: 0.2000 - val_accuracy: 0.9276\n",
      "Epoch 63/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1841 - accuracy: 0.9341 - val_loss: 0.1993 - val_accuracy: 0.9280\n",
      "Epoch 64/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1833 - accuracy: 0.9342 - val_loss: 0.1984 - val_accuracy: 0.9283\n",
      "Epoch 65/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1824 - accuracy: 0.9345 - val_loss: 0.1979 - val_accuracy: 0.9283\n",
      "Epoch 66/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1816 - accuracy: 0.9347 - val_loss: 0.1966 - val_accuracy: 0.9288\n",
      "Epoch 67/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1808 - accuracy: 0.9350 - val_loss: 0.1962 - val_accuracy: 0.9289\n",
      "Epoch 68/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1801 - accuracy: 0.9351 - val_loss: 0.1956 - val_accuracy: 0.9291\n",
      "Epoch 69/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1794 - accuracy: 0.9353 - val_loss: 0.1952 - val_accuracy: 0.9293\n",
      "Epoch 70/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1787 - accuracy: 0.9355 - val_loss: 0.1945 - val_accuracy: 0.9296\n",
      "Epoch 71/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1782 - accuracy: 0.9357 - val_loss: 0.1936 - val_accuracy: 0.9298\n",
      "Epoch 72/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1774 - accuracy: 0.9360 - val_loss: 0.1934 - val_accuracy: 0.9301\n",
      "Epoch 73/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1768 - accuracy: 0.9362 - val_loss: 0.1932 - val_accuracy: 0.9300\n",
      "Epoch 74/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1763 - accuracy: 0.9364 - val_loss: 0.1923 - val_accuracy: 0.9304\n",
      "Epoch 75/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1757 - accuracy: 0.9366 - val_loss: 0.1920 - val_accuracy: 0.9311\n",
      "Epoch 76/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1752 - accuracy: 0.9369 - val_loss: 0.1912 - val_accuracy: 0.9308\n",
      "Epoch 77/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1746 - accuracy: 0.9369 - val_loss: 0.1912 - val_accuracy: 0.9308\n",
      "Epoch 78/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1741 - accuracy: 0.9371 - val_loss: 0.1902 - val_accuracy: 0.9311\n",
      "Epoch 79/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1736 - accuracy: 0.9372 - val_loss: 0.1900 - val_accuracy: 0.9310\n",
      "Epoch 80/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1731 - accuracy: 0.9374 - val_loss: 0.1894 - val_accuracy: 0.9313\n",
      "Epoch 81/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1726 - accuracy: 0.9377 - val_loss: 0.1892 - val_accuracy: 0.9315\n",
      "Epoch 82/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1721 - accuracy: 0.9378 - val_loss: 0.1885 - val_accuracy: 0.9316\n",
      "Epoch 83/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1716 - accuracy: 0.9381 - val_loss: 0.1882 - val_accuracy: 0.9317\n",
      "Epoch 84/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1712 - accuracy: 0.9380 - val_loss: 0.1876 - val_accuracy: 0.9321\n",
      "Epoch 85/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1707 - accuracy: 0.9382 - val_loss: 0.1874 - val_accuracy: 0.9319\n",
      "Epoch 86/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1702 - accuracy: 0.9384 - val_loss: 0.1873 - val_accuracy: 0.9323\n",
      "Epoch 87/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1698 - accuracy: 0.9386 - val_loss: 0.1864 - val_accuracy: 0.9325\n",
      "Epoch 88/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1693 - accuracy: 0.9389 - val_loss: 0.1866 - val_accuracy: 0.9324\n",
      "Epoch 89/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1689 - accuracy: 0.9388 - val_loss: 0.1854 - val_accuracy: 0.9326\n",
      "Epoch 90/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1684 - accuracy: 0.9391 - val_loss: 0.1854 - val_accuracy: 0.9328\n",
      "Epoch 91/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1680 - accuracy: 0.9391 - val_loss: 0.1848 - val_accuracy: 0.9328\n",
      "Epoch 92/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1675 - accuracy: 0.9394 - val_loss: 0.1842 - val_accuracy: 0.9332\n",
      "Epoch 93/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1670 - accuracy: 0.9394 - val_loss: 0.1838 - val_accuracy: 0.9334\n",
      "Epoch 94/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1666 - accuracy: 0.9399 - val_loss: 0.1834 - val_accuracy: 0.9334\n",
      "Epoch 95/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1662 - accuracy: 0.9398 - val_loss: 0.1829 - val_accuracy: 0.9334\n",
      "Epoch 96/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1657 - accuracy: 0.9399 - val_loss: 0.1828 - val_accuracy: 0.9336\n",
      "Epoch 97/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1652 - accuracy: 0.9402 - val_loss: 0.1824 - val_accuracy: 0.9336\n",
      "Epoch 98/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1647 - accuracy: 0.9404 - val_loss: 0.1819 - val_accuracy: 0.9340\n",
      "Epoch 99/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1644 - accuracy: 0.9403 - val_loss: 0.1816 - val_accuracy: 0.9338\n",
      "Epoch 100/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1639 - accuracy: 0.9405 - val_loss: 0.1811 - val_accuracy: 0.9343\n",
      "Epoch 101/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1635 - accuracy: 0.9406 - val_loss: 0.1811 - val_accuracy: 0.9345\n",
      "Epoch 102/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1631 - accuracy: 0.9407 - val_loss: 0.1805 - val_accuracy: 0.9345\n",
      "Epoch 103/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1626 - accuracy: 0.9409 - val_loss: 0.1799 - val_accuracy: 0.9347\n",
      "Epoch 104/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1623 - accuracy: 0.9410 - val_loss: 0.1798 - val_accuracy: 0.9342\n",
      "Epoch 105/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1618 - accuracy: 0.9409 - val_loss: 0.1792 - val_accuracy: 0.9347\n",
      "Epoch 106/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1615 - accuracy: 0.9413 - val_loss: 0.1788 - val_accuracy: 0.9348\n",
      "Epoch 107/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1611 - accuracy: 0.9413 - val_loss: 0.1786 - val_accuracy: 0.9350\n",
      "Epoch 108/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1607 - accuracy: 0.9416 - val_loss: 0.1786 - val_accuracy: 0.9351\n",
      "Epoch 109/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1604 - accuracy: 0.9417 - val_loss: 0.1777 - val_accuracy: 0.9351\n",
      "Epoch 110/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1600 - accuracy: 0.9417 - val_loss: 0.1776 - val_accuracy: 0.9354\n",
      "Epoch 111/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1596 - accuracy: 0.9418 - val_loss: 0.1777 - val_accuracy: 0.9352\n",
      "Epoch 112/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1593 - accuracy: 0.9419 - val_loss: 0.1772 - val_accuracy: 0.9352\n",
      "Epoch 113/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1589 - accuracy: 0.9422 - val_loss: 0.1769 - val_accuracy: 0.9353\n",
      "Epoch 114/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1586 - accuracy: 0.9422 - val_loss: 0.1764 - val_accuracy: 0.9356\n",
      "Epoch 115/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1584 - accuracy: 0.9425 - val_loss: 0.1762 - val_accuracy: 0.9355\n",
      "Epoch 116/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1580 - accuracy: 0.9425 - val_loss: 0.1758 - val_accuracy: 0.9357\n",
      "Epoch 117/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1576 - accuracy: 0.9425 - val_loss: 0.1759 - val_accuracy: 0.9358\n",
      "Epoch 118/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1573 - accuracy: 0.9426 - val_loss: 0.1755 - val_accuracy: 0.9358\n",
      "Epoch 119/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1570 - accuracy: 0.9427 - val_loss: 0.1749 - val_accuracy: 0.9362\n",
      "Epoch 120/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1567 - accuracy: 0.9429 - val_loss: 0.1746 - val_accuracy: 0.9362\n",
      "Epoch 121/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1564 - accuracy: 0.9430 - val_loss: 0.1746 - val_accuracy: 0.9365\n",
      "Epoch 122/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1561 - accuracy: 0.9431 - val_loss: 0.1743 - val_accuracy: 0.9367\n",
      "Epoch 123/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1558 - accuracy: 0.9433 - val_loss: 0.1740 - val_accuracy: 0.9364\n",
      "Epoch 124/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1555 - accuracy: 0.9434 - val_loss: 0.1736 - val_accuracy: 0.9366\n",
      "Epoch 125/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1553 - accuracy: 0.9435 - val_loss: 0.1739 - val_accuracy: 0.9362\n",
      "Epoch 126/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1550 - accuracy: 0.9434 - val_loss: 0.1732 - val_accuracy: 0.9366\n",
      "Epoch 127/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1547 - accuracy: 0.9435 - val_loss: 0.1729 - val_accuracy: 0.9368\n",
      "Epoch 128/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1545 - accuracy: 0.9437 - val_loss: 0.1724 - val_accuracy: 0.9371\n",
      "Epoch 129/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1542 - accuracy: 0.9438 - val_loss: 0.1722 - val_accuracy: 0.9369\n",
      "Epoch 130/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1539 - accuracy: 0.9438 - val_loss: 0.1728 - val_accuracy: 0.9367\n",
      "Epoch 131/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1537 - accuracy: 0.9438 - val_loss: 0.1723 - val_accuracy: 0.9371\n",
      "Epoch 132/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.94 - 0s 74us/step - loss: 0.1534 - accuracy: 0.9440 - val_loss: 0.1717 - val_accuracy: 0.9368\n",
      "Epoch 133/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1532 - accuracy: 0.9441 - val_loss: 0.1714 - val_accuracy: 0.9371\n",
      "Epoch 134/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1529 - accuracy: 0.9443 - val_loss: 0.1714 - val_accuracy: 0.9371\n",
      "Epoch 135/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1526 - accuracy: 0.9442 - val_loss: 0.1714 - val_accuracy: 0.9374\n",
      "Epoch 136/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1525 - accuracy: 0.9443 - val_loss: 0.1715 - val_accuracy: 0.9378\n",
      "Epoch 137/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1522 - accuracy: 0.9445 - val_loss: 0.1711 - val_accuracy: 0.9374\n",
      "Epoch 138/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1520 - accuracy: 0.9444 - val_loss: 0.1707 - val_accuracy: 0.9378\n",
      "Epoch 139/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1518 - accuracy: 0.9444 - val_loss: 0.1701 - val_accuracy: 0.9380\n",
      "Epoch 140/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1516 - accuracy: 0.9446 - val_loss: 0.1703 - val_accuracy: 0.9379\n",
      "Epoch 141/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1513 - accuracy: 0.9448 - val_loss: 0.1701 - val_accuracy: 0.9375\n",
      "Epoch 142/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1511 - accuracy: 0.9446 - val_loss: 0.1697 - val_accuracy: 0.9381\n",
      "Epoch 143/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1508 - accuracy: 0.9447 - val_loss: 0.1704 - val_accuracy: 0.9378\n",
      "Epoch 144/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1507 - accuracy: 0.9449 - val_loss: 0.1696 - val_accuracy: 0.9381\n",
      "Epoch 145/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1504 - accuracy: 0.9449 - val_loss: 0.1697 - val_accuracy: 0.9381\n",
      "Epoch 146/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1502 - accuracy: 0.9451 - val_loss: 0.1692 - val_accuracy: 0.9381\n",
      "Epoch 147/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1500 - accuracy: 0.9452 - val_loss: 0.1692 - val_accuracy: 0.9381\n",
      "Epoch 148/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1498 - accuracy: 0.9450 - val_loss: 0.1686 - val_accuracy: 0.9383\n",
      "Epoch 149/800\n",
      "6000/6000 [==============================] - 0s 70us/step - loss: 0.1496 - accuracy: 0.9452 - val_loss: 0.1694 - val_accuracy: 0.9379\n",
      "Epoch 150/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1494 - accuracy: 0.9453 - val_loss: 0.1689 - val_accuracy: 0.9385\n",
      "Epoch 151/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1492 - accuracy: 0.9453 - val_loss: 0.1683 - val_accuracy: 0.9383\n",
      "Epoch 152/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1490 - accuracy: 0.9454 - val_loss: 0.1684 - val_accuracy: 0.9382\n",
      "Epoch 153/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1488 - accuracy: 0.9454 - val_loss: 0.1683 - val_accuracy: 0.9389\n",
      "Epoch 154/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1486 - accuracy: 0.9454 - val_loss: 0.1680 - val_accuracy: 0.9381\n",
      "Epoch 155/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1483 - accuracy: 0.9456 - val_loss: 0.1678 - val_accuracy: 0.9387\n",
      "Epoch 156/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1483 - accuracy: 0.9455 - val_loss: 0.1678 - val_accuracy: 0.9388\n",
      "Epoch 157/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1480 - accuracy: 0.9457 - val_loss: 0.1680 - val_accuracy: 0.9382\n",
      "Epoch 158/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1478 - accuracy: 0.9458 - val_loss: 0.1674 - val_accuracy: 0.9391\n",
      "Epoch 159/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1477 - accuracy: 0.9459 - val_loss: 0.1672 - val_accuracy: 0.9384\n",
      "Epoch 160/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1475 - accuracy: 0.9459 - val_loss: 0.1671 - val_accuracy: 0.9389\n",
      "Epoch 161/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1473 - accuracy: 0.9459 - val_loss: 0.1668 - val_accuracy: 0.9393\n",
      "Epoch 162/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1471 - accuracy: 0.9460 - val_loss: 0.1668 - val_accuracy: 0.9389\n",
      "Epoch 163/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1469 - accuracy: 0.9460 - val_loss: 0.1664 - val_accuracy: 0.9391\n",
      "Epoch 164/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1466 - accuracy: 0.9461 - val_loss: 0.1666 - val_accuracy: 0.9391\n",
      "Epoch 165/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1465 - accuracy: 0.9461 - val_loss: 0.1666 - val_accuracy: 0.9391\n",
      "Epoch 166/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1463 - accuracy: 0.9462 - val_loss: 0.1672 - val_accuracy: 0.9387\n",
      "Epoch 167/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1461 - accuracy: 0.9463 - val_loss: 0.1661 - val_accuracy: 0.9395\n",
      "Epoch 168/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1459 - accuracy: 0.9463 - val_loss: 0.1661 - val_accuracy: 0.9392\n",
      "Epoch 169/800\n",
      "6000/6000 [==============================] - 1s 101us/step - loss: 0.1458 - accuracy: 0.9463 - val_loss: 0.1661 - val_accuracy: 0.9392\n",
      "Epoch 170/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1456 - accuracy: 0.9464 - val_loss: 0.1659 - val_accuracy: 0.9393\n",
      "Epoch 171/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1454 - accuracy: 0.9465 - val_loss: 0.1654 - val_accuracy: 0.9394\n",
      "Epoch 172/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1452 - accuracy: 0.9465 - val_loss: 0.1653 - val_accuracy: 0.9396\n",
      "Epoch 173/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1450 - accuracy: 0.9468 - val_loss: 0.1655 - val_accuracy: 0.9395\n",
      "Epoch 174/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1448 - accuracy: 0.9467 - val_loss: 0.1649 - val_accuracy: 0.9394\n",
      "Epoch 175/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1446 - accuracy: 0.9468 - val_loss: 0.1650 - val_accuracy: 0.9395\n",
      "Epoch 176/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1445 - accuracy: 0.9469 - val_loss: 0.1648 - val_accuracy: 0.9397\n",
      "Epoch 177/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1443 - accuracy: 0.9468 - val_loss: 0.1646 - val_accuracy: 0.9397\n",
      "Epoch 178/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1441 - accuracy: 0.9471 - val_loss: 0.1645 - val_accuracy: 0.9398\n",
      "Epoch 179/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1440 - accuracy: 0.9469 - val_loss: 0.1648 - val_accuracy: 0.9396\n",
      "Epoch 180/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1438 - accuracy: 0.9469 - val_loss: 0.1643 - val_accuracy: 0.9401\n",
      "Epoch 181/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1436 - accuracy: 0.9470 - val_loss: 0.1654 - val_accuracy: 0.9394\n",
      "Epoch 182/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1435 - accuracy: 0.9470 - val_loss: 0.1642 - val_accuracy: 0.9394\n",
      "Epoch 183/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1434 - accuracy: 0.9470 - val_loss: 0.1644 - val_accuracy: 0.9400\n",
      "Epoch 184/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1432 - accuracy: 0.9473 - val_loss: 0.1639 - val_accuracy: 0.9398\n",
      "Epoch 185/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1430 - accuracy: 0.9472 - val_loss: 0.1639 - val_accuracy: 0.9395\n",
      "Epoch 186/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1429 - accuracy: 0.9474 - val_loss: 0.1637 - val_accuracy: 0.9400\n",
      "Epoch 187/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1426 - accuracy: 0.9473 - val_loss: 0.1635 - val_accuracy: 0.9401\n",
      "Epoch 188/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1425 - accuracy: 0.9474 - val_loss: 0.1631 - val_accuracy: 0.9404\n",
      "Epoch 189/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1423 - accuracy: 0.9475 - val_loss: 0.1638 - val_accuracy: 0.9401\n",
      "Epoch 190/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1422 - accuracy: 0.9475 - val_loss: 0.1632 - val_accuracy: 0.9401\n",
      "Epoch 191/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1420 - accuracy: 0.9474 - val_loss: 0.1633 - val_accuracy: 0.9398\n",
      "Epoch 192/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1419 - accuracy: 0.9477 - val_loss: 0.1632 - val_accuracy: 0.9403\n",
      "Epoch 193/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1418 - accuracy: 0.9476 - val_loss: 0.1629 - val_accuracy: 0.9403\n",
      "Epoch 194/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1415 - accuracy: 0.9477 - val_loss: 0.1627 - val_accuracy: 0.9405\n",
      "Epoch 195/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1415 - accuracy: 0.9476 - val_loss: 0.1626 - val_accuracy: 0.9403\n",
      "Epoch 196/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1413 - accuracy: 0.9478 - val_loss: 0.1627 - val_accuracy: 0.9405\n",
      "Epoch 197/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1411 - accuracy: 0.9479 - val_loss: 0.1623 - val_accuracy: 0.9404\n",
      "Epoch 198/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1409 - accuracy: 0.9478 - val_loss: 0.1623 - val_accuracy: 0.9406\n",
      "Epoch 199/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1408 - accuracy: 0.9477 - val_loss: 0.1623 - val_accuracy: 0.9405\n",
      "Epoch 200/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1406 - accuracy: 0.9479 - val_loss: 0.1620 - val_accuracy: 0.9404\n",
      "Epoch 201/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1405 - accuracy: 0.9479 - val_loss: 0.1619 - val_accuracy: 0.9407\n",
      "Epoch 202/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1404 - accuracy: 0.9481 - val_loss: 0.1620 - val_accuracy: 0.9406\n",
      "Epoch 203/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1402 - accuracy: 0.9480 - val_loss: 0.1618 - val_accuracy: 0.9410\n",
      "Epoch 204/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1401 - accuracy: 0.9481 - val_loss: 0.1619 - val_accuracy: 0.9405\n",
      "Epoch 205/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1399 - accuracy: 0.9480 - val_loss: 0.1618 - val_accuracy: 0.9410\n",
      "Epoch 206/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1398 - accuracy: 0.9482 - val_loss: 0.1611 - val_accuracy: 0.9407\n",
      "Epoch 207/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1395 - accuracy: 0.9481 - val_loss: 0.1617 - val_accuracy: 0.9406\n",
      "Epoch 208/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1395 - accuracy: 0.9483 - val_loss: 0.1609 - val_accuracy: 0.9408\n",
      "Epoch 209/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1393 - accuracy: 0.9484 - val_loss: 0.1618 - val_accuracy: 0.9403\n",
      "Epoch 210/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1391 - accuracy: 0.9483 - val_loss: 0.1615 - val_accuracy: 0.9407\n",
      "Epoch 211/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1390 - accuracy: 0.9484 - val_loss: 0.1610 - val_accuracy: 0.9408\n",
      "Epoch 212/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1390 - accuracy: 0.9485 - val_loss: 0.1609 - val_accuracy: 0.9410\n",
      "Epoch 213/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1388 - accuracy: 0.9484 - val_loss: 0.1609 - val_accuracy: 0.9408\n",
      "Epoch 214/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1386 - accuracy: 0.9485 - val_loss: 0.1610 - val_accuracy: 0.9407\n",
      "Epoch 215/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1386 - accuracy: 0.9485 - val_loss: 0.1603 - val_accuracy: 0.9410\n",
      "Epoch 216/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1384 - accuracy: 0.9485 - val_loss: 0.1604 - val_accuracy: 0.9403\n",
      "Epoch 217/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1382 - accuracy: 0.9486 - val_loss: 0.1607 - val_accuracy: 0.9411\n",
      "Epoch 218/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1380 - accuracy: 0.9487 - val_loss: 0.1607 - val_accuracy: 0.9410\n",
      "Epoch 219/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1379 - accuracy: 0.9488 - val_loss: 0.1605 - val_accuracy: 0.9412\n",
      "Epoch 220/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1379 - accuracy: 0.9488 - val_loss: 0.1599 - val_accuracy: 0.9412\n",
      "Epoch 221/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1377 - accuracy: 0.9488 - val_loss: 0.1607 - val_accuracy: 0.9409\n",
      "Epoch 222/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1377 - accuracy: 0.9488 - val_loss: 0.1601 - val_accuracy: 0.9414\n",
      "Epoch 223/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1374 - accuracy: 0.9489 - val_loss: 0.1598 - val_accuracy: 0.9414\n",
      "Epoch 224/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1372 - accuracy: 0.9489 - val_loss: 0.1595 - val_accuracy: 0.9413\n",
      "Epoch 225/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1371 - accuracy: 0.9489 - val_loss: 0.1604 - val_accuracy: 0.9414\n",
      "Epoch 226/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1370 - accuracy: 0.9490 - val_loss: 0.1596 - val_accuracy: 0.9413\n",
      "Epoch 227/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1369 - accuracy: 0.9490 - val_loss: 0.1596 - val_accuracy: 0.9414\n",
      "Epoch 228/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1368 - accuracy: 0.9490 - val_loss: 0.1596 - val_accuracy: 0.9412\n",
      "Epoch 229/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1366 - accuracy: 0.9491 - val_loss: 0.1594 - val_accuracy: 0.9415\n",
      "Epoch 230/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1365 - accuracy: 0.9489 - val_loss: 0.1594 - val_accuracy: 0.9414\n",
      "Epoch 231/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1364 - accuracy: 0.9493 - val_loss: 0.1593 - val_accuracy: 0.9416\n",
      "Epoch 232/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1362 - accuracy: 0.9492 - val_loss: 0.1588 - val_accuracy: 0.9416\n",
      "Epoch 233/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1361 - accuracy: 0.9492 - val_loss: 0.1588 - val_accuracy: 0.9415\n",
      "Epoch 234/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1360 - accuracy: 0.9491 - val_loss: 0.1588 - val_accuracy: 0.9414\n",
      "Epoch 235/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1358 - accuracy: 0.9492 - val_loss: 0.1586 - val_accuracy: 0.9419\n",
      "Epoch 236/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1357 - accuracy: 0.9492 - val_loss: 0.1584 - val_accuracy: 0.9416\n",
      "Epoch 237/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.94 - 0s 72us/step - loss: 0.1355 - accuracy: 0.9492 - val_loss: 0.1587 - val_accuracy: 0.9420\n",
      "Epoch 238/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1354 - accuracy: 0.9494 - val_loss: 0.1586 - val_accuracy: 0.9417\n",
      "Epoch 239/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1353 - accuracy: 0.9493 - val_loss: 0.1586 - val_accuracy: 0.9418\n",
      "Epoch 240/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1352 - accuracy: 0.9494 - val_loss: 0.1584 - val_accuracy: 0.9416\n",
      "Epoch 241/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1350 - accuracy: 0.9495 - val_loss: 0.1581 - val_accuracy: 0.9417\n",
      "Epoch 242/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1349 - accuracy: 0.9495 - val_loss: 0.1590 - val_accuracy: 0.9415\n",
      "Epoch 243/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1348 - accuracy: 0.9495 - val_loss: 0.1587 - val_accuracy: 0.9417\n",
      "Epoch 244/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1347 - accuracy: 0.9497 - val_loss: 0.1584 - val_accuracy: 0.9412\n",
      "Epoch 245/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1346 - accuracy: 0.9496 - val_loss: 0.1579 - val_accuracy: 0.9418\n",
      "Epoch 246/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1345 - accuracy: 0.9497 - val_loss: 0.1581 - val_accuracy: 0.9420\n",
      "Epoch 247/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1343 - accuracy: 0.9495 - val_loss: 0.1589 - val_accuracy: 0.9418\n",
      "Epoch 248/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1343 - accuracy: 0.9498 - val_loss: 0.1580 - val_accuracy: 0.9417\n",
      "Epoch 249/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1341 - accuracy: 0.9497 - val_loss: 0.1583 - val_accuracy: 0.9415\n",
      "Epoch 250/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1340 - accuracy: 0.9499 - val_loss: 0.1575 - val_accuracy: 0.9421\n",
      "Epoch 251/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1339 - accuracy: 0.9498 - val_loss: 0.1578 - val_accuracy: 0.9416\n",
      "Epoch 252/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1338 - accuracy: 0.9499 - val_loss: 0.1574 - val_accuracy: 0.9421\n",
      "Epoch 253/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1337 - accuracy: 0.9499 - val_loss: 0.1579 - val_accuracy: 0.9414\n",
      "Epoch 254/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1335 - accuracy: 0.9498 - val_loss: 0.1579 - val_accuracy: 0.9420\n",
      "Epoch 255/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1334 - accuracy: 0.9498 - val_loss: 0.1584 - val_accuracy: 0.9418\n",
      "Epoch 256/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1332 - accuracy: 0.9499 - val_loss: 0.1575 - val_accuracy: 0.9416\n",
      "Epoch 257/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1332 - accuracy: 0.9499 - val_loss: 0.1575 - val_accuracy: 0.9418\n",
      "Epoch 258/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1331 - accuracy: 0.9500 - val_loss: 0.1572 - val_accuracy: 0.9423\n",
      "Epoch 259/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1329 - accuracy: 0.9499 - val_loss: 0.1570 - val_accuracy: 0.9420\n",
      "Epoch 260/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1329 - accuracy: 0.9500 - val_loss: 0.1571 - val_accuracy: 0.9422\n",
      "Epoch 261/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1328 - accuracy: 0.9501 - val_loss: 0.1574 - val_accuracy: 0.9420\n",
      "Epoch 262/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1327 - accuracy: 0.9501 - val_loss: 0.1570 - val_accuracy: 0.9419\n",
      "Epoch 263/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1326 - accuracy: 0.9501 - val_loss: 0.1568 - val_accuracy: 0.9422\n",
      "Epoch 264/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1325 - accuracy: 0.9501 - val_loss: 0.1570 - val_accuracy: 0.9424\n",
      "Epoch 265/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1324 - accuracy: 0.9501 - val_loss: 0.1571 - val_accuracy: 0.9425\n",
      "Epoch 266/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1323 - accuracy: 0.9503 - val_loss: 0.1573 - val_accuracy: 0.9420\n",
      "Epoch 267/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1321 - accuracy: 0.9503 - val_loss: 0.1567 - val_accuracy: 0.9424\n",
      "Epoch 268/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1321 - accuracy: 0.9502 - val_loss: 0.1578 - val_accuracy: 0.9417\n",
      "Epoch 269/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1320 - accuracy: 0.9503 - val_loss: 0.1569 - val_accuracy: 0.9422\n",
      "Epoch 270/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1318 - accuracy: 0.9504 - val_loss: 0.1565 - val_accuracy: 0.9426\n",
      "Epoch 271/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1317 - accuracy: 0.9502 - val_loss: 0.1570 - val_accuracy: 0.9421\n",
      "Epoch 272/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1316 - accuracy: 0.9504 - val_loss: 0.1567 - val_accuracy: 0.9425\n",
      "Epoch 273/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1315 - accuracy: 0.9502 - val_loss: 0.1566 - val_accuracy: 0.9420\n",
      "Epoch 274/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1314 - accuracy: 0.9503 - val_loss: 0.1567 - val_accuracy: 0.9423\n",
      "Epoch 275/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1314 - accuracy: 0.9504 - val_loss: 0.1565 - val_accuracy: 0.9424\n",
      "Epoch 276/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1312 - accuracy: 0.9504 - val_loss: 0.1561 - val_accuracy: 0.9424\n",
      "Epoch 277/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1311 - accuracy: 0.9506 - val_loss: 0.1560 - val_accuracy: 0.9422\n",
      "Epoch 278/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1311 - accuracy: 0.9503 - val_loss: 0.1560 - val_accuracy: 0.9426\n",
      "Epoch 279/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1309 - accuracy: 0.9504 - val_loss: 0.1563 - val_accuracy: 0.9424\n",
      "Epoch 280/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1308 - accuracy: 0.9505 - val_loss: 0.1565 - val_accuracy: 0.9428\n",
      "Epoch 281/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1307 - accuracy: 0.9507 - val_loss: 0.1565 - val_accuracy: 0.9420\n",
      "Epoch 282/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1307 - accuracy: 0.9505 - val_loss: 0.1563 - val_accuracy: 0.9420\n",
      "Epoch 283/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1306 - accuracy: 0.9506 - val_loss: 0.1559 - val_accuracy: 0.9421\n",
      "Epoch 284/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1305 - accuracy: 0.9506 - val_loss: 0.1556 - val_accuracy: 0.9421\n",
      "Epoch 285/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1304 - accuracy: 0.9506 - val_loss: 0.1561 - val_accuracy: 0.9424\n",
      "Epoch 286/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1303 - accuracy: 0.9507 - val_loss: 0.1559 - val_accuracy: 0.9424\n",
      "Epoch 287/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1302 - accuracy: 0.9508 - val_loss: 0.1558 - val_accuracy: 0.9421\n",
      "Epoch 288/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1301 - accuracy: 0.9507 - val_loss: 0.1565 - val_accuracy: 0.9423\n",
      "Epoch 289/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1301 - accuracy: 0.9506 - val_loss: 0.1555 - val_accuracy: 0.9423\n",
      "Epoch 290/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1299 - accuracy: 0.9507 - val_loss: 0.1561 - val_accuracy: 0.9427\n",
      "Epoch 291/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1299 - accuracy: 0.9508 - val_loss: 0.1556 - val_accuracy: 0.9423\n",
      "Epoch 292/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1298 - accuracy: 0.9507 - val_loss: 0.1556 - val_accuracy: 0.9420\n",
      "Epoch 293/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1297 - accuracy: 0.9509 - val_loss: 0.1558 - val_accuracy: 0.9427\n",
      "Epoch 294/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1297 - accuracy: 0.9508 - val_loss: 0.1556 - val_accuracy: 0.9421\n",
      "Epoch 295/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1295 - accuracy: 0.9509 - val_loss: 0.1558 - val_accuracy: 0.9423\n",
      "Epoch 296/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1294 - accuracy: 0.9509 - val_loss: 0.1554 - val_accuracy: 0.9418\n",
      "Epoch 297/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1293 - accuracy: 0.9510 - val_loss: 0.1555 - val_accuracy: 0.9421\n",
      "Epoch 298/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1292 - accuracy: 0.9508 - val_loss: 0.1549 - val_accuracy: 0.9425\n",
      "Epoch 299/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1291 - accuracy: 0.9509 - val_loss: 0.1554 - val_accuracy: 0.9420\n",
      "Epoch 300/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1291 - accuracy: 0.9509 - val_loss: 0.1556 - val_accuracy: 0.9423\n",
      "Epoch 301/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1290 - accuracy: 0.9511 - val_loss: 0.1555 - val_accuracy: 0.9423\n",
      "Epoch 302/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1289 - accuracy: 0.9511 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 303/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1288 - accuracy: 0.9512 - val_loss: 0.1556 - val_accuracy: 0.9419\n",
      "Epoch 304/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1286 - accuracy: 0.9512 - val_loss: 0.1555 - val_accuracy: 0.9430\n",
      "Epoch 305/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1286 - accuracy: 0.9511 - val_loss: 0.1550 - val_accuracy: 0.9424\n",
      "Epoch 306/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1284 - accuracy: 0.9513 - val_loss: 0.1560 - val_accuracy: 0.9421\n",
      "Epoch 307/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1284 - accuracy: 0.9512 - val_loss: 0.1552 - val_accuracy: 0.9422\n",
      "Epoch 308/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1284 - accuracy: 0.9512 - val_loss: 0.1552 - val_accuracy: 0.9427\n",
      "Epoch 309/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1283 - accuracy: 0.9513 - val_loss: 0.1555 - val_accuracy: 0.9426\n",
      "Epoch 310/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1282 - accuracy: 0.9512 - val_loss: 0.1553 - val_accuracy: 0.9425\n",
      "Epoch 311/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1282 - accuracy: 0.9512 - val_loss: 0.1554 - val_accuracy: 0.9425\n",
      "Epoch 312/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1281 - accuracy: 0.9511 - val_loss: 0.1558 - val_accuracy: 0.9424\n",
      "Epoch 313/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1279 - accuracy: 0.9512 - val_loss: 0.1555 - val_accuracy: 0.9423\n",
      "Epoch 314/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1277 - accuracy: 0.9514 - val_loss: 0.1552 - val_accuracy: 0.9426\n",
      "Epoch 315/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1278 - accuracy: 0.9512 - val_loss: 0.1549 - val_accuracy: 0.9428\n",
      "Epoch 316/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1276 - accuracy: 0.9514 - val_loss: 0.1552 - val_accuracy: 0.9425\n",
      "Epoch 317/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1276 - accuracy: 0.9514 - val_loss: 0.1557 - val_accuracy: 0.9423\n",
      "Epoch 318/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1275 - accuracy: 0.9514 - val_loss: 0.1551 - val_accuracy: 0.9422\n",
      "Epoch 319/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1274 - accuracy: 0.9514 - val_loss: 0.1548 - val_accuracy: 0.9422\n",
      "Epoch 320/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1273 - accuracy: 0.9514 - val_loss: 0.1563 - val_accuracy: 0.9423\n",
      "Epoch 321/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1273 - accuracy: 0.9515 - val_loss: 0.1554 - val_accuracy: 0.9424\n",
      "Epoch 322/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1271 - accuracy: 0.9515 - val_loss: 0.1557 - val_accuracy: 0.9423\n",
      "Epoch 323/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1271 - accuracy: 0.9513 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 324/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1271 - accuracy: 0.9516 - val_loss: 0.1550 - val_accuracy: 0.9424\n",
      "Epoch 325/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1269 - accuracy: 0.9517 - val_loss: 0.1550 - val_accuracy: 0.9421\n",
      "Epoch 326/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1269 - accuracy: 0.9517 - val_loss: 0.1549 - val_accuracy: 0.9425\n",
      "Epoch 327/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1268 - accuracy: 0.9517 - val_loss: 0.1553 - val_accuracy: 0.9423\n",
      "Epoch 328/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1267 - accuracy: 0.9516 - val_loss: 0.1548 - val_accuracy: 0.9424\n",
      "Epoch 329/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1266 - accuracy: 0.9516 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 330/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1265 - accuracy: 0.9517 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
      "Epoch 331/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1264 - accuracy: 0.9517 - val_loss: 0.1548 - val_accuracy: 0.9426\n",
      "Epoch 332/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1263 - accuracy: 0.9519 - val_loss: 0.1556 - val_accuracy: 0.9422\n",
      "Epoch 333/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1263 - accuracy: 0.9518 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
      "Epoch 334/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1263 - accuracy: 0.9521 - val_loss: 0.1550 - val_accuracy: 0.9418\n",
      "Epoch 335/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1261 - accuracy: 0.9517 - val_loss: 0.1549 - val_accuracy: 0.9429\n",
      "Epoch 336/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1260 - accuracy: 0.9519 - val_loss: 0.1546 - val_accuracy: 0.9421\n",
      "Epoch 337/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1259 - accuracy: 0.9518 - val_loss: 0.1552 - val_accuracy: 0.9423\n",
      "Epoch 338/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1259 - accuracy: 0.9519 - val_loss: 0.1542 - val_accuracy: 0.9425\n",
      "Epoch 339/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1258 - accuracy: 0.9519 - val_loss: 0.1551 - val_accuracy: 0.9426\n",
      "Epoch 340/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1256 - accuracy: 0.9522 - val_loss: 0.1546 - val_accuracy: 0.9421\n",
      "Epoch 341/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1255 - accuracy: 0.9520 - val_loss: 0.1549 - val_accuracy: 0.9412\n",
      "Epoch 342/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1256 - accuracy: 0.9520 - val_loss: 0.1546 - val_accuracy: 0.9422\n",
      "Epoch 343/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1255 - accuracy: 0.9521 - val_loss: 0.1544 - val_accuracy: 0.9426\n",
      "Epoch 344/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1254 - accuracy: 0.9520 - val_loss: 0.1549 - val_accuracy: 0.9422\n",
      "Epoch 345/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1254 - accuracy: 0.9520 - val_loss: 0.1549 - val_accuracy: 0.9419\n",
      "Epoch 346/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1253 - accuracy: 0.9520 - val_loss: 0.1549 - val_accuracy: 0.9420\n",
      "Epoch 347/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1251 - accuracy: 0.9522 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 348/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1251 - accuracy: 0.9523 - val_loss: 0.1546 - val_accuracy: 0.9423\n",
      "Epoch 349/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1251 - accuracy: 0.9522 - val_loss: 0.1543 - val_accuracy: 0.9421\n",
      "Epoch 350/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1249 - accuracy: 0.9521 - val_loss: 0.1549 - val_accuracy: 0.9421\n",
      "Epoch 351/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1248 - accuracy: 0.9523 - val_loss: 0.1547 - val_accuracy: 0.9424\n",
      "Epoch 352/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1247 - accuracy: 0.9523 - val_loss: 0.1544 - val_accuracy: 0.9428\n",
      "Epoch 353/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1247 - accuracy: 0.9523 - val_loss: 0.1543 - val_accuracy: 0.9417\n",
      "Epoch 354/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1246 - accuracy: 0.9522 - val_loss: 0.1547 - val_accuracy: 0.9420\n",
      "Epoch 355/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1246 - accuracy: 0.9524 - val_loss: 0.1542 - val_accuracy: 0.9425\n",
      "Epoch 356/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1244 - accuracy: 0.9523 - val_loss: 0.1539 - val_accuracy: 0.9420\n",
      "Epoch 357/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1244 - accuracy: 0.9527 - val_loss: 0.1546 - val_accuracy: 0.9424\n",
      "Epoch 358/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1243 - accuracy: 0.9522 - val_loss: 0.1541 - val_accuracy: 0.9423\n",
      "Epoch 359/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1242 - accuracy: 0.9525 - val_loss: 0.1540 - val_accuracy: 0.9423\n",
      "Epoch 360/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1241 - accuracy: 0.9526 - val_loss: 0.1541 - val_accuracy: 0.9425\n",
      "Epoch 361/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1241 - accuracy: 0.9524 - val_loss: 0.1547 - val_accuracy: 0.9422\n",
      "Epoch 362/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1241 - accuracy: 0.9526 - val_loss: 0.1539 - val_accuracy: 0.9426\n",
      "Epoch 363/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1240 - accuracy: 0.9524 - val_loss: 0.1539 - val_accuracy: 0.9420\n",
      "Epoch 364/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1239 - accuracy: 0.9525 - val_loss: 0.1542 - val_accuracy: 0.9425\n",
      "Epoch 365/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1239 - accuracy: 0.9526 - val_loss: 0.1547 - val_accuracy: 0.9422\n",
      "Epoch 366/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1238 - accuracy: 0.9525 - val_loss: 0.1544 - val_accuracy: 0.9424\n",
      "Epoch 367/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1236 - accuracy: 0.9525 - val_loss: 0.1541 - val_accuracy: 0.9426\n",
      "Epoch 368/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1235 - accuracy: 0.9526 - val_loss: 0.1535 - val_accuracy: 0.9419\n",
      "Epoch 369/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1235 - accuracy: 0.9528 - val_loss: 0.1539 - val_accuracy: 0.9424\n",
      "Epoch 370/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1234 - accuracy: 0.9528 - val_loss: 0.1544 - val_accuracy: 0.9429\n",
      "Epoch 371/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1234 - accuracy: 0.9527 - val_loss: 0.1540 - val_accuracy: 0.9425\n",
      "Epoch 372/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1232 - accuracy: 0.9529 - val_loss: 0.1542 - val_accuracy: 0.9422\n",
      "Epoch 373/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1231 - accuracy: 0.9527 - val_loss: 0.1543 - val_accuracy: 0.9424\n",
      "Epoch 374/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1231 - accuracy: 0.9528 - val_loss: 0.1541 - val_accuracy: 0.9422\n",
      "Epoch 375/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1230 - accuracy: 0.9529 - val_loss: 0.1540 - val_accuracy: 0.9426\n",
      "Epoch 376/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1229 - accuracy: 0.9526 - val_loss: 0.1541 - val_accuracy: 0.9423\n",
      "Epoch 377/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1228 - accuracy: 0.9528 - val_loss: 0.1544 - val_accuracy: 0.9423\n",
      "Epoch 378/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1228 - accuracy: 0.9526 - val_loss: 0.1541 - val_accuracy: 0.9423\n",
      "Epoch 379/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1227 - accuracy: 0.9529 - val_loss: 0.1542 - val_accuracy: 0.9422\n",
      "Epoch 380/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1226 - accuracy: 0.9530 - val_loss: 0.1544 - val_accuracy: 0.9423\n",
      "Epoch 381/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1226 - accuracy: 0.9530 - val_loss: 0.1539 - val_accuracy: 0.9420\n",
      "Epoch 382/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1225 - accuracy: 0.9531 - val_loss: 0.1542 - val_accuracy: 0.9423\n",
      "Epoch 383/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1224 - accuracy: 0.9531 - val_loss: 0.1538 - val_accuracy: 0.9421\n",
      "Epoch 384/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1223 - accuracy: 0.9531 - val_loss: 0.1538 - val_accuracy: 0.9424\n",
      "Epoch 385/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1223 - accuracy: 0.9529 - val_loss: 0.1538 - val_accuracy: 0.9425\n",
      "Epoch 386/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1222 - accuracy: 0.9531 - val_loss: 0.1539 - val_accuracy: 0.9413\n",
      "Epoch 387/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1221 - accuracy: 0.9530 - val_loss: 0.1544 - val_accuracy: 0.9425\n",
      "Epoch 388/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1221 - accuracy: 0.9530 - val_loss: 0.1543 - val_accuracy: 0.9422\n",
      "Epoch 389/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1220 - accuracy: 0.9530 - val_loss: 0.1540 - val_accuracy: 0.9418\n",
      "Epoch 390/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1218 - accuracy: 0.9532 - val_loss: 0.1544 - val_accuracy: 0.9419\n",
      "Epoch 391/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1217 - accuracy: 0.9533 - val_loss: 0.1543 - val_accuracy: 0.9422\n",
      "Epoch 392/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1217 - accuracy: 0.9532 - val_loss: 0.1538 - val_accuracy: 0.9424\n",
      "Epoch 393/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1216 - accuracy: 0.9533 - val_loss: 0.1544 - val_accuracy: 0.9417\n",
      "Epoch 394/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1216 - accuracy: 0.9534 - val_loss: 0.1549 - val_accuracy: 0.9419\n",
      "Epoch 395/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1216 - accuracy: 0.9534 - val_loss: 0.1538 - val_accuracy: 0.9427\n",
      "Epoch 396/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1214 - accuracy: 0.9533 - val_loss: 0.1541 - val_accuracy: 0.9420\n",
      "Epoch 397/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1214 - accuracy: 0.9532 - val_loss: 0.1539 - val_accuracy: 0.9425\n",
      "Epoch 398/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1213 - accuracy: 0.9536 - val_loss: 0.1542 - val_accuracy: 0.9421\n",
      "Epoch 399/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1213 - accuracy: 0.9534 - val_loss: 0.1537 - val_accuracy: 0.9418\n",
      "Epoch 400/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1212 - accuracy: 0.9534 - val_loss: 0.1545 - val_accuracy: 0.9419\n",
      "Epoch 401/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1211 - accuracy: 0.9533 - val_loss: 0.1543 - val_accuracy: 0.9422\n",
      "Epoch 402/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1210 - accuracy: 0.9534 - val_loss: 0.1554 - val_accuracy: 0.9422\n",
      "Epoch 403/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1209 - accuracy: 0.9535 - val_loss: 0.1540 - val_accuracy: 0.9427\n",
      "Epoch 404/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1209 - accuracy: 0.9534 - val_loss: 0.1535 - val_accuracy: 0.9425\n",
      "Epoch 405/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1207 - accuracy: 0.9535 - val_loss: 0.1539 - val_accuracy: 0.9421\n",
      "Epoch 406/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1206 - accuracy: 0.9537 - val_loss: 0.1549 - val_accuracy: 0.9417\n",
      "Epoch 407/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1207 - accuracy: 0.9535 - val_loss: 0.1540 - val_accuracy: 0.9423\n",
      "Epoch 408/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1205 - accuracy: 0.9535 - val_loss: 0.1542 - val_accuracy: 0.9424\n",
      "Epoch 409/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1205 - accuracy: 0.9535 - val_loss: 0.1539 - val_accuracy: 0.9430\n",
      "Epoch 410/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1204 - accuracy: 0.9535 - val_loss: 0.1540 - val_accuracy: 0.9424\n",
      "Epoch 411/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1203 - accuracy: 0.9539 - val_loss: 0.1543 - val_accuracy: 0.9415\n",
      "Epoch 412/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1203 - accuracy: 0.9536 - val_loss: 0.1538 - val_accuracy: 0.9426\n",
      "Epoch 413/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1202 - accuracy: 0.9539 - val_loss: 0.1541 - val_accuracy: 0.9416\n",
      "Epoch 414/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1201 - accuracy: 0.9537 - val_loss: 0.1536 - val_accuracy: 0.9418\n",
      "Epoch 415/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1201 - accuracy: 0.9536 - val_loss: 0.1542 - val_accuracy: 0.9421\n",
      "Epoch 416/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1201 - accuracy: 0.9536 - val_loss: 0.1539 - val_accuracy: 0.9418\n",
      "Epoch 417/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1199 - accuracy: 0.9538 - val_loss: 0.1539 - val_accuracy: 0.9421\n",
      "Epoch 418/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1198 - accuracy: 0.9538 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 419/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1198 - accuracy: 0.9538 - val_loss: 0.1543 - val_accuracy: 0.9423\n",
      "Epoch 420/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1198 - accuracy: 0.9538 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 421/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1196 - accuracy: 0.9539 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 422/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1196 - accuracy: 0.9540 - val_loss: 0.1539 - val_accuracy: 0.9420\n",
      "Epoch 423/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1195 - accuracy: 0.9540 - val_loss: 0.1537 - val_accuracy: 0.9422\n",
      "Epoch 424/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1194 - accuracy: 0.9539 - val_loss: 0.1544 - val_accuracy: 0.9420\n",
      "Epoch 425/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1193 - accuracy: 0.9540 - val_loss: 0.1541 - val_accuracy: 0.9427\n",
      "Epoch 426/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1193 - accuracy: 0.9539 - val_loss: 0.1545 - val_accuracy: 0.9422\n",
      "Epoch 427/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1192 - accuracy: 0.9542 - val_loss: 0.1534 - val_accuracy: 0.9418\n",
      "Epoch 428/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1190 - accuracy: 0.9540 - val_loss: 0.1538 - val_accuracy: 0.9423\n",
      "Epoch 429/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1190 - accuracy: 0.9539 - val_loss: 0.1538 - val_accuracy: 0.9418\n",
      "Epoch 430/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1190 - accuracy: 0.9542 - val_loss: 0.1546 - val_accuracy: 0.9419\n",
      "Epoch 431/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1188 - accuracy: 0.9542 - val_loss: 0.1551 - val_accuracy: 0.9419\n",
      "Epoch 432/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1188 - accuracy: 0.9541 - val_loss: 0.1544 - val_accuracy: 0.9418\n",
      "Epoch 433/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1188 - accuracy: 0.9540 - val_loss: 0.1537 - val_accuracy: 0.9416\n",
      "Epoch 434/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1188 - accuracy: 0.9540 - val_loss: 0.1541 - val_accuracy: 0.9418\n",
      "Epoch 435/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1187 - accuracy: 0.9539 - val_loss: 0.1543 - val_accuracy: 0.9418\n",
      "Epoch 436/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1186 - accuracy: 0.9541 - val_loss: 0.1540 - val_accuracy: 0.9421\n",
      "Epoch 437/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1186 - accuracy: 0.9541 - val_loss: 0.1539 - val_accuracy: 0.9421\n",
      "Epoch 438/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1185 - accuracy: 0.9541 - val_loss: 0.1535 - val_accuracy: 0.9417\n",
      "Epoch 439/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1183 - accuracy: 0.9542 - val_loss: 0.1547 - val_accuracy: 0.9419\n",
      "Epoch 440/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1183 - accuracy: 0.9545 - val_loss: 0.1537 - val_accuracy: 0.9419\n",
      "Epoch 441/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1183 - accuracy: 0.9545 - val_loss: 0.1543 - val_accuracy: 0.9419\n",
      "Epoch 442/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1181 - accuracy: 0.9543 - val_loss: 0.1542 - val_accuracy: 0.9422\n",
      "Epoch 443/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1181 - accuracy: 0.9543 - val_loss: 0.1543 - val_accuracy: 0.9420\n",
      "Epoch 444/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1180 - accuracy: 0.9544 - val_loss: 0.1547 - val_accuracy: 0.9423\n",
      "Epoch 445/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1179 - accuracy: 0.9543 - val_loss: 0.1545 - val_accuracy: 0.9419\n",
      "Epoch 446/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1179 - accuracy: 0.9544 - val_loss: 0.1549 - val_accuracy: 0.9417\n",
      "Epoch 447/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1179 - accuracy: 0.9543 - val_loss: 0.1542 - val_accuracy: 0.9422\n",
      "Epoch 448/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1177 - accuracy: 0.9544 - val_loss: 0.1540 - val_accuracy: 0.9418\n",
      "Epoch 449/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1177 - accuracy: 0.9545 - val_loss: 0.1543 - val_accuracy: 0.9417\n",
      "Epoch 450/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1176 - accuracy: 0.9546 - val_loss: 0.1551 - val_accuracy: 0.9409\n",
      "Epoch 451/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1176 - accuracy: 0.9546 - val_loss: 0.1547 - val_accuracy: 0.9420\n",
      "Epoch 452/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1175 - accuracy: 0.9546 - val_loss: 0.1537 - val_accuracy: 0.9417\n",
      "Epoch 453/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1174 - accuracy: 0.9545 - val_loss: 0.1547 - val_accuracy: 0.9421\n",
      "Epoch 454/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1173 - accuracy: 0.9544 - val_loss: 0.1543 - val_accuracy: 0.9415\n",
      "Epoch 455/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1173 - accuracy: 0.9546 - val_loss: 0.1540 - val_accuracy: 0.9411\n",
      "Epoch 456/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1171 - accuracy: 0.9548 - val_loss: 0.1546 - val_accuracy: 0.9424\n",
      "Epoch 457/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1171 - accuracy: 0.9544 - val_loss: 0.1552 - val_accuracy: 0.9414\n",
      "Epoch 458/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1170 - accuracy: 0.9545 - val_loss: 0.1543 - val_accuracy: 0.9419\n",
      "Epoch 459/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1170 - accuracy: 0.9547 - val_loss: 0.1547 - val_accuracy: 0.9420\n",
      "Epoch 460/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1168 - accuracy: 0.9546 - val_loss: 0.1543 - val_accuracy: 0.9418\n",
      "Epoch 461/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1169 - accuracy: 0.9547 - val_loss: 0.1548 - val_accuracy: 0.9418\n",
      "Epoch 462/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1168 - accuracy: 0.9547 - val_loss: 0.1553 - val_accuracy: 0.9411\n",
      "Epoch 463/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1167 - accuracy: 0.9548 - val_loss: 0.1544 - val_accuracy: 0.9419\n",
      "Epoch 464/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1166 - accuracy: 0.9548 - val_loss: 0.1538 - val_accuracy: 0.9419\n",
      "Epoch 465/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1164 - accuracy: 0.9549 - val_loss: 0.1549 - val_accuracy: 0.9423\n",
      "Epoch 466/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1164 - accuracy: 0.9549 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
      "Epoch 467/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1164 - accuracy: 0.9548 - val_loss: 0.1545 - val_accuracy: 0.9419\n",
      "Epoch 468/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1163 - accuracy: 0.9548 - val_loss: 0.1543 - val_accuracy: 0.9414\n",
      "Epoch 469/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1163 - accuracy: 0.9551 - val_loss: 0.1538 - val_accuracy: 0.9424\n",
      "Epoch 470/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1161 - accuracy: 0.9548 - val_loss: 0.1546 - val_accuracy: 0.9413\n",
      "Epoch 471/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1161 - accuracy: 0.9550 - val_loss: 0.1544 - val_accuracy: 0.9417\n",
      "Epoch 472/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1160 - accuracy: 0.9549 - val_loss: 0.1557 - val_accuracy: 0.9412\n",
      "Epoch 473/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1160 - accuracy: 0.9550 - val_loss: 0.1547 - val_accuracy: 0.9421\n",
      "Epoch 474/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1159 - accuracy: 0.9549 - val_loss: 0.1547 - val_accuracy: 0.9420\n",
      "Epoch 475/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1158 - accuracy: 0.9551 - val_loss: 0.1552 - val_accuracy: 0.9420\n",
      "Epoch 476/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1159 - accuracy: 0.9550 - val_loss: 0.1540 - val_accuracy: 0.9419\n",
      "Epoch 477/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1157 - accuracy: 0.9549 - val_loss: 0.1554 - val_accuracy: 0.9417\n",
      "Epoch 478/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1156 - accuracy: 0.9553 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 479/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1155 - accuracy: 0.9552 - val_loss: 0.1547 - val_accuracy: 0.9420\n",
      "Epoch 480/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1155 - accuracy: 0.9553 - val_loss: 0.1554 - val_accuracy: 0.9415\n",
      "Epoch 481/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1154 - accuracy: 0.9552 - val_loss: 0.1559 - val_accuracy: 0.9408\n",
      "Epoch 482/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1153 - accuracy: 0.9551 - val_loss: 0.1549 - val_accuracy: 0.9419\n",
      "Epoch 483/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1153 - accuracy: 0.9552 - val_loss: 0.1552 - val_accuracy: 0.9414\n",
      "Epoch 484/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1152 - accuracy: 0.9551 - val_loss: 0.1549 - val_accuracy: 0.9421\n",
      "Epoch 485/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1150 - accuracy: 0.9553 - val_loss: 0.1547 - val_accuracy: 0.9415\n",
      "Epoch 486/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1152 - accuracy: 0.9553 - val_loss: 0.1545 - val_accuracy: 0.9412\n",
      "Epoch 487/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1150 - accuracy: 0.9554 - val_loss: 0.1565 - val_accuracy: 0.9408\n",
      "Epoch 488/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1150 - accuracy: 0.9554 - val_loss: 0.1552 - val_accuracy: 0.9414\n",
      "Epoch 489/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1149 - accuracy: 0.9551 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 490/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1148 - accuracy: 0.9555 - val_loss: 0.1553 - val_accuracy: 0.9411\n",
      "Epoch 491/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1148 - accuracy: 0.9553 - val_loss: 0.1554 - val_accuracy: 0.9413\n",
      "Epoch 492/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1147 - accuracy: 0.9555 - val_loss: 0.1550 - val_accuracy: 0.9409\n",
      "Epoch 493/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1147 - accuracy: 0.9554 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 494/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1146 - accuracy: 0.9556 - val_loss: 0.1548 - val_accuracy: 0.9419\n",
      "Epoch 495/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1144 - accuracy: 0.9558 - val_loss: 0.1563 - val_accuracy: 0.9411\n",
      "Epoch 496/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1145 - accuracy: 0.9556 - val_loss: 0.1552 - val_accuracy: 0.9420\n",
      "Epoch 497/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1143 - accuracy: 0.9556 - val_loss: 0.1550 - val_accuracy: 0.9423\n",
      "Epoch 498/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1143 - accuracy: 0.9556 - val_loss: 0.1551 - val_accuracy: 0.9411\n",
      "Epoch 499/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1142 - accuracy: 0.9556 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
      "Epoch 500/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1141 - accuracy: 0.9555 - val_loss: 0.1547 - val_accuracy: 0.9418\n",
      "Epoch 501/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1142 - accuracy: 0.9555 - val_loss: 0.1551 - val_accuracy: 0.9420\n",
      "Epoch 502/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1139 - accuracy: 0.9557 - val_loss: 0.1554 - val_accuracy: 0.9410\n",
      "Epoch 503/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1139 - accuracy: 0.9557 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 504/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1138 - accuracy: 0.9557 - val_loss: 0.1557 - val_accuracy: 0.9413\n",
      "Epoch 505/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1138 - accuracy: 0.9557 - val_loss: 0.1556 - val_accuracy: 0.9421\n",
      "Epoch 506/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1138 - accuracy: 0.9559 - val_loss: 0.1553 - val_accuracy: 0.9417\n",
      "Epoch 507/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1136 - accuracy: 0.9558 - val_loss: 0.1557 - val_accuracy: 0.9420\n",
      "Epoch 508/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1136 - accuracy: 0.9557 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 509/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1136 - accuracy: 0.9557 - val_loss: 0.1554 - val_accuracy: 0.9417\n",
      "Epoch 510/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1134 - accuracy: 0.9559 - val_loss: 0.1556 - val_accuracy: 0.9412\n",
      "Epoch 511/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1133 - accuracy: 0.9559 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 512/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1134 - accuracy: 0.9559 - val_loss: 0.1557 - val_accuracy: 0.9413\n",
      "Epoch 513/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1133 - accuracy: 0.9562 - val_loss: 0.1559 - val_accuracy: 0.9411\n",
      "Epoch 514/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1132 - accuracy: 0.9561 - val_loss: 0.1553 - val_accuracy: 0.9417\n",
      "Epoch 515/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1131 - accuracy: 0.9560 - val_loss: 0.1559 - val_accuracy: 0.9413\n",
      "Epoch 516/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1131 - accuracy: 0.9560 - val_loss: 0.1547 - val_accuracy: 0.9413\n",
      "Epoch 517/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1129 - accuracy: 0.9563 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
      "Epoch 518/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1129 - accuracy: 0.9560 - val_loss: 0.1567 - val_accuracy: 0.9415\n",
      "Epoch 519/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1129 - accuracy: 0.9560 - val_loss: 0.1556 - val_accuracy: 0.9414\n",
      "Epoch 520/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1128 - accuracy: 0.9560 - val_loss: 0.1555 - val_accuracy: 0.9413\n",
      "Epoch 521/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1127 - accuracy: 0.9560 - val_loss: 0.1560 - val_accuracy: 0.9410\n",
      "Epoch 522/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1126 - accuracy: 0.9561 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 523/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1126 - accuracy: 0.9560 - val_loss: 0.1570 - val_accuracy: 0.9419\n",
      "Epoch 524/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1124 - accuracy: 0.9562 - val_loss: 0.1557 - val_accuracy: 0.9416\n",
      "Epoch 525/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1124 - accuracy: 0.9562 - val_loss: 0.1562 - val_accuracy: 0.9412\n",
      "Epoch 526/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1125 - accuracy: 0.9561 - val_loss: 0.1569 - val_accuracy: 0.9413\n",
      "Epoch 527/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1123 - accuracy: 0.9563 - val_loss: 0.1557 - val_accuracy: 0.9415\n",
      "Epoch 528/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1123 - accuracy: 0.9563 - val_loss: 0.1554 - val_accuracy: 0.9414\n",
      "Epoch 529/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1121 - accuracy: 0.9564 - val_loss: 0.1554 - val_accuracy: 0.9416\n",
      "Epoch 530/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1120 - accuracy: 0.9562 - val_loss: 0.1566 - val_accuracy: 0.9418\n",
      "Epoch 531/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1120 - accuracy: 0.9565 - val_loss: 0.1563 - val_accuracy: 0.9413\n",
      "Epoch 532/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1120 - accuracy: 0.9565 - val_loss: 0.1566 - val_accuracy: 0.9411\n",
      "Epoch 533/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1118 - accuracy: 0.9567 - val_loss: 0.1559 - val_accuracy: 0.9413\n",
      "Epoch 534/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1119 - accuracy: 0.9565 - val_loss: 0.1560 - val_accuracy: 0.9415\n",
      "Epoch 535/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1117 - accuracy: 0.9566 - val_loss: 0.1566 - val_accuracy: 0.9415\n",
      "Epoch 536/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1117 - accuracy: 0.9568 - val_loss: 0.1565 - val_accuracy: 0.9410\n",
      "Epoch 537/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1117 - accuracy: 0.9564 - val_loss: 0.1562 - val_accuracy: 0.9413\n",
      "Epoch 538/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1116 - accuracy: 0.9566 - val_loss: 0.1582 - val_accuracy: 0.9406\n",
      "Epoch 539/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1115 - accuracy: 0.9565 - val_loss: 0.1578 - val_accuracy: 0.9410\n",
      "Epoch 540/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1114 - accuracy: 0.9564 - val_loss: 0.1565 - val_accuracy: 0.9412\n",
      "Epoch 541/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1113 - accuracy: 0.9568 - val_loss: 0.1565 - val_accuracy: 0.9412\n",
      "Epoch 542/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1113 - accuracy: 0.9568 - val_loss: 0.1570 - val_accuracy: 0.9410\n",
      "Epoch 543/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1112 - accuracy: 0.9565 - val_loss: 0.1563 - val_accuracy: 0.9408\n",
      "Epoch 544/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1112 - accuracy: 0.9567 - val_loss: 0.1560 - val_accuracy: 0.9409\n",
      "Epoch 545/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1110 - accuracy: 0.9568 - val_loss: 0.1567 - val_accuracy: 0.9409\n",
      "Epoch 546/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1111 - accuracy: 0.9568 - val_loss: 0.1563 - val_accuracy: 0.9409\n",
      "Epoch 547/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1110 - accuracy: 0.9568 - val_loss: 0.1561 - val_accuracy: 0.9418\n",
      "Epoch 548/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1108 - accuracy: 0.9568 - val_loss: 0.1561 - val_accuracy: 0.9417\n",
      "Epoch 549/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1108 - accuracy: 0.9569 - val_loss: 0.1580 - val_accuracy: 0.9408\n",
      "Epoch 550/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1108 - accuracy: 0.9568 - val_loss: 0.1567 - val_accuracy: 0.9416\n",
      "Epoch 551/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1108 - accuracy: 0.9568 - val_loss: 0.1569 - val_accuracy: 0.9409\n",
      "Epoch 552/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1106 - accuracy: 0.9568 - val_loss: 0.1575 - val_accuracy: 0.9409\n",
      "Epoch 553/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1106 - accuracy: 0.9567 - val_loss: 0.1575 - val_accuracy: 0.9413\n",
      "Epoch 554/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1106 - accuracy: 0.9569 - val_loss: 0.1567 - val_accuracy: 0.9414\n",
      "Epoch 555/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1105 - accuracy: 0.9569 - val_loss: 0.1573 - val_accuracy: 0.9409\n",
      "Epoch 556/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1105 - accuracy: 0.9569 - val_loss: 0.1572 - val_accuracy: 0.9415\n",
      "Epoch 557/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1103 - accuracy: 0.9572 - val_loss: 0.1574 - val_accuracy: 0.9416\n",
      "Epoch 558/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1102 - accuracy: 0.9569 - val_loss: 0.1568 - val_accuracy: 0.9414\n",
      "Epoch 559/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1101 - accuracy: 0.9571 - val_loss: 0.1568 - val_accuracy: 0.9415\n",
      "Epoch 560/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1101 - accuracy: 0.9570 - val_loss: 0.1565 - val_accuracy: 0.9410\n",
      "Epoch 561/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1099 - accuracy: 0.9571 - val_loss: 0.1567 - val_accuracy: 0.9407\n",
      "Epoch 562/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1100 - accuracy: 0.9571 - val_loss: 0.1574 - val_accuracy: 0.9411\n",
      "Epoch 563/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1100 - accuracy: 0.9573 - val_loss: 0.1570 - val_accuracy: 0.9411\n",
      "Epoch 564/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1098 - accuracy: 0.9570 - val_loss: 0.1574 - val_accuracy: 0.9413\n",
      "Epoch 565/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1097 - accuracy: 0.9572 - val_loss: 0.1569 - val_accuracy: 0.9407\n",
      "Epoch 566/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1097 - accuracy: 0.9572 - val_loss: 0.1575 - val_accuracy: 0.9409\n",
      "Epoch 567/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1096 - accuracy: 0.9573 - val_loss: 0.1573 - val_accuracy: 0.9411\n",
      "Epoch 568/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1095 - accuracy: 0.9571 - val_loss: 0.1566 - val_accuracy: 0.9412\n",
      "Epoch 569/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1096 - accuracy: 0.9573 - val_loss: 0.1575 - val_accuracy: 0.9412\n",
      "Epoch 570/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1094 - accuracy: 0.9571 - val_loss: 0.1580 - val_accuracy: 0.9407\n",
      "Epoch 571/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1094 - accuracy: 0.9573 - val_loss: 0.1577 - val_accuracy: 0.9411\n",
      "Epoch 572/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1093 - accuracy: 0.9575 - val_loss: 0.1581 - val_accuracy: 0.9414\n",
      "Epoch 573/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1092 - accuracy: 0.9573 - val_loss: 0.1575 - val_accuracy: 0.9413\n",
      "Epoch 574/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1092 - accuracy: 0.9574 - val_loss: 0.1575 - val_accuracy: 0.9413\n",
      "Epoch 575/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1090 - accuracy: 0.9575 - val_loss: 0.1575 - val_accuracy: 0.9411\n",
      "Epoch 576/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1089 - accuracy: 0.9577 - val_loss: 0.1580 - val_accuracy: 0.9416\n",
      "Epoch 577/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1090 - accuracy: 0.9574 - val_loss: 0.1581 - val_accuracy: 0.9404\n",
      "Epoch 578/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1089 - accuracy: 0.9574 - val_loss: 0.1575 - val_accuracy: 0.9415\n",
      "Epoch 579/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1088 - accuracy: 0.9574 - val_loss: 0.1592 - val_accuracy: 0.9408\n",
      "Epoch 580/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1088 - accuracy: 0.9577 - val_loss: 0.1587 - val_accuracy: 0.9410\n",
      "Epoch 581/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1087 - accuracy: 0.9576 - val_loss: 0.1578 - val_accuracy: 0.9412\n",
      "Epoch 582/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1086 - accuracy: 0.9576 - val_loss: 0.1573 - val_accuracy: 0.9405\n",
      "Epoch 583/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1085 - accuracy: 0.9577 - val_loss: 0.1586 - val_accuracy: 0.9410\n",
      "Epoch 584/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1085 - accuracy: 0.9578 - val_loss: 0.1584 - val_accuracy: 0.9404\n",
      "Epoch 585/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1083 - accuracy: 0.9578 - val_loss: 0.1576 - val_accuracy: 0.9407\n",
      "Epoch 586/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1084 - accuracy: 0.9577 - val_loss: 0.1582 - val_accuracy: 0.9420\n",
      "Epoch 587/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1082 - accuracy: 0.9580 - val_loss: 0.1581 - val_accuracy: 0.9411\n",
      "Epoch 588/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1083 - accuracy: 0.9577 - val_loss: 0.1577 - val_accuracy: 0.9408\n",
      "Epoch 589/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1081 - accuracy: 0.9581 - val_loss: 0.1582 - val_accuracy: 0.9412\n",
      "Epoch 590/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1080 - accuracy: 0.9578 - val_loss: 0.1590 - val_accuracy: 0.9406\n",
      "Epoch 591/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1081 - accuracy: 0.9580 - val_loss: 0.1595 - val_accuracy: 0.9405\n",
      "Epoch 592/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1080 - accuracy: 0.9579 - val_loss: 0.1588 - val_accuracy: 0.9410\n",
      "Epoch 593/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1078 - accuracy: 0.9581 - val_loss: 0.1580 - val_accuracy: 0.9406\n",
      "Epoch 594/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1079 - accuracy: 0.9578 - val_loss: 0.1581 - val_accuracy: 0.9402\n",
      "Epoch 595/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1077 - accuracy: 0.9579 - val_loss: 0.1584 - val_accuracy: 0.9407\n",
      "Epoch 596/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1077 - accuracy: 0.9580 - val_loss: 0.1596 - val_accuracy: 0.9405\n",
      "Epoch 597/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1077 - accuracy: 0.9582 - val_loss: 0.1577 - val_accuracy: 0.9414\n",
      "Epoch 598/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1075 - accuracy: 0.9582 - val_loss: 0.1582 - val_accuracy: 0.9408\n",
      "Epoch 599/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1074 - accuracy: 0.9581 - val_loss: 0.1586 - val_accuracy: 0.9403\n",
      "Epoch 600/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1074 - accuracy: 0.9580 - val_loss: 0.1587 - val_accuracy: 0.9413\n",
      "Epoch 601/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1074 - accuracy: 0.9581 - val_loss: 0.1595 - val_accuracy: 0.9404\n",
      "Epoch 602/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1074 - accuracy: 0.9581 - val_loss: 0.1582 - val_accuracy: 0.9411\n",
      "Epoch 603/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1074 - accuracy: 0.9580 - val_loss: 0.1591 - val_accuracy: 0.9408\n",
      "Epoch 604/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1071 - accuracy: 0.9584 - val_loss: 0.1590 - val_accuracy: 0.9404\n",
      "Epoch 605/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1070 - accuracy: 0.9582 - val_loss: 0.1595 - val_accuracy: 0.9403\n",
      "Epoch 606/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1071 - accuracy: 0.9581 - val_loss: 0.1583 - val_accuracy: 0.9407\n",
      "Epoch 607/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1069 - accuracy: 0.9584 - val_loss: 0.1586 - val_accuracy: 0.9416\n",
      "Epoch 608/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1070 - accuracy: 0.9583 - val_loss: 0.1583 - val_accuracy: 0.9407\n",
      "Epoch 609/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.1592 - val_accuracy: 0.9406\n",
      "Epoch 610/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.1599 - val_accuracy: 0.9408\n",
      "Epoch 611/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.1591 - val_accuracy: 0.9410\n",
      "Epoch 612/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1066 - accuracy: 0.9582 - val_loss: 0.1609 - val_accuracy: 0.9400\n",
      "Epoch 613/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1066 - accuracy: 0.9583 - val_loss: 0.1598 - val_accuracy: 0.9400\n",
      "Epoch 614/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1066 - accuracy: 0.9583 - val_loss: 0.1594 - val_accuracy: 0.9403\n",
      "Epoch 615/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1065 - accuracy: 0.9584 - val_loss: 0.1589 - val_accuracy: 0.9404\n",
      "Epoch 616/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1064 - accuracy: 0.9584 - val_loss: 0.1591 - val_accuracy: 0.9409\n",
      "Epoch 617/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1064 - accuracy: 0.9585 - val_loss: 0.1598 - val_accuracy: 0.9410\n",
      "Epoch 618/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1063 - accuracy: 0.9585 - val_loss: 0.1609 - val_accuracy: 0.9403\n",
      "Epoch 619/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1062 - accuracy: 0.9583 - val_loss: 0.1588 - val_accuracy: 0.9408\n",
      "Epoch 620/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1061 - accuracy: 0.9586 - val_loss: 0.1597 - val_accuracy: 0.9405\n",
      "Epoch 621/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1059 - accuracy: 0.9585 - val_loss: 0.1597 - val_accuracy: 0.9412\n",
      "Epoch 622/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1059 - accuracy: 0.9589 - val_loss: 0.1608 - val_accuracy: 0.9400\n",
      "Epoch 623/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1059 - accuracy: 0.9588 - val_loss: 0.1597 - val_accuracy: 0.9408\n",
      "Epoch 624/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1058 - accuracy: 0.9587 - val_loss: 0.1602 - val_accuracy: 0.9399\n",
      "Epoch 625/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1058 - accuracy: 0.9584 - val_loss: 0.1604 - val_accuracy: 0.9403\n",
      "Epoch 626/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1057 - accuracy: 0.9587 - val_loss: 0.1600 - val_accuracy: 0.9409\n",
      "Epoch 627/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1056 - accuracy: 0.9587 - val_loss: 0.1604 - val_accuracy: 0.9395\n",
      "Epoch 628/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1057 - accuracy: 0.9590 - val_loss: 0.1602 - val_accuracy: 0.9407\n",
      "Epoch 629/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1054 - accuracy: 0.9589 - val_loss: 0.1602 - val_accuracy: 0.9403\n",
      "Epoch 630/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1054 - accuracy: 0.9587 - val_loss: 0.1606 - val_accuracy: 0.9412\n",
      "Epoch 631/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1054 - accuracy: 0.9589 - val_loss: 0.1600 - val_accuracy: 0.9402\n",
      "Epoch 632/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1052 - accuracy: 0.9591 - val_loss: 0.1599 - val_accuracy: 0.9410\n",
      "Epoch 633/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1052 - accuracy: 0.9592 - val_loss: 0.1604 - val_accuracy: 0.9405\n",
      "Epoch 634/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1052 - accuracy: 0.9589 - val_loss: 0.1613 - val_accuracy: 0.9400\n",
      "Epoch 635/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1051 - accuracy: 0.9588 - val_loss: 0.1619 - val_accuracy: 0.9405\n",
      "Epoch 636/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1050 - accuracy: 0.9590 - val_loss: 0.1601 - val_accuracy: 0.9414\n",
      "Epoch 637/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1050 - accuracy: 0.9591 - val_loss: 0.1602 - val_accuracy: 0.9404\n",
      "Epoch 638/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1047 - accuracy: 0.9590 - val_loss: 0.1604 - val_accuracy: 0.9404\n",
      "Epoch 639/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1048 - accuracy: 0.9590 - val_loss: 0.1609 - val_accuracy: 0.9405\n",
      "Epoch 640/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1048 - accuracy: 0.9590 - val_loss: 0.1604 - val_accuracy: 0.9405\n",
      "Epoch 641/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1048 - accuracy: 0.9589 - val_loss: 0.1603 - val_accuracy: 0.9410\n",
      "Epoch 642/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1046 - accuracy: 0.9592 - val_loss: 0.1621 - val_accuracy: 0.9406\n",
      "Epoch 643/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1045 - accuracy: 0.9590 - val_loss: 0.1615 - val_accuracy: 0.9401\n",
      "Epoch 644/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1046 - accuracy: 0.9590 - val_loss: 0.1609 - val_accuracy: 0.9407\n",
      "Epoch 645/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1044 - accuracy: 0.9595 - val_loss: 0.1614 - val_accuracy: 0.9400\n",
      "Epoch 646/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1044 - accuracy: 0.9593 - val_loss: 0.1614 - val_accuracy: 0.9396\n",
      "Epoch 647/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1042 - accuracy: 0.9593 - val_loss: 0.1608 - val_accuracy: 0.9401\n",
      "Epoch 648/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1043 - accuracy: 0.9594 - val_loss: 0.1606 - val_accuracy: 0.9402\n",
      "Epoch 649/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1041 - accuracy: 0.9593 - val_loss: 0.1614 - val_accuracy: 0.9410\n",
      "Epoch 650/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1042 - accuracy: 0.9594 - val_loss: 0.1618 - val_accuracy: 0.9399\n",
      "Epoch 651/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1041 - accuracy: 0.9592 - val_loss: 0.1617 - val_accuracy: 0.9408\n",
      "Epoch 652/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1039 - accuracy: 0.9593 - val_loss: 0.1611 - val_accuracy: 0.9408\n",
      "Epoch 653/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1039 - accuracy: 0.9593 - val_loss: 0.1605 - val_accuracy: 0.9407\n",
      "Epoch 654/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1038 - accuracy: 0.9595 - val_loss: 0.1623 - val_accuracy: 0.9405\n",
      "Epoch 655/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1038 - accuracy: 0.9595 - val_loss: 0.1605 - val_accuracy: 0.9404\n",
      "Epoch 656/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1037 - accuracy: 0.9596 - val_loss: 0.1624 - val_accuracy: 0.9404\n",
      "Epoch 657/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1036 - accuracy: 0.9594 - val_loss: 0.1626 - val_accuracy: 0.9399\n",
      "Epoch 658/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1036 - accuracy: 0.9593 - val_loss: 0.1606 - val_accuracy: 0.9402\n",
      "Epoch 659/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1035 - accuracy: 0.9597 - val_loss: 0.1634 - val_accuracy: 0.9406\n",
      "Epoch 660/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1034 - accuracy: 0.9597 - val_loss: 0.1621 - val_accuracy: 0.9399\n",
      "Epoch 661/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1034 - accuracy: 0.9595 - val_loss: 0.1624 - val_accuracy: 0.9401\n",
      "Epoch 662/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1034 - accuracy: 0.9596 - val_loss: 0.1625 - val_accuracy: 0.9399\n",
      "Epoch 663/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1032 - accuracy: 0.9599 - val_loss: 0.1619 - val_accuracy: 0.9399\n",
      "Epoch 664/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1030 - accuracy: 0.9595 - val_loss: 0.1616 - val_accuracy: 0.9403\n",
      "Epoch 665/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1030 - accuracy: 0.9599 - val_loss: 0.1618 - val_accuracy: 0.9400\n",
      "Epoch 666/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1030 - accuracy: 0.9596 - val_loss: 0.1631 - val_accuracy: 0.9400\n",
      "Epoch 667/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1029 - accuracy: 0.9599 - val_loss: 0.1622 - val_accuracy: 0.9401\n",
      "Epoch 668/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1029 - accuracy: 0.9596 - val_loss: 0.1624 - val_accuracy: 0.9402\n",
      "Epoch 669/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1027 - accuracy: 0.9598 - val_loss: 0.1632 - val_accuracy: 0.9403\n",
      "Epoch 670/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1026 - accuracy: 0.9600 - val_loss: 0.1624 - val_accuracy: 0.9408\n",
      "Epoch 671/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1027 - accuracy: 0.9598 - val_loss: 0.1622 - val_accuracy: 0.9401\n",
      "Epoch 672/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1026 - accuracy: 0.9600 - val_loss: 0.1634 - val_accuracy: 0.9397\n",
      "Epoch 673/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1025 - accuracy: 0.9601 - val_loss: 0.1629 - val_accuracy: 0.9400\n",
      "Epoch 674/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1024 - accuracy: 0.9598 - val_loss: 0.1630 - val_accuracy: 0.9390\n",
      "Epoch 675/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1025 - accuracy: 0.9598 - val_loss: 0.1627 - val_accuracy: 0.9401\n",
      "Epoch 676/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1022 - accuracy: 0.9603 - val_loss: 0.1633 - val_accuracy: 0.9403\n",
      "Epoch 677/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1023 - accuracy: 0.9600 - val_loss: 0.1638 - val_accuracy: 0.9407\n",
      "Epoch 678/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1021 - accuracy: 0.9600 - val_loss: 0.1637 - val_accuracy: 0.9387\n",
      "Epoch 679/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1021 - accuracy: 0.9598 - val_loss: 0.1637 - val_accuracy: 0.9397\n",
      "Epoch 680/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1021 - accuracy: 0.9601 - val_loss: 0.1635 - val_accuracy: 0.9394\n",
      "Epoch 681/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1019 - accuracy: 0.9601 - val_loss: 0.1625 - val_accuracy: 0.9403\n",
      "Epoch 682/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1019 - accuracy: 0.9602 - val_loss: 0.1627 - val_accuracy: 0.9396\n",
      "Epoch 683/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1019 - accuracy: 0.9600 - val_loss: 0.1630 - val_accuracy: 0.9396\n",
      "Epoch 684/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1017 - accuracy: 0.9601 - val_loss: 0.1632 - val_accuracy: 0.9400\n",
      "Epoch 685/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1017 - accuracy: 0.9603 - val_loss: 0.1638 - val_accuracy: 0.9402\n",
      "Epoch 686/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1016 - accuracy: 0.9603 - val_loss: 0.1651 - val_accuracy: 0.9402\n",
      "Epoch 687/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1015 - accuracy: 0.9604 - val_loss: 0.1642 - val_accuracy: 0.9395\n",
      "Epoch 688/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1015 - accuracy: 0.9603 - val_loss: 0.1641 - val_accuracy: 0.9399\n",
      "Epoch 689/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1014 - accuracy: 0.9603 - val_loss: 0.1648 - val_accuracy: 0.9398\n",
      "Epoch 690/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1012 - accuracy: 0.9606 - val_loss: 0.1639 - val_accuracy: 0.9402\n",
      "Epoch 691/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1013 - accuracy: 0.9603 - val_loss: 0.1649 - val_accuracy: 0.9395\n",
      "Epoch 692/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1012 - accuracy: 0.9604 - val_loss: 0.1653 - val_accuracy: 0.9398\n",
      "Epoch 693/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1012 - accuracy: 0.9605 - val_loss: 0.1642 - val_accuracy: 0.9398\n",
      "Epoch 694/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1010 - accuracy: 0.9608 - val_loss: 0.1643 - val_accuracy: 0.9404\n",
      "Epoch 695/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1010 - accuracy: 0.9605 - val_loss: 0.1644 - val_accuracy: 0.9394\n",
      "Epoch 696/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1010 - accuracy: 0.9603 - val_loss: 0.1637 - val_accuracy: 0.9404\n",
      "Epoch 697/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1010 - accuracy: 0.9603 - val_loss: 0.1640 - val_accuracy: 0.9396\n",
      "Epoch 698/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1009 - accuracy: 0.9606 - val_loss: 0.1644 - val_accuracy: 0.9402\n",
      "Epoch 699/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1006 - accuracy: 0.9607 - val_loss: 0.1641 - val_accuracy: 0.9396\n",
      "Epoch 700/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1007 - accuracy: 0.9605 - val_loss: 0.1643 - val_accuracy: 0.9403\n",
      "Epoch 701/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1006 - accuracy: 0.9608 - val_loss: 0.1651 - val_accuracy: 0.9393\n",
      "Epoch 702/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1004 - accuracy: 0.9603 - val_loss: 0.1659 - val_accuracy: 0.9396\n",
      "Epoch 703/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1004 - accuracy: 0.9610 - val_loss: 0.1647 - val_accuracy: 0.9400\n",
      "Epoch 704/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1004 - accuracy: 0.9606 - val_loss: 0.1657 - val_accuracy: 0.9404\n",
      "Epoch 705/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1003 - accuracy: 0.9609 - val_loss: 0.1647 - val_accuracy: 0.9396\n",
      "Epoch 706/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1002 - accuracy: 0.9605 - val_loss: 0.1655 - val_accuracy: 0.9396\n",
      "Epoch 707/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1684 - val_accuracy: 0.9384\n",
      "Epoch 708/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1001 - accuracy: 0.9609 - val_loss: 0.1648 - val_accuracy: 0.9395\n",
      "Epoch 709/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1001 - accuracy: 0.9607 - val_loss: 0.1652 - val_accuracy: 0.9392\n",
      "Epoch 710/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0998 - accuracy: 0.9613 - val_loss: 0.1648 - val_accuracy: 0.9394\n",
      "Epoch 711/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0998 - accuracy: 0.9611 - val_loss: 0.1659 - val_accuracy: 0.9393\n",
      "Epoch 712/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0998 - accuracy: 0.9607 - val_loss: 0.1649 - val_accuracy: 0.9399\n",
      "Epoch 713/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0997 - accuracy: 0.9609 - val_loss: 0.1670 - val_accuracy: 0.9382\n",
      "Epoch 714/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0997 - accuracy: 0.9610 - val_loss: 0.1659 - val_accuracy: 0.9396\n",
      "Epoch 715/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0997 - accuracy: 0.9611 - val_loss: 0.1651 - val_accuracy: 0.9404\n",
      "Epoch 716/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0994 - accuracy: 0.9613 - val_loss: 0.1651 - val_accuracy: 0.9399\n",
      "Epoch 717/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0994 - accuracy: 0.9611 - val_loss: 0.1655 - val_accuracy: 0.9399\n",
      "Epoch 718/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0993 - accuracy: 0.9612 - val_loss: 0.1659 - val_accuracy: 0.9400\n",
      "Epoch 719/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0993 - accuracy: 0.9612 - val_loss: 0.1660 - val_accuracy: 0.9398\n",
      "Epoch 720/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0992 - accuracy: 0.9614 - val_loss: 0.1664 - val_accuracy: 0.9396\n",
      "Epoch 721/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0991 - accuracy: 0.9612 - val_loss: 0.1680 - val_accuracy: 0.9389\n",
      "Epoch 722/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0990 - accuracy: 0.9613 - val_loss: 0.1672 - val_accuracy: 0.9400\n",
      "Epoch 723/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0988 - accuracy: 0.9614 - val_loss: 0.1692 - val_accuracy: 0.9394\n",
      "Epoch 724/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0991 - accuracy: 0.9613 - val_loss: 0.1657 - val_accuracy: 0.9396\n",
      "Epoch 725/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0989 - accuracy: 0.9613 - val_loss: 0.1658 - val_accuracy: 0.9395\n",
      "Epoch 726/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0988 - accuracy: 0.9612 - val_loss: 0.1663 - val_accuracy: 0.9399\n",
      "Epoch 727/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0988 - accuracy: 0.9612 - val_loss: 0.1666 - val_accuracy: 0.9396\n",
      "Epoch 728/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0986 - accuracy: 0.9614 - val_loss: 0.1655 - val_accuracy: 0.9400\n",
      "Epoch 729/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0987 - accuracy: 0.9614 - val_loss: 0.1672 - val_accuracy: 0.9391\n",
      "Epoch 730/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0986 - accuracy: 0.9612 - val_loss: 0.1674 - val_accuracy: 0.9398\n",
      "Epoch 731/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.96 - 0s 72us/step - loss: 0.0984 - accuracy: 0.9615 - val_loss: 0.1675 - val_accuracy: 0.9393\n",
      "Epoch 732/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1677 - val_accuracy: 0.9396\n",
      "Epoch 733/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1659 - val_accuracy: 0.9395\n",
      "Epoch 734/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0982 - accuracy: 0.9616 - val_loss: 0.1672 - val_accuracy: 0.9391\n",
      "Epoch 735/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0982 - accuracy: 0.9617 - val_loss: 0.1705 - val_accuracy: 0.9376\n",
      "Epoch 736/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0981 - accuracy: 0.9615 - val_loss: 0.1684 - val_accuracy: 0.9384\n",
      "Epoch 737/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0979 - accuracy: 0.9615 - val_loss: 0.1675 - val_accuracy: 0.9386\n",
      "Epoch 738/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0980 - accuracy: 0.9615 - val_loss: 0.1683 - val_accuracy: 0.9390\n",
      "Epoch 739/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0979 - accuracy: 0.9616 - val_loss: 0.1672 - val_accuracy: 0.9393\n",
      "Epoch 740/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0978 - accuracy: 0.9617 - val_loss: 0.1678 - val_accuracy: 0.9394\n",
      "Epoch 741/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0976 - accuracy: 0.9617 - val_loss: 0.1670 - val_accuracy: 0.9395\n",
      "Epoch 742/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0977 - accuracy: 0.9617 - val_loss: 0.1697 - val_accuracy: 0.9377\n",
      "Epoch 743/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0975 - accuracy: 0.9619 - val_loss: 0.1708 - val_accuracy: 0.9381\n",
      "Epoch 744/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0975 - accuracy: 0.9620 - val_loss: 0.1692 - val_accuracy: 0.9396\n",
      "Epoch 745/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0974 - accuracy: 0.9618 - val_loss: 0.1689 - val_accuracy: 0.9388\n",
      "Epoch 746/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0974 - accuracy: 0.9619 - val_loss: 0.1679 - val_accuracy: 0.9392\n",
      "Epoch 747/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0972 - accuracy: 0.9619 - val_loss: 0.1692 - val_accuracy: 0.9395\n",
      "Epoch 748/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0972 - accuracy: 0.9618 - val_loss: 0.1688 - val_accuracy: 0.9388\n",
      "Epoch 749/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0972 - accuracy: 0.9621 - val_loss: 0.1695 - val_accuracy: 0.9386\n",
      "Epoch 750/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0970 - accuracy: 0.9621 - val_loss: 0.1718 - val_accuracy: 0.9386\n",
      "Epoch 751/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0971 - accuracy: 0.9621 - val_loss: 0.1696 - val_accuracy: 0.9391\n",
      "Epoch 752/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0969 - accuracy: 0.9621 - val_loss: 0.1685 - val_accuracy: 0.9397\n",
      "Epoch 753/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0968 - accuracy: 0.9625 - val_loss: 0.1707 - val_accuracy: 0.9375\n",
      "Epoch 754/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0968 - accuracy: 0.9621 - val_loss: 0.1695 - val_accuracy: 0.9398\n",
      "Epoch 755/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0968 - accuracy: 0.9623 - val_loss: 0.1701 - val_accuracy: 0.9385\n",
      "Epoch 756/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0965 - accuracy: 0.9623 - val_loss: 0.1707 - val_accuracy: 0.9392\n",
      "Epoch 757/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0967 - accuracy: 0.9622 - val_loss: 0.1702 - val_accuracy: 0.9384\n",
      "Epoch 758/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0964 - accuracy: 0.9626 - val_loss: 0.1684 - val_accuracy: 0.9392\n",
      "Epoch 759/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0964 - accuracy: 0.9623 - val_loss: 0.1683 - val_accuracy: 0.9389\n",
      "Epoch 760/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0964 - accuracy: 0.9625 - val_loss: 0.1706 - val_accuracy: 0.9391\n",
      "Epoch 761/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0962 - accuracy: 0.9625 - val_loss: 0.1697 - val_accuracy: 0.9386\n",
      "Epoch 762/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0961 - accuracy: 0.9623 - val_loss: 0.1695 - val_accuracy: 0.9392\n",
      "Epoch 763/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0962 - accuracy: 0.9623 - val_loss: 0.1697 - val_accuracy: 0.9389\n",
      "Epoch 764/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.1728 - val_accuracy: 0.9375\n",
      "Epoch 765/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0961 - accuracy: 0.9625 - val_loss: 0.1707 - val_accuracy: 0.9389\n",
      "Epoch 766/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0959 - accuracy: 0.9626 - val_loss: 0.1702 - val_accuracy: 0.9386\n",
      "Epoch 767/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0959 - accuracy: 0.9626 - val_loss: 0.1717 - val_accuracy: 0.9377\n",
      "Epoch 768/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0959 - accuracy: 0.9626 - val_loss: 0.1698 - val_accuracy: 0.9386\n",
      "Epoch 769/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0956 - accuracy: 0.9625 - val_loss: 0.1704 - val_accuracy: 0.9388\n",
      "Epoch 770/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0955 - accuracy: 0.9625 - val_loss: 0.1699 - val_accuracy: 0.9397\n",
      "Epoch 771/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0956 - accuracy: 0.9627 - val_loss: 0.1713 - val_accuracy: 0.9395\n",
      "Epoch 772/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0954 - accuracy: 0.9627 - val_loss: 0.1715 - val_accuracy: 0.9394\n",
      "Epoch 773/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0955 - accuracy: 0.9626 - val_loss: 0.1713 - val_accuracy: 0.9396\n",
      "Epoch 774/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0954 - accuracy: 0.9627 - val_loss: 0.1704 - val_accuracy: 0.9383\n",
      "Epoch 775/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0953 - accuracy: 0.9629 - val_loss: 0.1704 - val_accuracy: 0.9382\n",
      "Epoch 776/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0950 - accuracy: 0.9628 - val_loss: 0.1713 - val_accuracy: 0.9384\n",
      "Epoch 777/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0950 - accuracy: 0.9628 - val_loss: 0.1718 - val_accuracy: 0.9388\n",
      "Epoch 778/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0950 - accuracy: 0.9627 - val_loss: 0.1716 - val_accuracy: 0.9381\n",
      "Epoch 779/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0950 - accuracy: 0.9629 - val_loss: 0.1712 - val_accuracy: 0.9395\n",
      "Epoch 780/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0949 - accuracy: 0.9629 - val_loss: 0.1713 - val_accuracy: 0.9395\n",
      "Epoch 781/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0948 - accuracy: 0.9627 - val_loss: 0.1732 - val_accuracy: 0.9384\n",
      "Epoch 782/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0947 - accuracy: 0.9630 - val_loss: 0.1715 - val_accuracy: 0.9389\n",
      "Epoch 783/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0946 - accuracy: 0.9630 - val_loss: 0.1724 - val_accuracy: 0.9383\n",
      "Epoch 784/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0947 - accuracy: 0.9629 - val_loss: 0.1715 - val_accuracy: 0.9383\n",
      "Epoch 785/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0946 - accuracy: 0.9628 - val_loss: 0.1719 - val_accuracy: 0.9383\n",
      "Epoch 786/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0945 - accuracy: 0.9629 - val_loss: 0.1717 - val_accuracy: 0.9393\n",
      "Epoch 787/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0943 - accuracy: 0.9632 - val_loss: 0.1723 - val_accuracy: 0.9387\n",
      "Epoch 788/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0943 - accuracy: 0.9632 - val_loss: 0.1731 - val_accuracy: 0.9384\n",
      "Epoch 789/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0941 - accuracy: 0.9632 - val_loss: 0.1721 - val_accuracy: 0.9385\n",
      "Epoch 790/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0943 - accuracy: 0.9634 - val_loss: 0.1729 - val_accuracy: 0.9384\n",
      "Epoch 791/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0941 - accuracy: 0.9633 - val_loss: 0.1714 - val_accuracy: 0.9385\n",
      "Epoch 792/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.0941 - accuracy: 0.9630 - val_loss: 0.1723 - val_accuracy: 0.9385\n",
      "Epoch 793/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0939 - accuracy: 0.9632 - val_loss: 0.1723 - val_accuracy: 0.9385\n",
      "Epoch 794/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0938 - accuracy: 0.9633 - val_loss: 0.1749 - val_accuracy: 0.9364\n",
      "Epoch 795/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0939 - accuracy: 0.9632 - val_loss: 0.1749 - val_accuracy: 0.9375\n",
      "Epoch 796/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0936 - accuracy: 0.9635 - val_loss: 0.1733 - val_accuracy: 0.9389\n",
      "Epoch 797/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0937 - accuracy: 0.9632 - val_loss: 0.1726 - val_accuracy: 0.9387\n",
      "Epoch 798/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0936 - accuracy: 0.9636 - val_loss: 0.1719 - val_accuracy: 0.9390\n",
      "Epoch 799/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0935 - accuracy: 0.9634 - val_loss: 0.1725 - val_accuracy: 0.9387\n",
      "Epoch 800/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0934 - accuracy: 0.9634 - val_loss: 0.1720 - val_accuracy: 0.9385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:122: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Train on 6000 samples, validate on 2001 samples\n",
      "Epoch 1/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.3886 - accuracy: 0.8688 - val_loss: 0.2650 - val_accuracy: 0.9250\n",
      "Epoch 2/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2491 - accuracy: 0.9306 - val_loss: 0.2642 - val_accuracy: 0.9250\n",
      "Epoch 3/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2486 - accuracy: 0.9306 - val_loss: 0.2638 - val_accuracy: 0.9250\n",
      "Epoch 4/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2483 - accuracy: 0.9306 - val_loss: 0.2634 - val_accuracy: 0.9250\n",
      "Epoch 5/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2480 - accuracy: 0.9306 - val_loss: 0.2631 - val_accuracy: 0.9250\n",
      "Epoch 6/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2477 - accuracy: 0.9306 - val_loss: 0.2629 - val_accuracy: 0.9250\n",
      "Epoch 7/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.2474 - accuracy: 0.9306 - val_loss: 0.2624 - val_accuracy: 0.9250\n",
      "Epoch 8/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2470 - accuracy: 0.9306 - val_loss: 0.2621 - val_accuracy: 0.9250\n",
      "Epoch 9/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2467 - accuracy: 0.9306 - val_loss: 0.2619 - val_accuracy: 0.9250\n",
      "Epoch 10/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2463 - accuracy: 0.9306 - val_loss: 0.2614 - val_accuracy: 0.9250\n",
      "Epoch 11/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2459 - accuracy: 0.9306 - val_loss: 0.2610 - val_accuracy: 0.9250\n",
      "Epoch 12/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2455 - accuracy: 0.9306 - val_loss: 0.2607 - val_accuracy: 0.9250\n",
      "Epoch 13/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2450 - accuracy: 0.9306 - val_loss: 0.2599 - val_accuracy: 0.9250\n",
      "Epoch 14/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2444 - accuracy: 0.9306 - val_loss: 0.2592 - val_accuracy: 0.9250\n",
      "Epoch 15/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2438 - accuracy: 0.9306 - val_loss: 0.2587 - val_accuracy: 0.9250\n",
      "Epoch 16/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2431 - accuracy: 0.9306 - val_loss: 0.2579 - val_accuracy: 0.9250\n",
      "Epoch 17/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2424 - accuracy: 0.9306 - val_loss: 0.2571 - val_accuracy: 0.9250\n",
      "Epoch 18/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.2416 - accuracy: 0.9306 - val_loss: 0.2562 - val_accuracy: 0.9250\n",
      "Epoch 19/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.2406 - accuracy: 0.9306 - val_loss: 0.2551 - val_accuracy: 0.9250\n",
      "Epoch 20/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.2396 - accuracy: 0.9306 - val_loss: 0.2540 - val_accuracy: 0.9250\n",
      "Epoch 21/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2384 - accuracy: 0.9306 - val_loss: 0.2530 - val_accuracy: 0.9250\n",
      "Epoch 22/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.2372 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9250\n",
      "Epoch 23/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.2359 - accuracy: 0.9306 - val_loss: 0.2502 - val_accuracy: 0.9250\n",
      "Epoch 24/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.2345 - accuracy: 0.9306 - val_loss: 0.2488 - val_accuracy: 0.9250\n",
      "Epoch 25/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.2330 - accuracy: 0.9306 - val_loss: 0.2472 - val_accuracy: 0.9250\n",
      "Epoch 26/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.2315 - accuracy: 0.9306 - val_loss: 0.2457 - val_accuracy: 0.9250\n",
      "Epoch 27/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.2299 - accuracy: 0.9306 - val_loss: 0.2440 - val_accuracy: 0.9250\n",
      "Epoch 28/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2282 - accuracy: 0.9307 - val_loss: 0.2423 - val_accuracy: 0.9251\n",
      "Epoch 29/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2266 - accuracy: 0.9307 - val_loss: 0.2404 - val_accuracy: 0.9252\n",
      "Epoch 30/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2248 - accuracy: 0.9307 - val_loss: 0.2388 - val_accuracy: 0.9252\n",
      "Epoch 31/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2231 - accuracy: 0.9308 - val_loss: 0.2372 - val_accuracy: 0.9252\n",
      "Epoch 32/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2215 - accuracy: 0.9308 - val_loss: 0.2354 - val_accuracy: 0.9253\n",
      "Epoch 33/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2198 - accuracy: 0.9308 - val_loss: 0.2338 - val_accuracy: 0.9253\n",
      "Epoch 34/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2181 - accuracy: 0.9309 - val_loss: 0.2322 - val_accuracy: 0.9253\n",
      "Epoch 35/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2165 - accuracy: 0.9309 - val_loss: 0.2304 - val_accuracy: 0.9254\n",
      "Epoch 36/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2149 - accuracy: 0.9309 - val_loss: 0.2289 - val_accuracy: 0.9255\n",
      "Epoch 37/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2134 - accuracy: 0.9310 - val_loss: 0.2273 - val_accuracy: 0.9255\n",
      "Epoch 38/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2117 - accuracy: 0.9309 - val_loss: 0.2260 - val_accuracy: 0.9257\n",
      "Epoch 39/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2102 - accuracy: 0.9310 - val_loss: 0.2245 - val_accuracy: 0.9258\n",
      "Epoch 40/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2087 - accuracy: 0.9311 - val_loss: 0.2229 - val_accuracy: 0.9257\n",
      "Epoch 41/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2072 - accuracy: 0.9312 - val_loss: 0.2216 - val_accuracy: 0.9258\n",
      "Epoch 42/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2058 - accuracy: 0.9313 - val_loss: 0.2204 - val_accuracy: 0.9260\n",
      "Epoch 43/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.2044 - accuracy: 0.9313 - val_loss: 0.2189 - val_accuracy: 0.9260\n",
      "Epoch 44/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2029 - accuracy: 0.9313 - val_loss: 0.2175 - val_accuracy: 0.9262\n",
      "Epoch 45/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2016 - accuracy: 0.9315 - val_loss: 0.2162 - val_accuracy: 0.9265\n",
      "Epoch 46/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2002 - accuracy: 0.9316 - val_loss: 0.2151 - val_accuracy: 0.9263\n",
      "Epoch 47/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1988 - accuracy: 0.9316 - val_loss: 0.2136 - val_accuracy: 0.9265\n",
      "Epoch 48/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1974 - accuracy: 0.9317 - val_loss: 0.2126 - val_accuracy: 0.9267\n",
      "Epoch 49/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1961 - accuracy: 0.9318 - val_loss: 0.2111 - val_accuracy: 0.9266\n",
      "Epoch 50/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1947 - accuracy: 0.9321 - val_loss: 0.2098 - val_accuracy: 0.9268\n",
      "Epoch 51/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1934 - accuracy: 0.9321 - val_loss: 0.2089 - val_accuracy: 0.9267\n",
      "Epoch 52/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1921 - accuracy: 0.9323 - val_loss: 0.2072 - val_accuracy: 0.9268\n",
      "Epoch 53/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1907 - accuracy: 0.9325 - val_loss: 0.2060 - val_accuracy: 0.9268\n",
      "Epoch 54/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1894 - accuracy: 0.9328 - val_loss: 0.2048 - val_accuracy: 0.9270\n",
      "Epoch 55/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1882 - accuracy: 0.9330 - val_loss: 0.2038 - val_accuracy: 0.9272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1870 - accuracy: 0.9333 - val_loss: 0.2025 - val_accuracy: 0.9273\n",
      "Epoch 57/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1858 - accuracy: 0.9335 - val_loss: 0.2016 - val_accuracy: 0.9277\n",
      "Epoch 58/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1848 - accuracy: 0.9338 - val_loss: 0.2008 - val_accuracy: 0.9276\n",
      "Epoch 59/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1838 - accuracy: 0.9341 - val_loss: 0.2001 - val_accuracy: 0.9280\n",
      "Epoch 60/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1828 - accuracy: 0.9342 - val_loss: 0.1989 - val_accuracy: 0.9280\n",
      "Epoch 61/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1819 - accuracy: 0.9344 - val_loss: 0.1982 - val_accuracy: 0.9282\n",
      "Epoch 62/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1811 - accuracy: 0.9347 - val_loss: 0.1978 - val_accuracy: 0.9281\n",
      "Epoch 63/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1802 - accuracy: 0.9348 - val_loss: 0.1968 - val_accuracy: 0.9283\n",
      "Epoch 64/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1795 - accuracy: 0.9350 - val_loss: 0.1962 - val_accuracy: 0.9290\n",
      "Epoch 65/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1787 - accuracy: 0.9352 - val_loss: 0.1954 - val_accuracy: 0.9286\n",
      "Epoch 66/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1779 - accuracy: 0.9354 - val_loss: 0.1953 - val_accuracy: 0.9287\n",
      "Epoch 67/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1773 - accuracy: 0.9356 - val_loss: 0.1942 - val_accuracy: 0.9292\n",
      "Epoch 68/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1766 - accuracy: 0.9356 - val_loss: 0.1932 - val_accuracy: 0.9293\n",
      "Epoch 69/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1759 - accuracy: 0.9360 - val_loss: 0.1929 - val_accuracy: 0.9295\n",
      "Epoch 70/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1753 - accuracy: 0.9361 - val_loss: 0.1923 - val_accuracy: 0.9293\n",
      "Epoch 71/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1747 - accuracy: 0.9363 - val_loss: 0.1918 - val_accuracy: 0.9297\n",
      "Epoch 72/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1742 - accuracy: 0.9364 - val_loss: 0.1913 - val_accuracy: 0.9301\n",
      "Epoch 73/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1736 - accuracy: 0.9368 - val_loss: 0.1908 - val_accuracy: 0.9300\n",
      "Epoch 74/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1730 - accuracy: 0.9369 - val_loss: 0.1904 - val_accuracy: 0.9300\n",
      "Epoch 75/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1726 - accuracy: 0.9370 - val_loss: 0.1898 - val_accuracy: 0.9300\n",
      "Epoch 76/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1720 - accuracy: 0.9372 - val_loss: 0.1894 - val_accuracy: 0.9301\n",
      "Epoch 77/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1715 - accuracy: 0.9374 - val_loss: 0.1891 - val_accuracy: 0.9304\n",
      "Epoch 78/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1710 - accuracy: 0.9376 - val_loss: 0.1886 - val_accuracy: 0.9303\n",
      "Epoch 79/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1706 - accuracy: 0.9378 - val_loss: 0.1885 - val_accuracy: 0.9308\n",
      "Epoch 80/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1701 - accuracy: 0.9380 - val_loss: 0.1878 - val_accuracy: 0.9309\n",
      "Epoch 81/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1696 - accuracy: 0.9382 - val_loss: 0.1877 - val_accuracy: 0.9312\n",
      "Epoch 82/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1692 - accuracy: 0.9383 - val_loss: 0.1868 - val_accuracy: 0.9317\n",
      "Epoch 83/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1688 - accuracy: 0.9385 - val_loss: 0.1864 - val_accuracy: 0.9314\n",
      "Epoch 84/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1684 - accuracy: 0.9385 - val_loss: 0.1863 - val_accuracy: 0.9314\n",
      "Epoch 85/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1680 - accuracy: 0.9387 - val_loss: 0.1861 - val_accuracy: 0.9318\n",
      "Epoch 86/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1676 - accuracy: 0.9388 - val_loss: 0.1864 - val_accuracy: 0.9315\n",
      "Epoch 87/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1672 - accuracy: 0.9390 - val_loss: 0.1851 - val_accuracy: 0.9316\n",
      "Epoch 88/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1668 - accuracy: 0.9392 - val_loss: 0.1850 - val_accuracy: 0.9320\n",
      "Epoch 89/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1664 - accuracy: 0.9394 - val_loss: 0.1846 - val_accuracy: 0.9323\n",
      "Epoch 90/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1661 - accuracy: 0.9395 - val_loss: 0.1847 - val_accuracy: 0.9325\n",
      "Epoch 91/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1657 - accuracy: 0.9396 - val_loss: 0.1837 - val_accuracy: 0.9321\n",
      "Epoch 92/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1654 - accuracy: 0.9396 - val_loss: 0.1841 - val_accuracy: 0.9325\n",
      "Epoch 93/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1650 - accuracy: 0.9397 - val_loss: 0.1833 - val_accuracy: 0.9323\n",
      "Epoch 94/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1647 - accuracy: 0.9398 - val_loss: 0.1831 - val_accuracy: 0.9327\n",
      "Epoch 95/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1643 - accuracy: 0.9400 - val_loss: 0.1831 - val_accuracy: 0.9325\n",
      "Epoch 96/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1640 - accuracy: 0.9402 - val_loss: 0.1822 - val_accuracy: 0.9328\n",
      "Epoch 97/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1635 - accuracy: 0.9403 - val_loss: 0.1824 - val_accuracy: 0.9328\n",
      "Epoch 98/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1632 - accuracy: 0.9403 - val_loss: 0.1819 - val_accuracy: 0.9329\n",
      "Epoch 99/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1628 - accuracy: 0.9407 - val_loss: 0.1813 - val_accuracy: 0.9332\n",
      "Epoch 100/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1625 - accuracy: 0.9406 - val_loss: 0.1815 - val_accuracy: 0.9338\n",
      "Epoch 101/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1620 - accuracy: 0.9408 - val_loss: 0.1809 - val_accuracy: 0.9333\n",
      "Epoch 102/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1617 - accuracy: 0.9409 - val_loss: 0.1807 - val_accuracy: 0.9335\n",
      "Epoch 103/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1613 - accuracy: 0.9410 - val_loss: 0.1801 - val_accuracy: 0.9335\n",
      "Epoch 104/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1608 - accuracy: 0.9413 - val_loss: 0.1800 - val_accuracy: 0.9340\n",
      "Epoch 105/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1605 - accuracy: 0.9413 - val_loss: 0.1795 - val_accuracy: 0.9338\n",
      "Epoch 106/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1600 - accuracy: 0.9414 - val_loss: 0.1785 - val_accuracy: 0.9342\n",
      "Epoch 107/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1596 - accuracy: 0.9415 - val_loss: 0.1787 - val_accuracy: 0.9339\n",
      "Epoch 108/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1592 - accuracy: 0.9417 - val_loss: 0.1782 - val_accuracy: 0.9339\n",
      "Epoch 109/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1588 - accuracy: 0.9419 - val_loss: 0.1774 - val_accuracy: 0.9348\n",
      "Epoch 110/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1584 - accuracy: 0.9421 - val_loss: 0.1775 - val_accuracy: 0.9347\n",
      "Epoch 111/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1580 - accuracy: 0.9423 - val_loss: 0.1766 - val_accuracy: 0.9349\n",
      "Epoch 112/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1576 - accuracy: 0.9424 - val_loss: 0.1766 - val_accuracy: 0.9349\n",
      "Epoch 113/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1573 - accuracy: 0.9425 - val_loss: 0.1766 - val_accuracy: 0.9346\n",
      "Epoch 114/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1568 - accuracy: 0.9427 - val_loss: 0.1758 - val_accuracy: 0.9352\n",
      "Epoch 115/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1564 - accuracy: 0.9427 - val_loss: 0.1758 - val_accuracy: 0.9356\n",
      "Epoch 116/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1561 - accuracy: 0.9429 - val_loss: 0.1754 - val_accuracy: 0.9356\n",
      "Epoch 117/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1557 - accuracy: 0.9432 - val_loss: 0.1749 - val_accuracy: 0.9356\n",
      "Epoch 118/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1554 - accuracy: 0.9433 - val_loss: 0.1749 - val_accuracy: 0.9354\n",
      "Epoch 119/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1551 - accuracy: 0.9433 - val_loss: 0.1744 - val_accuracy: 0.9363\n",
      "Epoch 120/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1547 - accuracy: 0.9434 - val_loss: 0.1740 - val_accuracy: 0.9362\n",
      "Epoch 121/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1544 - accuracy: 0.9436 - val_loss: 0.1744 - val_accuracy: 0.9359\n",
      "Epoch 122/800\n",
      "6000/6000 [==============================] - 1s 83us/step - loss: 0.1541 - accuracy: 0.9439 - val_loss: 0.1738 - val_accuracy: 0.9361\n",
      "Epoch 123/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1538 - accuracy: 0.9438 - val_loss: 0.1738 - val_accuracy: 0.9364\n",
      "Epoch 124/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1534 - accuracy: 0.9439 - val_loss: 0.1729 - val_accuracy: 0.9364\n",
      "Epoch 125/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1532 - accuracy: 0.9442 - val_loss: 0.1729 - val_accuracy: 0.9365\n",
      "Epoch 126/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1530 - accuracy: 0.9442 - val_loss: 0.1726 - val_accuracy: 0.9369\n",
      "Epoch 127/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1526 - accuracy: 0.9444 - val_loss: 0.1726 - val_accuracy: 0.9370\n",
      "Epoch 128/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1524 - accuracy: 0.9445 - val_loss: 0.1732 - val_accuracy: 0.9365\n",
      "Epoch 129/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1522 - accuracy: 0.9443 - val_loss: 0.1720 - val_accuracy: 0.9369\n",
      "Epoch 130/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1519 - accuracy: 0.9445 - val_loss: 0.1718 - val_accuracy: 0.9368\n",
      "Epoch 131/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1517 - accuracy: 0.9446 - val_loss: 0.1717 - val_accuracy: 0.9370\n",
      "Epoch 132/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1515 - accuracy: 0.9449 - val_loss: 0.1714 - val_accuracy: 0.9371\n",
      "Epoch 133/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1513 - accuracy: 0.9449 - val_loss: 0.1715 - val_accuracy: 0.9372\n",
      "Epoch 134/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1510 - accuracy: 0.9450 - val_loss: 0.1711 - val_accuracy: 0.9371\n",
      "Epoch 135/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1508 - accuracy: 0.9452 - val_loss: 0.1711 - val_accuracy: 0.9372\n",
      "Epoch 136/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1507 - accuracy: 0.9452 - val_loss: 0.1711 - val_accuracy: 0.9373\n",
      "Epoch 137/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1504 - accuracy: 0.9453 - val_loss: 0.1711 - val_accuracy: 0.9368\n",
      "Epoch 138/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1502 - accuracy: 0.9450 - val_loss: 0.1705 - val_accuracy: 0.9373\n",
      "Epoch 139/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1500 - accuracy: 0.9455 - val_loss: 0.1704 - val_accuracy: 0.9376\n",
      "Epoch 140/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1498 - accuracy: 0.9455 - val_loss: 0.1701 - val_accuracy: 0.9377\n",
      "Epoch 141/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1497 - accuracy: 0.9455 - val_loss: 0.1708 - val_accuracy: 0.9374\n",
      "Epoch 142/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1495 - accuracy: 0.9456 - val_loss: 0.1703 - val_accuracy: 0.9379\n",
      "Epoch 143/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1493 - accuracy: 0.9458 - val_loss: 0.1708 - val_accuracy: 0.9375\n",
      "Epoch 144/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1490 - accuracy: 0.9457 - val_loss: 0.1699 - val_accuracy: 0.9374\n",
      "Epoch 145/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1490 - accuracy: 0.9460 - val_loss: 0.1698 - val_accuracy: 0.9383\n",
      "Epoch 146/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1487 - accuracy: 0.9459 - val_loss: 0.1700 - val_accuracy: 0.9378\n",
      "Epoch 147/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1485 - accuracy: 0.9458 - val_loss: 0.1702 - val_accuracy: 0.9378\n",
      "Epoch 148/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1484 - accuracy: 0.9462 - val_loss: 0.1696 - val_accuracy: 0.9377\n",
      "Epoch 149/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1483 - accuracy: 0.9460 - val_loss: 0.1697 - val_accuracy: 0.9377\n",
      "Epoch 150/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1480 - accuracy: 0.9459 - val_loss: 0.1692 - val_accuracy: 0.9379\n",
      "Epoch 151/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1478 - accuracy: 0.9462 - val_loss: 0.1688 - val_accuracy: 0.9379\n",
      "Epoch 152/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1477 - accuracy: 0.9462 - val_loss: 0.1692 - val_accuracy: 0.9382\n",
      "Epoch 153/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1475 - accuracy: 0.9462 - val_loss: 0.1689 - val_accuracy: 0.9384\n",
      "Epoch 154/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1474 - accuracy: 0.9463 - val_loss: 0.1689 - val_accuracy: 0.9383\n",
      "Epoch 155/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1472 - accuracy: 0.9464 - val_loss: 0.1684 - val_accuracy: 0.9382\n",
      "Epoch 156/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1471 - accuracy: 0.9464 - val_loss: 0.1680 - val_accuracy: 0.9384\n",
      "Epoch 157/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1468 - accuracy: 0.9466 - val_loss: 0.1680 - val_accuracy: 0.9383\n",
      "Epoch 158/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1467 - accuracy: 0.9466 - val_loss: 0.1678 - val_accuracy: 0.9388\n",
      "Epoch 159/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1466 - accuracy: 0.9467 - val_loss: 0.1677 - val_accuracy: 0.9382\n",
      "Epoch 160/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1463 - accuracy: 0.9466 - val_loss: 0.1682 - val_accuracy: 0.9384\n",
      "Epoch 161/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1461 - accuracy: 0.9466 - val_loss: 0.1679 - val_accuracy: 0.9385\n",
      "Epoch 162/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1461 - accuracy: 0.9467 - val_loss: 0.1677 - val_accuracy: 0.9385\n",
      "Epoch 163/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1459 - accuracy: 0.9469 - val_loss: 0.1676 - val_accuracy: 0.9384\n",
      "Epoch 164/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1457 - accuracy: 0.9466 - val_loss: 0.1676 - val_accuracy: 0.9383\n",
      "Epoch 165/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1455 - accuracy: 0.9468 - val_loss: 0.1670 - val_accuracy: 0.9386\n",
      "Epoch 166/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1453 - accuracy: 0.9470 - val_loss: 0.1675 - val_accuracy: 0.9385\n",
      "Epoch 167/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1452 - accuracy: 0.9469 - val_loss: 0.1669 - val_accuracy: 0.9389\n",
      "Epoch 168/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1450 - accuracy: 0.9470 - val_loss: 0.1672 - val_accuracy: 0.9390\n",
      "Epoch 169/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1448 - accuracy: 0.9470 - val_loss: 0.1664 - val_accuracy: 0.9389\n",
      "Epoch 170/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1446 - accuracy: 0.9473 - val_loss: 0.1665 - val_accuracy: 0.9385\n",
      "Epoch 171/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1445 - accuracy: 0.9473 - val_loss: 0.1662 - val_accuracy: 0.9387\n",
      "Epoch 172/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1443 - accuracy: 0.9473 - val_loss: 0.1663 - val_accuracy: 0.9391\n",
      "Epoch 173/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1441 - accuracy: 0.9472 - val_loss: 0.1666 - val_accuracy: 0.9389\n",
      "Epoch 174/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1440 - accuracy: 0.9473 - val_loss: 0.1660 - val_accuracy: 0.9392\n",
      "Epoch 175/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1438 - accuracy: 0.9475 - val_loss: 0.1666 - val_accuracy: 0.9389\n",
      "Epoch 176/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1437 - accuracy: 0.9474 - val_loss: 0.1659 - val_accuracy: 0.9389\n",
      "Epoch 177/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1435 - accuracy: 0.9474 - val_loss: 0.1660 - val_accuracy: 0.9389\n",
      "Epoch 178/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1433 - accuracy: 0.9474 - val_loss: 0.1658 - val_accuracy: 0.9388\n",
      "Epoch 179/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1431 - accuracy: 0.9474 - val_loss: 0.1655 - val_accuracy: 0.9392\n",
      "Epoch 180/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1430 - accuracy: 0.9475 - val_loss: 0.1655 - val_accuracy: 0.9390\n",
      "Epoch 181/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1428 - accuracy: 0.9477 - val_loss: 0.1652 - val_accuracy: 0.9389\n",
      "Epoch 182/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1427 - accuracy: 0.9476 - val_loss: 0.1653 - val_accuracy: 0.9388\n",
      "Epoch 183/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1425 - accuracy: 0.9477 - val_loss: 0.1649 - val_accuracy: 0.9391\n",
      "Epoch 184/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1423 - accuracy: 0.9477 - val_loss: 0.1647 - val_accuracy: 0.9392\n",
      "Epoch 185/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1421 - accuracy: 0.9479 - val_loss: 0.1649 - val_accuracy: 0.9396\n",
      "Epoch 186/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1420 - accuracy: 0.9480 - val_loss: 0.1650 - val_accuracy: 0.9392\n",
      "Epoch 187/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1417 - accuracy: 0.9480 - val_loss: 0.1646 - val_accuracy: 0.9393\n",
      "Epoch 188/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1416 - accuracy: 0.9480 - val_loss: 0.1645 - val_accuracy: 0.9389\n",
      "Epoch 189/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1415 - accuracy: 0.9482 - val_loss: 0.1646 - val_accuracy: 0.9390\n",
      "Epoch 190/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1413 - accuracy: 0.9481 - val_loss: 0.1643 - val_accuracy: 0.9393\n",
      "Epoch 191/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1411 - accuracy: 0.9481 - val_loss: 0.1645 - val_accuracy: 0.9391\n",
      "Epoch 192/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1410 - accuracy: 0.9484 - val_loss: 0.1641 - val_accuracy: 0.9394\n",
      "Epoch 193/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1408 - accuracy: 0.9480 - val_loss: 0.1635 - val_accuracy: 0.9398\n",
      "Epoch 194/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1406 - accuracy: 0.9484 - val_loss: 0.1639 - val_accuracy: 0.9393\n",
      "Epoch 195/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1406 - accuracy: 0.9483 - val_loss: 0.1638 - val_accuracy: 0.9398\n",
      "Epoch 196/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1404 - accuracy: 0.9484 - val_loss: 0.1631 - val_accuracy: 0.9392\n",
      "Epoch 197/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1401 - accuracy: 0.9483 - val_loss: 0.1631 - val_accuracy: 0.9402\n",
      "Epoch 198/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1400 - accuracy: 0.9485 - val_loss: 0.1637 - val_accuracy: 0.9395\n",
      "Epoch 199/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1398 - accuracy: 0.9485 - val_loss: 0.1629 - val_accuracy: 0.9393\n",
      "Epoch 200/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1397 - accuracy: 0.9485 - val_loss: 0.1630 - val_accuracy: 0.9395\n",
      "Epoch 201/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1395 - accuracy: 0.9486 - val_loss: 0.1633 - val_accuracy: 0.9400\n",
      "Epoch 202/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1394 - accuracy: 0.9484 - val_loss: 0.1628 - val_accuracy: 0.9404\n",
      "Epoch 203/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1392 - accuracy: 0.9487 - val_loss: 0.1632 - val_accuracy: 0.9397\n",
      "Epoch 204/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1390 - accuracy: 0.9488 - val_loss: 0.1626 - val_accuracy: 0.9399\n",
      "Epoch 205/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1389 - accuracy: 0.9488 - val_loss: 0.1626 - val_accuracy: 0.9400\n",
      "Epoch 206/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1387 - accuracy: 0.9489 - val_loss: 0.1626 - val_accuracy: 0.9398\n",
      "Epoch 207/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1386 - accuracy: 0.9488 - val_loss: 0.1620 - val_accuracy: 0.9399\n",
      "Epoch 208/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1384 - accuracy: 0.9489 - val_loss: 0.1625 - val_accuracy: 0.9397\n",
      "Epoch 209/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1382 - accuracy: 0.9490 - val_loss: 0.1623 - val_accuracy: 0.9401\n",
      "Epoch 210/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1381 - accuracy: 0.9490 - val_loss: 0.1619 - val_accuracy: 0.9398\n",
      "Epoch 211/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1379 - accuracy: 0.9491 - val_loss: 0.1620 - val_accuracy: 0.9404\n",
      "Epoch 212/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1378 - accuracy: 0.9489 - val_loss: 0.1621 - val_accuracy: 0.9401\n",
      "Epoch 213/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1377 - accuracy: 0.9491 - val_loss: 0.1617 - val_accuracy: 0.9402\n",
      "Epoch 214/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1376 - accuracy: 0.9491 - val_loss: 0.1618 - val_accuracy: 0.9402\n",
      "Epoch 215/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1374 - accuracy: 0.9492 - val_loss: 0.1616 - val_accuracy: 0.9400\n",
      "Epoch 216/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1372 - accuracy: 0.9492 - val_loss: 0.1612 - val_accuracy: 0.9399\n",
      "Epoch 217/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1370 - accuracy: 0.9494 - val_loss: 0.1611 - val_accuracy: 0.9405\n",
      "Epoch 218/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1369 - accuracy: 0.9495 - val_loss: 0.1607 - val_accuracy: 0.9400\n",
      "Epoch 219/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1368 - accuracy: 0.9492 - val_loss: 0.1614 - val_accuracy: 0.9400\n",
      "Epoch 220/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1366 - accuracy: 0.9494 - val_loss: 0.1610 - val_accuracy: 0.9407\n",
      "Epoch 221/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1365 - accuracy: 0.9492 - val_loss: 0.1611 - val_accuracy: 0.9404\n",
      "Epoch 222/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1363 - accuracy: 0.9493 - val_loss: 0.1609 - val_accuracy: 0.9407\n",
      "Epoch 223/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1362 - accuracy: 0.9496 - val_loss: 0.1608 - val_accuracy: 0.9406\n",
      "Epoch 224/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1361 - accuracy: 0.9496 - val_loss: 0.1612 - val_accuracy: 0.9404\n",
      "Epoch 225/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1359 - accuracy: 0.9495 - val_loss: 0.1605 - val_accuracy: 0.9406\n",
      "Epoch 226/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1358 - accuracy: 0.9497 - val_loss: 0.1604 - val_accuracy: 0.9404\n",
      "Epoch 227/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1357 - accuracy: 0.9495 - val_loss: 0.1606 - val_accuracy: 0.9404\n",
      "Epoch 228/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1355 - accuracy: 0.9496 - val_loss: 0.1605 - val_accuracy: 0.9402\n",
      "Epoch 229/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1353 - accuracy: 0.9497 - val_loss: 0.1603 - val_accuracy: 0.9404\n",
      "Epoch 230/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1353 - accuracy: 0.9495 - val_loss: 0.1603 - val_accuracy: 0.9405\n",
      "Epoch 231/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1351 - accuracy: 0.9497 - val_loss: 0.1601 - val_accuracy: 0.9400\n",
      "Epoch 232/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1350 - accuracy: 0.9496 - val_loss: 0.1598 - val_accuracy: 0.9410\n",
      "Epoch 233/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1348 - accuracy: 0.9499 - val_loss: 0.1602 - val_accuracy: 0.9411\n",
      "Epoch 234/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1347 - accuracy: 0.9497 - val_loss: 0.1598 - val_accuracy: 0.9409\n",
      "Epoch 235/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1345 - accuracy: 0.9500 - val_loss: 0.1601 - val_accuracy: 0.9401\n",
      "Epoch 236/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1344 - accuracy: 0.9501 - val_loss: 0.1598 - val_accuracy: 0.9405\n",
      "Epoch 237/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1343 - accuracy: 0.9500 - val_loss: 0.1602 - val_accuracy: 0.9408\n",
      "Epoch 238/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1341 - accuracy: 0.9501 - val_loss: 0.1589 - val_accuracy: 0.9407\n",
      "Epoch 239/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1340 - accuracy: 0.9501 - val_loss: 0.1595 - val_accuracy: 0.9408\n",
      "Epoch 240/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1339 - accuracy: 0.9502 - val_loss: 0.1590 - val_accuracy: 0.9409\n",
      "Epoch 241/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1338 - accuracy: 0.9502 - val_loss: 0.1598 - val_accuracy: 0.9407\n",
      "Epoch 242/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1337 - accuracy: 0.9501 - val_loss: 0.1592 - val_accuracy: 0.9408\n",
      "Epoch 243/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1336 - accuracy: 0.9502 - val_loss: 0.1587 - val_accuracy: 0.9412\n",
      "Epoch 244/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1335 - accuracy: 0.9501 - val_loss: 0.1593 - val_accuracy: 0.9408\n",
      "Epoch 245/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1333 - accuracy: 0.9504 - val_loss: 0.1588 - val_accuracy: 0.9412\n",
      "Epoch 246/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1332 - accuracy: 0.9501 - val_loss: 0.1592 - val_accuracy: 0.9407\n",
      "Epoch 247/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1331 - accuracy: 0.9504 - val_loss: 0.1590 - val_accuracy: 0.9407\n",
      "Epoch 248/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1329 - accuracy: 0.9506 - val_loss: 0.1591 - val_accuracy: 0.9411\n",
      "Epoch 249/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1328 - accuracy: 0.9503 - val_loss: 0.1593 - val_accuracy: 0.9406\n",
      "Epoch 250/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1327 - accuracy: 0.9504 - val_loss: 0.1588 - val_accuracy: 0.9412\n",
      "Epoch 251/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1326 - accuracy: 0.9504 - val_loss: 0.1591 - val_accuracy: 0.9410\n",
      "Epoch 252/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1325 - accuracy: 0.9506 - val_loss: 0.1590 - val_accuracy: 0.9413\n",
      "Epoch 253/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1324 - accuracy: 0.9507 - val_loss: 0.1587 - val_accuracy: 0.9410\n",
      "Epoch 254/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1323 - accuracy: 0.9505 - val_loss: 0.1580 - val_accuracy: 0.9412\n",
      "Epoch 255/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1321 - accuracy: 0.9507 - val_loss: 0.1585 - val_accuracy: 0.9413\n",
      "Epoch 256/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1319 - accuracy: 0.9505 - val_loss: 0.1585 - val_accuracy: 0.9411\n",
      "Epoch 257/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1319 - accuracy: 0.9505 - val_loss: 0.1587 - val_accuracy: 0.9409\n",
      "Epoch 258/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1318 - accuracy: 0.9506 - val_loss: 0.1584 - val_accuracy: 0.9414\n",
      "Epoch 259/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1317 - accuracy: 0.9506 - val_loss: 0.1586 - val_accuracy: 0.9413\n",
      "Epoch 260/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1316 - accuracy: 0.9508 - val_loss: 0.1585 - val_accuracy: 0.9414\n",
      "Epoch 261/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1316 - accuracy: 0.9506 - val_loss: 0.1580 - val_accuracy: 0.9411\n",
      "Epoch 262/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1314 - accuracy: 0.9508 - val_loss: 0.1581 - val_accuracy: 0.9410\n",
      "Epoch 263/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1312 - accuracy: 0.9507 - val_loss: 0.1588 - val_accuracy: 0.9404\n",
      "Epoch 264/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1311 - accuracy: 0.9508 - val_loss: 0.1579 - val_accuracy: 0.9416\n",
      "Epoch 265/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1311 - accuracy: 0.9509 - val_loss: 0.1575 - val_accuracy: 0.9412\n",
      "Epoch 266/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1309 - accuracy: 0.9508 - val_loss: 0.1578 - val_accuracy: 0.9414\n",
      "Epoch 267/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1308 - accuracy: 0.9510 - val_loss: 0.1581 - val_accuracy: 0.9413\n",
      "Epoch 268/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1307 - accuracy: 0.9511 - val_loss: 0.1574 - val_accuracy: 0.9412\n",
      "Epoch 269/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.1574 - val_accuracy: 0.9409\n",
      "Epoch 270/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1305 - accuracy: 0.9512 - val_loss: 0.1577 - val_accuracy: 0.9410\n",
      "Epoch 271/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1304 - accuracy: 0.9510 - val_loss: 0.1573 - val_accuracy: 0.9418\n",
      "Epoch 272/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1302 - accuracy: 0.9512 - val_loss: 0.1576 - val_accuracy: 0.9415\n",
      "Epoch 273/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1302 - accuracy: 0.9512 - val_loss: 0.1572 - val_accuracy: 0.9414\n",
      "Epoch 274/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1301 - accuracy: 0.9512 - val_loss: 0.1578 - val_accuracy: 0.9412\n",
      "Epoch 275/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1299 - accuracy: 0.9512 - val_loss: 0.1573 - val_accuracy: 0.9412\n",
      "Epoch 276/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1299 - accuracy: 0.9512 - val_loss: 0.1574 - val_accuracy: 0.9416\n",
      "Epoch 277/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1297 - accuracy: 0.9510 - val_loss: 0.1569 - val_accuracy: 0.9413\n",
      "Epoch 278/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1297 - accuracy: 0.9512 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
      "Epoch 279/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1296 - accuracy: 0.9512 - val_loss: 0.1574 - val_accuracy: 0.9411\n",
      "Epoch 280/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1295 - accuracy: 0.9514 - val_loss: 0.1568 - val_accuracy: 0.9413\n",
      "Epoch 281/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1294 - accuracy: 0.9513 - val_loss: 0.1571 - val_accuracy: 0.9419\n",
      "Epoch 282/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1293 - accuracy: 0.9515 - val_loss: 0.1569 - val_accuracy: 0.9413\n",
      "Epoch 283/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1291 - accuracy: 0.9514 - val_loss: 0.1573 - val_accuracy: 0.9413\n",
      "Epoch 284/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1290 - accuracy: 0.9512 - val_loss: 0.1568 - val_accuracy: 0.9414\n",
      "Epoch 285/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1289 - accuracy: 0.9514 - val_loss: 0.1567 - val_accuracy: 0.9416\n",
      "Epoch 286/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1288 - accuracy: 0.9515 - val_loss: 0.1570 - val_accuracy: 0.9415\n",
      "Epoch 287/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1288 - accuracy: 0.9515 - val_loss: 0.1569 - val_accuracy: 0.9413\n",
      "Epoch 288/800\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.95 - 0s 74us/step - loss: 0.1286 - accuracy: 0.9515 - val_loss: 0.1565 - val_accuracy: 0.9413\n",
      "Epoch 289/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1285 - accuracy: 0.9515 - val_loss: 0.1569 - val_accuracy: 0.9414\n",
      "Epoch 290/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1285 - accuracy: 0.9517 - val_loss: 0.1568 - val_accuracy: 0.9416\n",
      "Epoch 291/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1284 - accuracy: 0.9516 - val_loss: 0.1567 - val_accuracy: 0.9415\n",
      "Epoch 292/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1282 - accuracy: 0.9515 - val_loss: 0.1564 - val_accuracy: 0.9418\n",
      "Epoch 293/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1282 - accuracy: 0.9516 - val_loss: 0.1563 - val_accuracy: 0.9413\n",
      "Epoch 294/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1281 - accuracy: 0.9516 - val_loss: 0.1565 - val_accuracy: 0.9417\n",
      "Epoch 295/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1281 - accuracy: 0.9517 - val_loss: 0.1566 - val_accuracy: 0.9412\n",
      "Epoch 296/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1278 - accuracy: 0.9519 - val_loss: 0.1574 - val_accuracy: 0.9411\n",
      "Epoch 297/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1278 - accuracy: 0.9518 - val_loss: 0.1565 - val_accuracy: 0.9415\n",
      "Epoch 298/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1277 - accuracy: 0.9519 - val_loss: 0.1562 - val_accuracy: 0.9414\n",
      "Epoch 299/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1276 - accuracy: 0.9517 - val_loss: 0.1557 - val_accuracy: 0.9416\n",
      "Epoch 300/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1275 - accuracy: 0.9518 - val_loss: 0.1566 - val_accuracy: 0.9417\n",
      "Epoch 301/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1274 - accuracy: 0.9516 - val_loss: 0.1561 - val_accuracy: 0.9416\n",
      "Epoch 302/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1274 - accuracy: 0.9518 - val_loss: 0.1559 - val_accuracy: 0.9418\n",
      "Epoch 303/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1273 - accuracy: 0.9520 - val_loss: 0.1568 - val_accuracy: 0.9416\n",
      "Epoch 304/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1271 - accuracy: 0.9519 - val_loss: 0.1562 - val_accuracy: 0.9418\n",
      "Epoch 305/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1270 - accuracy: 0.9520 - val_loss: 0.1564 - val_accuracy: 0.9417\n",
      "Epoch 306/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1269 - accuracy: 0.9520 - val_loss: 0.1569 - val_accuracy: 0.9420\n",
      "Epoch 307/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1268 - accuracy: 0.9518 - val_loss: 0.1562 - val_accuracy: 0.9419\n",
      "Epoch 308/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1268 - accuracy: 0.9520 - val_loss: 0.1560 - val_accuracy: 0.9415\n",
      "Epoch 309/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1268 - accuracy: 0.9521 - val_loss: 0.1562 - val_accuracy: 0.9415\n",
      "Epoch 310/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1266 - accuracy: 0.9520 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
      "Epoch 311/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1265 - accuracy: 0.9520 - val_loss: 0.1559 - val_accuracy: 0.9415\n",
      "Epoch 312/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1264 - accuracy: 0.9521 - val_loss: 0.1561 - val_accuracy: 0.9420\n",
      "Epoch 313/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1263 - accuracy: 0.9520 - val_loss: 0.1557 - val_accuracy: 0.9423\n",
      "Epoch 314/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1263 - accuracy: 0.9520 - val_loss: 0.1556 - val_accuracy: 0.9419\n",
      "Epoch 315/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1261 - accuracy: 0.9521 - val_loss: 0.1571 - val_accuracy: 0.9416\n",
      "Epoch 316/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1260 - accuracy: 0.9524 - val_loss: 0.1556 - val_accuracy: 0.9418\n",
      "Epoch 317/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1259 - accuracy: 0.9523 - val_loss: 0.1555 - val_accuracy: 0.9417\n",
      "Epoch 318/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1258 - accuracy: 0.9524 - val_loss: 0.1564 - val_accuracy: 0.9416\n",
      "Epoch 319/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1258 - accuracy: 0.9524 - val_loss: 0.1560 - val_accuracy: 0.9416\n",
      "Epoch 320/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1257 - accuracy: 0.9524 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 321/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1256 - accuracy: 0.9525 - val_loss: 0.1560 - val_accuracy: 0.9413\n",
      "Epoch 322/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1256 - accuracy: 0.9523 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
      "Epoch 323/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1254 - accuracy: 0.9525 - val_loss: 0.1558 - val_accuracy: 0.9414\n",
      "Epoch 324/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1253 - accuracy: 0.9527 - val_loss: 0.1558 - val_accuracy: 0.9418\n",
      "Epoch 325/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1253 - accuracy: 0.9524 - val_loss: 0.1556 - val_accuracy: 0.9417\n",
      "Epoch 326/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1252 - accuracy: 0.9524 - val_loss: 0.1558 - val_accuracy: 0.9418\n",
      "Epoch 327/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1250 - accuracy: 0.9525 - val_loss: 0.1560 - val_accuracy: 0.9419\n",
      "Epoch 328/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1250 - accuracy: 0.9523 - val_loss: 0.1556 - val_accuracy: 0.9418\n",
      "Epoch 329/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1249 - accuracy: 0.9526 - val_loss: 0.1556 - val_accuracy: 0.9414\n",
      "Epoch 330/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1248 - accuracy: 0.9526 - val_loss: 0.1552 - val_accuracy: 0.9419\n",
      "Epoch 331/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1247 - accuracy: 0.9527 - val_loss: 0.1554 - val_accuracy: 0.9416\n",
      "Epoch 332/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1246 - accuracy: 0.9525 - val_loss: 0.1557 - val_accuracy: 0.9417\n",
      "Epoch 333/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1245 - accuracy: 0.9527 - val_loss: 0.1551 - val_accuracy: 0.9416\n",
      "Epoch 334/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1245 - accuracy: 0.9527 - val_loss: 0.1558 - val_accuracy: 0.9419\n",
      "Epoch 335/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1244 - accuracy: 0.9527 - val_loss: 0.1556 - val_accuracy: 0.9418\n",
      "Epoch 336/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1243 - accuracy: 0.9529 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
      "Epoch 337/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1242 - accuracy: 0.9526 - val_loss: 0.1557 - val_accuracy: 0.9416\n",
      "Epoch 338/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1242 - accuracy: 0.9529 - val_loss: 0.1552 - val_accuracy: 0.9419\n",
      "Epoch 339/800\n",
      "6000/6000 [==============================] - 1s 101us/step - loss: 0.1242 - accuracy: 0.9528 - val_loss: 0.1557 - val_accuracy: 0.9419\n",
      "Epoch 340/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1240 - accuracy: 0.9529 - val_loss: 0.1558 - val_accuracy: 0.9416\n",
      "Epoch 341/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1239 - accuracy: 0.9527 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 342/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1238 - accuracy: 0.9529 - val_loss: 0.1563 - val_accuracy: 0.9413\n",
      "Epoch 343/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.1237 - accuracy: 0.9529 - val_loss: 0.1552 - val_accuracy: 0.9420\n",
      "Epoch 344/800\n",
      "6000/6000 [==============================] - 1s 100us/step - loss: 0.1237 - accuracy: 0.9528 - val_loss: 0.1549 - val_accuracy: 0.9419\n",
      "Epoch 345/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1236 - accuracy: 0.9530 - val_loss: 0.1554 - val_accuracy: 0.9417\n",
      "Epoch 346/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1234 - accuracy: 0.9529 - val_loss: 0.1555 - val_accuracy: 0.9423\n",
      "Epoch 347/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1235 - accuracy: 0.9527 - val_loss: 0.1551 - val_accuracy: 0.9416\n",
      "Epoch 348/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1234 - accuracy: 0.9531 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
      "Epoch 349/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1233 - accuracy: 0.9529 - val_loss: 0.1552 - val_accuracy: 0.9420\n",
      "Epoch 350/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1231 - accuracy: 0.9529 - val_loss: 0.1550 - val_accuracy: 0.9417\n",
      "Epoch 351/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1232 - accuracy: 0.9531 - val_loss: 0.1551 - val_accuracy: 0.9417\n",
      "Epoch 352/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1231 - accuracy: 0.9531 - val_loss: 0.1549 - val_accuracy: 0.9419\n",
      "Epoch 353/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1230 - accuracy: 0.9532 - val_loss: 0.1554 - val_accuracy: 0.9419\n",
      "Epoch 354/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1229 - accuracy: 0.9532 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 355/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1228 - accuracy: 0.9532 - val_loss: 0.1553 - val_accuracy: 0.9416\n",
      "Epoch 356/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1226 - accuracy: 0.9530 - val_loss: 0.1561 - val_accuracy: 0.9414\n",
      "Epoch 357/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1227 - accuracy: 0.9532 - val_loss: 0.1551 - val_accuracy: 0.9415\n",
      "Epoch 358/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1226 - accuracy: 0.9529 - val_loss: 0.1551 - val_accuracy: 0.9413\n",
      "Epoch 359/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1224 - accuracy: 0.9532 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 360/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1224 - accuracy: 0.9533 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 361/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1223 - accuracy: 0.9531 - val_loss: 0.1558 - val_accuracy: 0.9413\n",
      "Epoch 362/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1222 - accuracy: 0.9534 - val_loss: 0.1550 - val_accuracy: 0.9417\n",
      "Epoch 363/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1221 - accuracy: 0.9532 - val_loss: 0.1554 - val_accuracy: 0.9421\n",
      "Epoch 364/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1220 - accuracy: 0.9535 - val_loss: 0.1548 - val_accuracy: 0.9418\n",
      "Epoch 365/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1220 - accuracy: 0.9532 - val_loss: 0.1549 - val_accuracy: 0.9421\n",
      "Epoch 366/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1219 - accuracy: 0.9534 - val_loss: 0.1547 - val_accuracy: 0.9419\n",
      "Epoch 367/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1219 - accuracy: 0.9533 - val_loss: 0.1548 - val_accuracy: 0.9415\n",
      "Epoch 368/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1217 - accuracy: 0.9534 - val_loss: 0.1553 - val_accuracy: 0.9417\n",
      "Epoch 369/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1217 - accuracy: 0.9535 - val_loss: 0.1559 - val_accuracy: 0.9418\n",
      "Epoch 370/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1217 - accuracy: 0.9537 - val_loss: 0.1555 - val_accuracy: 0.9416\n",
      "Epoch 371/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1215 - accuracy: 0.9534 - val_loss: 0.1554 - val_accuracy: 0.9419\n",
      "Epoch 372/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1215 - accuracy: 0.9535 - val_loss: 0.1555 - val_accuracy: 0.9415\n",
      "Epoch 373/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1214 - accuracy: 0.9537 - val_loss: 0.1548 - val_accuracy: 0.9417\n",
      "Epoch 374/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1212 - accuracy: 0.9534 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 375/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1212 - accuracy: 0.9536 - val_loss: 0.1558 - val_accuracy: 0.9411\n",
      "Epoch 376/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1211 - accuracy: 0.9534 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
      "Epoch 377/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1211 - accuracy: 0.9536 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 378/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1210 - accuracy: 0.9536 - val_loss: 0.1551 - val_accuracy: 0.9419\n",
      "Epoch 379/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1210 - accuracy: 0.9535 - val_loss: 0.1548 - val_accuracy: 0.9422\n",
      "Epoch 380/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1208 - accuracy: 0.9536 - val_loss: 0.1554 - val_accuracy: 0.9413\n",
      "Epoch 381/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1208 - accuracy: 0.9539 - val_loss: 0.1551 - val_accuracy: 0.9422\n",
      "Epoch 382/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1207 - accuracy: 0.9536 - val_loss: 0.1555 - val_accuracy: 0.9417\n",
      "Epoch 383/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1206 - accuracy: 0.9536 - val_loss: 0.1551 - val_accuracy: 0.9417\n",
      "Epoch 384/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1205 - accuracy: 0.9538 - val_loss: 0.1551 - val_accuracy: 0.9422\n",
      "Epoch 385/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1205 - accuracy: 0.9538 - val_loss: 0.1552 - val_accuracy: 0.9419\n",
      "Epoch 386/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1204 - accuracy: 0.9539 - val_loss: 0.1547 - val_accuracy: 0.9415\n",
      "Epoch 387/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1203 - accuracy: 0.9538 - val_loss: 0.1546 - val_accuracy: 0.9414\n",
      "Epoch 388/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1203 - accuracy: 0.9537 - val_loss: 0.1552 - val_accuracy: 0.9411\n",
      "Epoch 389/800\n",
      "6000/6000 [==============================] - 1s 104us/step - loss: 0.1201 - accuracy: 0.9539 - val_loss: 0.1556 - val_accuracy: 0.9414\n",
      "Epoch 390/800\n",
      "6000/6000 [==============================] - 1s 102us/step - loss: 0.1201 - accuracy: 0.9538 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 391/800\n",
      "6000/6000 [==============================] - 1s 101us/step - loss: 0.1199 - accuracy: 0.9542 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 392/800\n",
      "6000/6000 [==============================] - 1s 102us/step - loss: 0.1200 - accuracy: 0.9539 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 393/800\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 0.1199 - accuracy: 0.9538 - val_loss: 0.1554 - val_accuracy: 0.9416\n",
      "Epoch 394/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1198 - accuracy: 0.9540 - val_loss: 0.1552 - val_accuracy: 0.9419\n",
      "Epoch 395/800\n",
      "6000/6000 [==============================] - 1s 101us/step - loss: 0.1197 - accuracy: 0.9540 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 396/800\n",
      "6000/6000 [==============================] - 1s 95us/step - loss: 0.1197 - accuracy: 0.9539 - val_loss: 0.1551 - val_accuracy: 0.9417\n",
      "Epoch 397/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1196 - accuracy: 0.9538 - val_loss: 0.1548 - val_accuracy: 0.9415\n",
      "Epoch 398/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1195 - accuracy: 0.9539 - val_loss: 0.1551 - val_accuracy: 0.9418\n",
      "Epoch 399/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1195 - accuracy: 0.9542 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
      "Epoch 400/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1193 - accuracy: 0.9540 - val_loss: 0.1548 - val_accuracy: 0.9418\n",
      "Epoch 401/800\n",
      "6000/6000 [==============================] - 1s 97us/step - loss: 0.1193 - accuracy: 0.9542 - val_loss: 0.1545 - val_accuracy: 0.9421\n",
      "Epoch 402/800\n",
      "6000/6000 [==============================] - 1s 98us/step - loss: 0.1192 - accuracy: 0.9541 - val_loss: 0.1553 - val_accuracy: 0.9412\n",
      "Epoch 403/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1192 - accuracy: 0.9539 - val_loss: 0.1549 - val_accuracy: 0.9417\n",
      "Epoch 404/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1190 - accuracy: 0.9543 - val_loss: 0.1548 - val_accuracy: 0.9419\n",
      "Epoch 405/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1190 - accuracy: 0.9541 - val_loss: 0.1548 - val_accuracy: 0.9415\n",
      "Epoch 406/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1189 - accuracy: 0.9542 - val_loss: 0.1551 - val_accuracy: 0.9416\n",
      "Epoch 407/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1189 - accuracy: 0.9543 - val_loss: 0.1549 - val_accuracy: 0.9417\n",
      "Epoch 408/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1188 - accuracy: 0.9544 - val_loss: 0.1549 - val_accuracy: 0.9421\n",
      "Epoch 409/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1188 - accuracy: 0.9542 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 410/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1186 - accuracy: 0.9545 - val_loss: 0.1560 - val_accuracy: 0.9411\n",
      "Epoch 411/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1185 - accuracy: 0.9544 - val_loss: 0.1555 - val_accuracy: 0.9413\n",
      "Epoch 412/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1185 - accuracy: 0.9544 - val_loss: 0.1552 - val_accuracy: 0.9413\n",
      "Epoch 413/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1185 - accuracy: 0.9542 - val_loss: 0.1554 - val_accuracy: 0.9414\n",
      "Epoch 414/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1184 - accuracy: 0.9544 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 415/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1182 - accuracy: 0.9542 - val_loss: 0.1550 - val_accuracy: 0.9414\n",
      "Epoch 416/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1183 - accuracy: 0.9543 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 417/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1181 - accuracy: 0.9547 - val_loss: 0.1559 - val_accuracy: 0.9418\n",
      "Epoch 418/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1180 - accuracy: 0.9545 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 419/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1180 - accuracy: 0.9544 - val_loss: 0.1553 - val_accuracy: 0.9415\n",
      "Epoch 420/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1180 - accuracy: 0.9543 - val_loss: 0.1546 - val_accuracy: 0.9419\n",
      "Epoch 421/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1178 - accuracy: 0.9546 - val_loss: 0.1543 - val_accuracy: 0.9419\n",
      "Epoch 422/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1177 - accuracy: 0.9546 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 423/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1177 - accuracy: 0.9546 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 424/800\n",
      "6000/6000 [==============================] - 1s 96us/step - loss: 0.1176 - accuracy: 0.9545 - val_loss: 0.1548 - val_accuracy: 0.9416\n",
      "Epoch 425/800\n",
      "6000/6000 [==============================] - 1s 99us/step - loss: 0.1175 - accuracy: 0.9546 - val_loss: 0.1544 - val_accuracy: 0.9414\n",
      "Epoch 426/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1175 - accuracy: 0.9547 - val_loss: 0.1550 - val_accuracy: 0.9418\n",
      "Epoch 427/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1174 - accuracy: 0.9547 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 428/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.1174 - accuracy: 0.9548 - val_loss: 0.1548 - val_accuracy: 0.9419\n",
      "Epoch 429/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1173 - accuracy: 0.9548 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 430/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1172 - accuracy: 0.9546 - val_loss: 0.1552 - val_accuracy: 0.9421\n",
      "Epoch 431/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1171 - accuracy: 0.9549 - val_loss: 0.1551 - val_accuracy: 0.9419\n",
      "Epoch 432/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1171 - accuracy: 0.9549 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
      "Epoch 433/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1170 - accuracy: 0.9549 - val_loss: 0.1552 - val_accuracy: 0.9416\n",
      "Epoch 434/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1169 - accuracy: 0.9552 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 435/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1168 - accuracy: 0.9549 - val_loss: 0.1552 - val_accuracy: 0.9419\n",
      "Epoch 436/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1168 - accuracy: 0.9550 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 437/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1168 - accuracy: 0.9546 - val_loss: 0.1545 - val_accuracy: 0.9414\n",
      "Epoch 438/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.1166 - accuracy: 0.9549 - val_loss: 0.1549 - val_accuracy: 0.9418\n",
      "Epoch 439/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1166 - accuracy: 0.9550 - val_loss: 0.1551 - val_accuracy: 0.9418\n",
      "Epoch 440/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1165 - accuracy: 0.9549 - val_loss: 0.1558 - val_accuracy: 0.9415\n",
      "Epoch 441/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1164 - accuracy: 0.9550 - val_loss: 0.1544 - val_accuracy: 0.9419\n",
      "Epoch 442/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1164 - accuracy: 0.9550 - val_loss: 0.1551 - val_accuracy: 0.9411\n",
      "Epoch 443/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1163 - accuracy: 0.9549 - val_loss: 0.1551 - val_accuracy: 0.9419\n",
      "Epoch 444/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1161 - accuracy: 0.9551 - val_loss: 0.1550 - val_accuracy: 0.9421\n",
      "Epoch 445/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1162 - accuracy: 0.9552 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
      "Epoch 446/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1160 - accuracy: 0.9551 - val_loss: 0.1557 - val_accuracy: 0.9414\n",
      "Epoch 447/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1160 - accuracy: 0.9552 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 448/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1160 - accuracy: 0.9552 - val_loss: 0.1560 - val_accuracy: 0.9413\n",
      "Epoch 449/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1159 - accuracy: 0.9550 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
      "Epoch 450/800\n",
      "6000/6000 [==============================] - 1s 94us/step - loss: 0.1158 - accuracy: 0.9552 - val_loss: 0.1551 - val_accuracy: 0.9416\n",
      "Epoch 451/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1158 - accuracy: 0.9551 - val_loss: 0.1559 - val_accuracy: 0.9416\n",
      "Epoch 452/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1158 - accuracy: 0.9552 - val_loss: 0.1554 - val_accuracy: 0.9415\n",
      "Epoch 453/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1156 - accuracy: 0.9552 - val_loss: 0.1557 - val_accuracy: 0.9419\n",
      "Epoch 454/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1155 - accuracy: 0.9554 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 455/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1156 - accuracy: 0.9551 - val_loss: 0.1550 - val_accuracy: 0.9422\n",
      "Epoch 456/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1154 - accuracy: 0.9552 - val_loss: 0.1553 - val_accuracy: 0.9416\n",
      "Epoch 457/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1154 - accuracy: 0.9553 - val_loss: 0.1553 - val_accuracy: 0.9418\n",
      "Epoch 458/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1152 - accuracy: 0.9554 - val_loss: 0.1554 - val_accuracy: 0.9419\n",
      "Epoch 459/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1152 - accuracy: 0.9554 - val_loss: 0.1556 - val_accuracy: 0.9417\n",
      "Epoch 460/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1151 - accuracy: 0.9553 - val_loss: 0.1561 - val_accuracy: 0.9412\n",
      "Epoch 461/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1150 - accuracy: 0.9555 - val_loss: 0.1554 - val_accuracy: 0.9418\n",
      "Epoch 462/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1150 - accuracy: 0.9555 - val_loss: 0.1558 - val_accuracy: 0.9420\n",
      "Epoch 463/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1149 - accuracy: 0.9555 - val_loss: 0.1551 - val_accuracy: 0.9413\n",
      "Epoch 464/800\n",
      "6000/6000 [==============================] - 1s 95us/step - loss: 0.1149 - accuracy: 0.9556 - val_loss: 0.1551 - val_accuracy: 0.9412\n",
      "Epoch 465/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1148 - accuracy: 0.9556 - val_loss: 0.1557 - val_accuracy: 0.9419\n",
      "Epoch 466/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1148 - accuracy: 0.9555 - val_loss: 0.1559 - val_accuracy: 0.9414\n",
      "Epoch 467/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1146 - accuracy: 0.9555 - val_loss: 0.1552 - val_accuracy: 0.9418\n",
      "Epoch 468/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1146 - accuracy: 0.9556 - val_loss: 0.1556 - val_accuracy: 0.9416\n",
      "Epoch 469/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1145 - accuracy: 0.9557 - val_loss: 0.1555 - val_accuracy: 0.9416\n",
      "Epoch 470/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1144 - accuracy: 0.9556 - val_loss: 0.1556 - val_accuracy: 0.9420\n",
      "Epoch 471/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1142 - accuracy: 0.9558 - val_loss: 0.1557 - val_accuracy: 0.9410\n",
      "Epoch 472/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1143 - accuracy: 0.9558 - val_loss: 0.1562 - val_accuracy: 0.9409\n",
      "Epoch 473/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1143 - accuracy: 0.9557 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 474/800\n",
      "6000/6000 [==============================] - 1s 95us/step - loss: 0.1141 - accuracy: 0.9557 - val_loss: 0.1566 - val_accuracy: 0.9409\n",
      "Epoch 475/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1140 - accuracy: 0.9558 - val_loss: 0.1570 - val_accuracy: 0.9410\n",
      "Epoch 476/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1140 - accuracy: 0.9558 - val_loss: 0.1555 - val_accuracy: 0.9415\n",
      "Epoch 477/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1139 - accuracy: 0.9559 - val_loss: 0.1558 - val_accuracy: 0.9419\n",
      "Epoch 478/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1139 - accuracy: 0.9558 - val_loss: 0.1553 - val_accuracy: 0.9414\n",
      "Epoch 479/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1138 - accuracy: 0.9558 - val_loss: 0.1552 - val_accuracy: 0.9420\n",
      "Epoch 480/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1138 - accuracy: 0.9559 - val_loss: 0.1552 - val_accuracy: 0.9421\n",
      "Epoch 481/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1136 - accuracy: 0.9558 - val_loss: 0.1555 - val_accuracy: 0.9421\n",
      "Epoch 482/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1135 - accuracy: 0.9559 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
      "Epoch 483/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1135 - accuracy: 0.9559 - val_loss: 0.1557 - val_accuracy: 0.9415\n",
      "Epoch 484/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1135 - accuracy: 0.9557 - val_loss: 0.1554 - val_accuracy: 0.9413\n",
      "Epoch 485/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1135 - accuracy: 0.9560 - val_loss: 0.1556 - val_accuracy: 0.9419\n",
      "Epoch 486/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1132 - accuracy: 0.9561 - val_loss: 0.1560 - val_accuracy: 0.9420\n",
      "Epoch 487/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1132 - accuracy: 0.9561 - val_loss: 0.1555 - val_accuracy: 0.9411\n",
      "Epoch 488/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1132 - accuracy: 0.9561 - val_loss: 0.1560 - val_accuracy: 0.9415\n",
      "Epoch 489/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1132 - accuracy: 0.9562 - val_loss: 0.1562 - val_accuracy: 0.9409\n",
      "Epoch 490/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1131 - accuracy: 0.9558 - val_loss: 0.1560 - val_accuracy: 0.9415\n",
      "Epoch 491/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1130 - accuracy: 0.9561 - val_loss: 0.1559 - val_accuracy: 0.9415\n",
      "Epoch 492/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1129 - accuracy: 0.9561 - val_loss: 0.1565 - val_accuracy: 0.9414\n",
      "Epoch 493/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1129 - accuracy: 0.9562 - val_loss: 0.1557 - val_accuracy: 0.9419\n",
      "Epoch 494/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1128 - accuracy: 0.9561 - val_loss: 0.1556 - val_accuracy: 0.9416\n",
      "Epoch 495/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1127 - accuracy: 0.9561 - val_loss: 0.1557 - val_accuracy: 0.9415\n",
      "Epoch 496/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1127 - accuracy: 0.9563 - val_loss: 0.1559 - val_accuracy: 0.9417\n",
      "Epoch 497/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1126 - accuracy: 0.9562 - val_loss: 0.1565 - val_accuracy: 0.9413\n",
      "Epoch 498/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1125 - accuracy: 0.9564 - val_loss: 0.1564 - val_accuracy: 0.9414\n",
      "Epoch 499/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1124 - accuracy: 0.9563 - val_loss: 0.1580 - val_accuracy: 0.9402\n",
      "Epoch 500/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1123 - accuracy: 0.9564 - val_loss: 0.1563 - val_accuracy: 0.9418\n",
      "Epoch 501/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1124 - accuracy: 0.9563 - val_loss: 0.1575 - val_accuracy: 0.9418\n",
      "Epoch 502/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1123 - accuracy: 0.9563 - val_loss: 0.1557 - val_accuracy: 0.9414\n",
      "Epoch 503/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1121 - accuracy: 0.9564 - val_loss: 0.1558 - val_accuracy: 0.9415\n",
      "Epoch 504/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1122 - accuracy: 0.9563 - val_loss: 0.1553 - val_accuracy: 0.9419\n",
      "Epoch 505/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1120 - accuracy: 0.9566 - val_loss: 0.1562 - val_accuracy: 0.9415\n",
      "Epoch 506/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1121 - accuracy: 0.9565 - val_loss: 0.1569 - val_accuracy: 0.9412\n",
      "Epoch 507/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1119 - accuracy: 0.9565 - val_loss: 0.1561 - val_accuracy: 0.9412\n",
      "Epoch 508/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1118 - accuracy: 0.9566 - val_loss: 0.1562 - val_accuracy: 0.9409\n",
      "Epoch 509/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1117 - accuracy: 0.9564 - val_loss: 0.1562 - val_accuracy: 0.9413\n",
      "Epoch 510/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1117 - accuracy: 0.9568 - val_loss: 0.1557 - val_accuracy: 0.9418\n",
      "Epoch 511/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1118 - accuracy: 0.9567 - val_loss: 0.1556 - val_accuracy: 0.9414\n",
      "Epoch 512/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1116 - accuracy: 0.9567 - val_loss: 0.1566 - val_accuracy: 0.9415\n",
      "Epoch 513/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1115 - accuracy: 0.9566 - val_loss: 0.1569 - val_accuracy: 0.9417\n",
      "Epoch 514/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1115 - accuracy: 0.9567 - val_loss: 0.1567 - val_accuracy: 0.9407\n",
      "Epoch 515/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1114 - accuracy: 0.9565 - val_loss: 0.1573 - val_accuracy: 0.9405\n",
      "Epoch 516/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1113 - accuracy: 0.9567 - val_loss: 0.1562 - val_accuracy: 0.9413\n",
      "Epoch 517/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1112 - accuracy: 0.9568 - val_loss: 0.1564 - val_accuracy: 0.9415\n",
      "Epoch 518/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1112 - accuracy: 0.9565 - val_loss: 0.1564 - val_accuracy: 0.9412\n",
      "Epoch 519/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1110 - accuracy: 0.9569 - val_loss: 0.1568 - val_accuracy: 0.9406\n",
      "Epoch 520/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1111 - accuracy: 0.9570 - val_loss: 0.1568 - val_accuracy: 0.9414\n",
      "Epoch 521/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1111 - accuracy: 0.9567 - val_loss: 0.1560 - val_accuracy: 0.9418\n",
      "Epoch 522/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1109 - accuracy: 0.9568 - val_loss: 0.1568 - val_accuracy: 0.9414\n",
      "Epoch 523/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1109 - accuracy: 0.9569 - val_loss: 0.1561 - val_accuracy: 0.9413\n",
      "Epoch 524/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1107 - accuracy: 0.9570 - val_loss: 0.1567 - val_accuracy: 0.9414\n",
      "Epoch 525/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1108 - accuracy: 0.9570 - val_loss: 0.1564 - val_accuracy: 0.9414\n",
      "Epoch 526/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1107 - accuracy: 0.9570 - val_loss: 0.1560 - val_accuracy: 0.9409\n",
      "Epoch 527/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1106 - accuracy: 0.9569 - val_loss: 0.1568 - val_accuracy: 0.9415\n",
      "Epoch 528/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1106 - accuracy: 0.9568 - val_loss: 0.1568 - val_accuracy: 0.9415\n",
      "Epoch 529/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1104 - accuracy: 0.9569 - val_loss: 0.1574 - val_accuracy: 0.9409\n",
      "Epoch 530/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1104 - accuracy: 0.9571 - val_loss: 0.1570 - val_accuracy: 0.9411\n",
      "Epoch 531/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1102 - accuracy: 0.9572 - val_loss: 0.1571 - val_accuracy: 0.9414\n",
      "Epoch 532/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1103 - accuracy: 0.9571 - val_loss: 0.1573 - val_accuracy: 0.9419\n",
      "Epoch 533/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1103 - accuracy: 0.9569 - val_loss: 0.1579 - val_accuracy: 0.9413\n",
      "Epoch 534/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1101 - accuracy: 0.9570 - val_loss: 0.1572 - val_accuracy: 0.9411\n",
      "Epoch 535/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1100 - accuracy: 0.9573 - val_loss: 0.1574 - val_accuracy: 0.9420\n",
      "Epoch 536/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1101 - accuracy: 0.9572 - val_loss: 0.1572 - val_accuracy: 0.9406\n",
      "Epoch 537/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1099 - accuracy: 0.9572 - val_loss: 0.1572 - val_accuracy: 0.9416\n",
      "Epoch 538/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1099 - accuracy: 0.9571 - val_loss: 0.1575 - val_accuracy: 0.9407\n",
      "Epoch 539/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1098 - accuracy: 0.9572 - val_loss: 0.1563 - val_accuracy: 0.9413\n",
      "Epoch 540/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1098 - accuracy: 0.9571 - val_loss: 0.1569 - val_accuracy: 0.9410\n",
      "Epoch 541/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1097 - accuracy: 0.9572 - val_loss: 0.1567 - val_accuracy: 0.9414\n",
      "Epoch 542/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1096 - accuracy: 0.9573 - val_loss: 0.1574 - val_accuracy: 0.9418\n",
      "Epoch 543/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1096 - accuracy: 0.9573 - val_loss: 0.1578 - val_accuracy: 0.9408\n",
      "Epoch 544/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1093 - accuracy: 0.9571 - val_loss: 0.1575 - val_accuracy: 0.9418\n",
      "Epoch 545/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1093 - accuracy: 0.9576 - val_loss: 0.1572 - val_accuracy: 0.9412\n",
      "Epoch 546/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1093 - accuracy: 0.9574 - val_loss: 0.1571 - val_accuracy: 0.9406\n",
      "Epoch 547/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1094 - accuracy: 0.9573 - val_loss: 0.1571 - val_accuracy: 0.9409\n",
      "Epoch 548/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1090 - accuracy: 0.9575 - val_loss: 0.1579 - val_accuracy: 0.9409\n",
      "Epoch 549/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1091 - accuracy: 0.9574 - val_loss: 0.1577 - val_accuracy: 0.9416\n",
      "Epoch 550/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1091 - accuracy: 0.9574 - val_loss: 0.1571 - val_accuracy: 0.9415\n",
      "Epoch 551/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1091 - accuracy: 0.9574 - val_loss: 0.1571 - val_accuracy: 0.9411\n",
      "Epoch 552/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1089 - accuracy: 0.9574 - val_loss: 0.1581 - val_accuracy: 0.9412\n",
      "Epoch 553/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1089 - accuracy: 0.9575 - val_loss: 0.1572 - val_accuracy: 0.9411\n",
      "Epoch 554/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1088 - accuracy: 0.9576 - val_loss: 0.1580 - val_accuracy: 0.9415\n",
      "Epoch 555/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1088 - accuracy: 0.9578 - val_loss: 0.1577 - val_accuracy: 0.9410\n",
      "Epoch 556/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1087 - accuracy: 0.9575 - val_loss: 0.1570 - val_accuracy: 0.9416\n",
      "Epoch 557/800\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.1085 - accuracy: 0.9577 - val_loss: 0.1575 - val_accuracy: 0.9412\n",
      "Epoch 558/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1086 - accuracy: 0.9577 - val_loss: 0.1585 - val_accuracy: 0.9412\n",
      "Epoch 559/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1085 - accuracy: 0.9576 - val_loss: 0.1586 - val_accuracy: 0.9409\n",
      "Epoch 560/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1085 - accuracy: 0.9578 - val_loss: 0.1576 - val_accuracy: 0.9410\n",
      "Epoch 561/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1083 - accuracy: 0.9578 - val_loss: 0.1578 - val_accuracy: 0.9410\n",
      "Epoch 562/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1082 - accuracy: 0.9578 - val_loss: 0.1579 - val_accuracy: 0.9408\n",
      "Epoch 563/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1080 - accuracy: 0.9580 - val_loss: 0.1578 - val_accuracy: 0.9413\n",
      "Epoch 564/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1082 - accuracy: 0.9580 - val_loss: 0.1580 - val_accuracy: 0.9409\n",
      "Epoch 565/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1081 - accuracy: 0.9579 - val_loss: 0.1578 - val_accuracy: 0.9410\n",
      "Epoch 566/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1080 - accuracy: 0.9578 - val_loss: 0.1585 - val_accuracy: 0.9407\n",
      "Epoch 567/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1078 - accuracy: 0.9579 - val_loss: 0.1587 - val_accuracy: 0.9401\n",
      "Epoch 568/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1079 - accuracy: 0.9580 - val_loss: 0.1584 - val_accuracy: 0.9418\n",
      "Epoch 569/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1078 - accuracy: 0.9582 - val_loss: 0.1584 - val_accuracy: 0.9414\n",
      "Epoch 570/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1078 - accuracy: 0.9582 - val_loss: 0.1580 - val_accuracy: 0.9411\n",
      "Epoch 571/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1077 - accuracy: 0.9581 - val_loss: 0.1578 - val_accuracy: 0.9417\n",
      "Epoch 572/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1077 - accuracy: 0.9580 - val_loss: 0.1580 - val_accuracy: 0.9412\n",
      "Epoch 573/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1076 - accuracy: 0.9580 - val_loss: 0.1583 - val_accuracy: 0.9411\n",
      "Epoch 574/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1075 - accuracy: 0.9582 - val_loss: 0.1590 - val_accuracy: 0.9414\n",
      "Epoch 575/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1073 - accuracy: 0.9583 - val_loss: 0.1583 - val_accuracy: 0.9415\n",
      "Epoch 576/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1073 - accuracy: 0.9581 - val_loss: 0.1586 - val_accuracy: 0.9409\n",
      "Epoch 577/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1073 - accuracy: 0.9581 - val_loss: 0.1593 - val_accuracy: 0.9404\n",
      "Epoch 578/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1073 - accuracy: 0.9581 - val_loss: 0.1585 - val_accuracy: 0.9406\n",
      "Epoch 579/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1071 - accuracy: 0.9581 - val_loss: 0.1591 - val_accuracy: 0.9409\n",
      "Epoch 580/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1071 - accuracy: 0.9580 - val_loss: 0.1592 - val_accuracy: 0.9404\n",
      "Epoch 581/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1071 - accuracy: 0.9582 - val_loss: 0.1580 - val_accuracy: 0.9411\n",
      "Epoch 582/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1070 - accuracy: 0.9581 - val_loss: 0.1581 - val_accuracy: 0.9409\n",
      "Epoch 583/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.1590 - val_accuracy: 0.9410\n",
      "Epoch 584/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1068 - accuracy: 0.9585 - val_loss: 0.1583 - val_accuracy: 0.9405\n",
      "Epoch 585/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.1595 - val_accuracy: 0.9406\n",
      "Epoch 586/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1066 - accuracy: 0.9585 - val_loss: 0.1587 - val_accuracy: 0.9405\n",
      "Epoch 587/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1066 - accuracy: 0.9583 - val_loss: 0.1593 - val_accuracy: 0.9407\n",
      "Epoch 588/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1065 - accuracy: 0.9583 - val_loss: 0.1582 - val_accuracy: 0.9406\n",
      "Epoch 589/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1064 - accuracy: 0.9585 - val_loss: 0.1596 - val_accuracy: 0.9402\n",
      "Epoch 590/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1063 - accuracy: 0.9586 - val_loss: 0.1591 - val_accuracy: 0.9412\n",
      "Epoch 591/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1064 - accuracy: 0.9584 - val_loss: 0.1598 - val_accuracy: 0.9406\n",
      "Epoch 592/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1063 - accuracy: 0.9585 - val_loss: 0.1597 - val_accuracy: 0.9408\n",
      "Epoch 593/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1062 - accuracy: 0.9585 - val_loss: 0.1595 - val_accuracy: 0.9407\n",
      "Epoch 594/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1062 - accuracy: 0.9585 - val_loss: 0.1596 - val_accuracy: 0.9406\n",
      "Epoch 595/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1061 - accuracy: 0.9584 - val_loss: 0.1592 - val_accuracy: 0.9411\n",
      "Epoch 596/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1061 - accuracy: 0.9586 - val_loss: 0.1591 - val_accuracy: 0.9411\n",
      "Epoch 597/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1059 - accuracy: 0.9587 - val_loss: 0.1592 - val_accuracy: 0.9412\n",
      "Epoch 598/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1058 - accuracy: 0.9585 - val_loss: 0.1601 - val_accuracy: 0.9411\n",
      "Epoch 599/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1058 - accuracy: 0.9587 - val_loss: 0.1597 - val_accuracy: 0.9410\n",
      "Epoch 600/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1058 - accuracy: 0.9586 - val_loss: 0.1597 - val_accuracy: 0.9408\n",
      "Epoch 601/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1058 - accuracy: 0.9587 - val_loss: 0.1599 - val_accuracy: 0.9411\n",
      "Epoch 602/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1056 - accuracy: 0.9588 - val_loss: 0.1599 - val_accuracy: 0.9403\n",
      "Epoch 603/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1055 - accuracy: 0.9585 - val_loss: 0.1592 - val_accuracy: 0.9415\n",
      "Epoch 604/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1055 - accuracy: 0.9587 - val_loss: 0.1599 - val_accuracy: 0.9406\n",
      "Epoch 605/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1054 - accuracy: 0.9587 - val_loss: 0.1601 - val_accuracy: 0.9414\n",
      "Epoch 606/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1054 - accuracy: 0.9588 - val_loss: 0.1598 - val_accuracy: 0.9406\n",
      "Epoch 607/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1052 - accuracy: 0.9588 - val_loss: 0.1598 - val_accuracy: 0.9412\n",
      "Epoch 608/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1052 - accuracy: 0.9589 - val_loss: 0.1606 - val_accuracy: 0.9411\n",
      "Epoch 609/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1051 - accuracy: 0.9589 - val_loss: 0.1608 - val_accuracy: 0.9411\n",
      "Epoch 610/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1050 - accuracy: 0.9591 - val_loss: 0.1618 - val_accuracy: 0.9411\n",
      "Epoch 611/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1050 - accuracy: 0.9591 - val_loss: 0.1602 - val_accuracy: 0.9405\n",
      "Epoch 612/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1050 - accuracy: 0.9591 - val_loss: 0.1600 - val_accuracy: 0.9412\n",
      "Epoch 613/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1049 - accuracy: 0.9591 - val_loss: 0.1618 - val_accuracy: 0.9404\n",
      "Epoch 614/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1048 - accuracy: 0.9592 - val_loss: 0.1600 - val_accuracy: 0.9406\n",
      "Epoch 615/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1049 - accuracy: 0.9589 - val_loss: 0.1592 - val_accuracy: 0.9410\n",
      "Epoch 616/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1047 - accuracy: 0.9591 - val_loss: 0.1598 - val_accuracy: 0.9410\n",
      "Epoch 617/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1047 - accuracy: 0.9589 - val_loss: 0.1610 - val_accuracy: 0.9405\n",
      "Epoch 618/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1044 - accuracy: 0.9591 - val_loss: 0.1626 - val_accuracy: 0.9404\n",
      "Epoch 619/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1045 - accuracy: 0.9593 - val_loss: 0.1607 - val_accuracy: 0.9406\n",
      "Epoch 620/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1044 - accuracy: 0.9592 - val_loss: 0.1611 - val_accuracy: 0.9406\n",
      "Epoch 621/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1044 - accuracy: 0.9592 - val_loss: 0.1617 - val_accuracy: 0.9404\n",
      "Epoch 622/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1043 - accuracy: 0.9594 - val_loss: 0.1612 - val_accuracy: 0.9402\n",
      "Epoch 623/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1043 - accuracy: 0.9593 - val_loss: 0.1601 - val_accuracy: 0.9409\n",
      "Epoch 624/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1042 - accuracy: 0.9593 - val_loss: 0.1620 - val_accuracy: 0.9403\n",
      "Epoch 625/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1040 - accuracy: 0.9594 - val_loss: 0.1603 - val_accuracy: 0.9409\n",
      "Epoch 626/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1041 - accuracy: 0.9592 - val_loss: 0.1610 - val_accuracy: 0.9404\n",
      "Epoch 627/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1040 - accuracy: 0.9593 - val_loss: 0.1608 - val_accuracy: 0.9405\n",
      "Epoch 628/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1039 - accuracy: 0.9593 - val_loss: 0.1610 - val_accuracy: 0.9408\n",
      "Epoch 629/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1038 - accuracy: 0.9594 - val_loss: 0.1624 - val_accuracy: 0.9404\n",
      "Epoch 630/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1038 - accuracy: 0.9594 - val_loss: 0.1608 - val_accuracy: 0.9411\n",
      "Epoch 631/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1038 - accuracy: 0.9595 - val_loss: 0.1610 - val_accuracy: 0.9410\n",
      "Epoch 632/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1037 - accuracy: 0.9594 - val_loss: 0.1605 - val_accuracy: 0.9408\n",
      "Epoch 633/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1035 - accuracy: 0.9596 - val_loss: 0.1615 - val_accuracy: 0.9411\n",
      "Epoch 634/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1035 - accuracy: 0.9597 - val_loss: 0.1615 - val_accuracy: 0.9398\n",
      "Epoch 635/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1034 - accuracy: 0.9596 - val_loss: 0.1611 - val_accuracy: 0.9405\n",
      "Epoch 636/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1033 - accuracy: 0.9597 - val_loss: 0.1618 - val_accuracy: 0.9407\n",
      "Epoch 637/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1032 - accuracy: 0.9597 - val_loss: 0.1623 - val_accuracy: 0.9403\n",
      "Epoch 638/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1032 - accuracy: 0.9598 - val_loss: 0.1610 - val_accuracy: 0.9408\n",
      "Epoch 639/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1031 - accuracy: 0.9597 - val_loss: 0.1611 - val_accuracy: 0.9413\n",
      "Epoch 640/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1030 - accuracy: 0.9596 - val_loss: 0.1625 - val_accuracy: 0.9401\n",
      "Epoch 641/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1030 - accuracy: 0.9596 - val_loss: 0.1625 - val_accuracy: 0.9406\n",
      "Epoch 642/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1030 - accuracy: 0.9598 - val_loss: 0.1616 - val_accuracy: 0.9408\n",
      "Epoch 643/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1028 - accuracy: 0.9598 - val_loss: 0.1614 - val_accuracy: 0.9405\n",
      "Epoch 644/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1027 - accuracy: 0.9600 - val_loss: 0.1623 - val_accuracy: 0.9399\n",
      "Epoch 645/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1028 - accuracy: 0.9600 - val_loss: 0.1614 - val_accuracy: 0.9406\n",
      "Epoch 646/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1027 - accuracy: 0.9598 - val_loss: 0.1617 - val_accuracy: 0.9403\n",
      "Epoch 647/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1027 - accuracy: 0.9598 - val_loss: 0.1624 - val_accuracy: 0.9400\n",
      "Epoch 648/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1025 - accuracy: 0.9599 - val_loss: 0.1624 - val_accuracy: 0.9409\n",
      "Epoch 649/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1027 - accuracy: 0.9597 - val_loss: 0.1625 - val_accuracy: 0.9404\n",
      "Epoch 650/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1024 - accuracy: 0.9600 - val_loss: 0.1611 - val_accuracy: 0.9405\n",
      "Epoch 651/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1024 - accuracy: 0.9599 - val_loss: 0.1626 - val_accuracy: 0.9400\n",
      "Epoch 652/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1023 - accuracy: 0.9602 - val_loss: 0.1619 - val_accuracy: 0.9413\n",
      "Epoch 653/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1022 - accuracy: 0.9600 - val_loss: 0.1620 - val_accuracy: 0.9407\n",
      "Epoch 654/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1021 - accuracy: 0.9601 - val_loss: 0.1647 - val_accuracy: 0.9389\n",
      "Epoch 655/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1021 - accuracy: 0.9601 - val_loss: 0.1632 - val_accuracy: 0.9411\n",
      "Epoch 656/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1020 - accuracy: 0.9601 - val_loss: 0.1640 - val_accuracy: 0.9399\n",
      "Epoch 657/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1019 - accuracy: 0.9600 - val_loss: 0.1636 - val_accuracy: 0.9402\n",
      "Epoch 658/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1018 - accuracy: 0.9602 - val_loss: 0.1627 - val_accuracy: 0.9407\n",
      "Epoch 659/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1018 - accuracy: 0.9601 - val_loss: 0.1631 - val_accuracy: 0.9406\n",
      "Epoch 660/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1018 - accuracy: 0.9602 - val_loss: 0.1625 - val_accuracy: 0.9402\n",
      "Epoch 661/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1016 - accuracy: 0.9603 - val_loss: 0.1636 - val_accuracy: 0.9397\n",
      "Epoch 662/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1017 - accuracy: 0.9604 - val_loss: 0.1634 - val_accuracy: 0.9408\n",
      "Epoch 663/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1016 - accuracy: 0.9603 - val_loss: 0.1629 - val_accuracy: 0.9403\n",
      "Epoch 664/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1015 - accuracy: 0.9605 - val_loss: 0.1626 - val_accuracy: 0.9402\n",
      "Epoch 665/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1014 - accuracy: 0.9606 - val_loss: 0.1637 - val_accuracy: 0.9401\n",
      "Epoch 666/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1015 - accuracy: 0.9604 - val_loss: 0.1635 - val_accuracy: 0.9402\n",
      "Epoch 667/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1014 - accuracy: 0.9601 - val_loss: 0.1641 - val_accuracy: 0.9403\n",
      "Epoch 668/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1012 - accuracy: 0.9606 - val_loss: 0.1647 - val_accuracy: 0.9402\n",
      "Epoch 669/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1013 - accuracy: 0.9604 - val_loss: 0.1635 - val_accuracy: 0.9394\n",
      "Epoch 670/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1011 - accuracy: 0.9605 - val_loss: 0.1635 - val_accuracy: 0.9397\n",
      "Epoch 671/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1011 - accuracy: 0.9606 - val_loss: 0.1656 - val_accuracy: 0.9392\n",
      "Epoch 672/800\n",
      "6000/6000 [==============================] - 1s 83us/step - loss: 0.1009 - accuracy: 0.9607 - val_loss: 0.1640 - val_accuracy: 0.9404\n",
      "Epoch 673/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1009 - accuracy: 0.9606 - val_loss: 0.1637 - val_accuracy: 0.9406\n",
      "Epoch 674/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1008 - accuracy: 0.9606 - val_loss: 0.1643 - val_accuracy: 0.9406\n",
      "Epoch 675/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1007 - accuracy: 0.9606 - val_loss: 0.1640 - val_accuracy: 0.9397\n",
      "Epoch 676/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1007 - accuracy: 0.9608 - val_loss: 0.1647 - val_accuracy: 0.9394\n",
      "Epoch 677/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1008 - accuracy: 0.9606 - val_loss: 0.1638 - val_accuracy: 0.9406\n",
      "Epoch 678/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1006 - accuracy: 0.9606 - val_loss: 0.1638 - val_accuracy: 0.9404\n",
      "Epoch 679/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1004 - accuracy: 0.9606 - val_loss: 0.1638 - val_accuracy: 0.9402\n",
      "Epoch 680/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1004 - accuracy: 0.9608 - val_loss: 0.1652 - val_accuracy: 0.9396\n",
      "Epoch 681/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1005 - accuracy: 0.9608 - val_loss: 0.1640 - val_accuracy: 0.9405\n",
      "Epoch 682/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1003 - accuracy: 0.9609 - val_loss: 0.1637 - val_accuracy: 0.9406\n",
      "Epoch 683/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1648 - val_accuracy: 0.9400\n",
      "Epoch 684/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1644 - val_accuracy: 0.9401\n",
      "Epoch 685/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1635 - val_accuracy: 0.9405\n",
      "Epoch 686/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1001 - accuracy: 0.9609 - val_loss: 0.1639 - val_accuracy: 0.9401\n",
      "Epoch 687/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1000 - accuracy: 0.9609 - val_loss: 0.1641 - val_accuracy: 0.9405\n",
      "Epoch 688/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0999 - accuracy: 0.9610 - val_loss: 0.1643 - val_accuracy: 0.9400\n",
      "Epoch 689/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0998 - accuracy: 0.9608 - val_loss: 0.1642 - val_accuracy: 0.9401\n",
      "Epoch 690/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0997 - accuracy: 0.9610 - val_loss: 0.1652 - val_accuracy: 0.9400\n",
      "Epoch 691/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0996 - accuracy: 0.9611 - val_loss: 0.1661 - val_accuracy: 0.9393\n",
      "Epoch 692/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0996 - accuracy: 0.9609 - val_loss: 0.1643 - val_accuracy: 0.9397\n",
      "Epoch 693/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0996 - accuracy: 0.9610 - val_loss: 0.1662 - val_accuracy: 0.9399\n",
      "Epoch 694/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0994 - accuracy: 0.9609 - val_loss: 0.1651 - val_accuracy: 0.9402\n",
      "Epoch 695/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0993 - accuracy: 0.9612 - val_loss: 0.1650 - val_accuracy: 0.9398\n",
      "Epoch 696/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0993 - accuracy: 0.9609 - val_loss: 0.1651 - val_accuracy: 0.9403\n",
      "Epoch 697/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0992 - accuracy: 0.9612 - val_loss: 0.1643 - val_accuracy: 0.9400\n",
      "Epoch 698/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0992 - accuracy: 0.9614 - val_loss: 0.1654 - val_accuracy: 0.9398\n",
      "Epoch 699/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0991 - accuracy: 0.9613 - val_loss: 0.1669 - val_accuracy: 0.9396\n",
      "Epoch 700/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0990 - accuracy: 0.9613 - val_loss: 0.1662 - val_accuracy: 0.9407\n",
      "Epoch 701/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0989 - accuracy: 0.9615 - val_loss: 0.1660 - val_accuracy: 0.9400\n",
      "Epoch 702/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0990 - accuracy: 0.9613 - val_loss: 0.1660 - val_accuracy: 0.9399\n",
      "Epoch 703/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0988 - accuracy: 0.9614 - val_loss: 0.1668 - val_accuracy: 0.9399\n",
      "Epoch 704/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0988 - accuracy: 0.9615 - val_loss: 0.1665 - val_accuracy: 0.9403\n",
      "Epoch 705/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0987 - accuracy: 0.9614 - val_loss: 0.1658 - val_accuracy: 0.9395\n",
      "Epoch 706/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0987 - accuracy: 0.9613 - val_loss: 0.1658 - val_accuracy: 0.9401\n",
      "Epoch 707/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0985 - accuracy: 0.9615 - val_loss: 0.1674 - val_accuracy: 0.9390\n",
      "Epoch 708/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0985 - accuracy: 0.9617 - val_loss: 0.1661 - val_accuracy: 0.9406\n",
      "Epoch 709/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0984 - accuracy: 0.9616 - val_loss: 0.1665 - val_accuracy: 0.9404\n",
      "Epoch 710/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0984 - accuracy: 0.9615 - val_loss: 0.1666 - val_accuracy: 0.9402\n",
      "Epoch 711/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1669 - val_accuracy: 0.9397\n",
      "Epoch 712/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0982 - accuracy: 0.9616 - val_loss: 0.1668 - val_accuracy: 0.9398\n",
      "Epoch 713/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0981 - accuracy: 0.9617 - val_loss: 0.1671 - val_accuracy: 0.9394\n",
      "Epoch 714/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0981 - accuracy: 0.9619 - val_loss: 0.1669 - val_accuracy: 0.9395\n",
      "Epoch 715/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0981 - accuracy: 0.9615 - val_loss: 0.1662 - val_accuracy: 0.9400\n",
      "Epoch 716/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0980 - accuracy: 0.9615 - val_loss: 0.1664 - val_accuracy: 0.9398\n",
      "Epoch 717/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0979 - accuracy: 0.9618 - val_loss: 0.1668 - val_accuracy: 0.9398\n",
      "Epoch 718/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0979 - accuracy: 0.9616 - val_loss: 0.1666 - val_accuracy: 0.9398\n",
      "Epoch 719/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0978 - accuracy: 0.9619 - val_loss: 0.1655 - val_accuracy: 0.9401\n",
      "Epoch 720/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0977 - accuracy: 0.9618 - val_loss: 0.1676 - val_accuracy: 0.9394\n",
      "Epoch 721/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0975 - accuracy: 0.9618 - val_loss: 0.1663 - val_accuracy: 0.9398\n",
      "Epoch 722/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0975 - accuracy: 0.9619 - val_loss: 0.1671 - val_accuracy: 0.9394\n",
      "Epoch 723/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0975 - accuracy: 0.9618 - val_loss: 0.1666 - val_accuracy: 0.9395\n",
      "Epoch 724/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0973 - accuracy: 0.9621 - val_loss: 0.1667 - val_accuracy: 0.9400\n",
      "Epoch 725/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0973 - accuracy: 0.9622 - val_loss: 0.1700 - val_accuracy: 0.9393\n",
      "Epoch 726/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0973 - accuracy: 0.9620 - val_loss: 0.1683 - val_accuracy: 0.9395\n",
      "Epoch 727/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0972 - accuracy: 0.9621 - val_loss: 0.1675 - val_accuracy: 0.9397\n",
      "Epoch 728/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0972 - accuracy: 0.9621 - val_loss: 0.1684 - val_accuracy: 0.9398\n",
      "Epoch 729/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0971 - accuracy: 0.9621 - val_loss: 0.1666 - val_accuracy: 0.9397\n",
      "Epoch 730/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0969 - accuracy: 0.9624 - val_loss: 0.1681 - val_accuracy: 0.9395\n",
      "Epoch 731/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0970 - accuracy: 0.9620 - val_loss: 0.1670 - val_accuracy: 0.9404\n",
      "Epoch 732/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0970 - accuracy: 0.9622 - val_loss: 0.1671 - val_accuracy: 0.9397\n",
      "Epoch 733/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0968 - accuracy: 0.9620 - val_loss: 0.1688 - val_accuracy: 0.9387\n",
      "Epoch 734/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0968 - accuracy: 0.9618 - val_loss: 0.1686 - val_accuracy: 0.9398\n",
      "Epoch 735/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0967 - accuracy: 0.9622 - val_loss: 0.1686 - val_accuracy: 0.9398\n",
      "Epoch 736/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0966 - accuracy: 0.9623 - val_loss: 0.1667 - val_accuracy: 0.9397\n",
      "Epoch 737/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0965 - accuracy: 0.9624 - val_loss: 0.1690 - val_accuracy: 0.9389\n",
      "Epoch 738/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0965 - accuracy: 0.9622 - val_loss: 0.1675 - val_accuracy: 0.9404\n",
      "Epoch 739/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0964 - accuracy: 0.9624 - val_loss: 0.1710 - val_accuracy: 0.9389\n",
      "Epoch 740/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0964 - accuracy: 0.9624 - val_loss: 0.1682 - val_accuracy: 0.9396\n",
      "Epoch 741/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0962 - accuracy: 0.9624 - val_loss: 0.1684 - val_accuracy: 0.9399\n",
      "Epoch 742/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0962 - accuracy: 0.9623 - val_loss: 0.1698 - val_accuracy: 0.9384\n",
      "Epoch 743/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.1676 - val_accuracy: 0.9396\n",
      "Epoch 744/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.1688 - val_accuracy: 0.9400\n",
      "Epoch 745/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0961 - accuracy: 0.9626 - val_loss: 0.1686 - val_accuracy: 0.9395\n",
      "Epoch 746/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0959 - accuracy: 0.9625 - val_loss: 0.1705 - val_accuracy: 0.9393\n",
      "Epoch 747/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0958 - accuracy: 0.9625 - val_loss: 0.1695 - val_accuracy: 0.9387\n",
      "Epoch 748/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0958 - accuracy: 0.9626 - val_loss: 0.1682 - val_accuracy: 0.9390\n",
      "Epoch 749/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0958 - accuracy: 0.9626 - val_loss: 0.1697 - val_accuracy: 0.9385\n",
      "Epoch 750/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0957 - accuracy: 0.9626 - val_loss: 0.1691 - val_accuracy: 0.9396\n",
      "Epoch 751/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0954 - accuracy: 0.9627 - val_loss: 0.1684 - val_accuracy: 0.9399\n",
      "Epoch 752/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0954 - accuracy: 0.9628 - val_loss: 0.1683 - val_accuracy: 0.9395\n",
      "Epoch 753/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0954 - accuracy: 0.9628 - val_loss: 0.1690 - val_accuracy: 0.9399\n",
      "Epoch 754/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0953 - accuracy: 0.9628 - val_loss: 0.1705 - val_accuracy: 0.9393\n",
      "Epoch 755/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0953 - accuracy: 0.9630 - val_loss: 0.1696 - val_accuracy: 0.9397\n",
      "Epoch 756/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0951 - accuracy: 0.9630 - val_loss: 0.1697 - val_accuracy: 0.9400\n",
      "Epoch 757/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0951 - accuracy: 0.9629 - val_loss: 0.1690 - val_accuracy: 0.9389\n",
      "Epoch 758/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0951 - accuracy: 0.9629 - val_loss: 0.1710 - val_accuracy: 0.9384\n",
      "Epoch 759/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0950 - accuracy: 0.9630 - val_loss: 0.1704 - val_accuracy: 0.9394\n",
      "Epoch 760/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0950 - accuracy: 0.9629 - val_loss: 0.1702 - val_accuracy: 0.9389\n",
      "Epoch 761/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0950 - accuracy: 0.9628 - val_loss: 0.1700 - val_accuracy: 0.9391\n",
      "Epoch 762/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0949 - accuracy: 0.9631 - val_loss: 0.1703 - val_accuracy: 0.9394\n",
      "Epoch 763/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0946 - accuracy: 0.9631 - val_loss: 0.1717 - val_accuracy: 0.9391\n",
      "Epoch 764/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0947 - accuracy: 0.9632 - val_loss: 0.1704 - val_accuracy: 0.9399\n",
      "Epoch 765/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0946 - accuracy: 0.9632 - val_loss: 0.1712 - val_accuracy: 0.9398\n",
      "Epoch 766/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0944 - accuracy: 0.9633 - val_loss: 0.1730 - val_accuracy: 0.9383\n",
      "Epoch 767/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0945 - accuracy: 0.9631 - val_loss: 0.1701 - val_accuracy: 0.9396\n",
      "Epoch 768/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0942 - accuracy: 0.9633 - val_loss: 0.1717 - val_accuracy: 0.9389\n",
      "Epoch 769/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0942 - accuracy: 0.9631 - val_loss: 0.1716 - val_accuracy: 0.9390\n",
      "Epoch 770/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0942 - accuracy: 0.9631 - val_loss: 0.1709 - val_accuracy: 0.9394\n",
      "Epoch 771/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0943 - accuracy: 0.9632 - val_loss: 0.1710 - val_accuracy: 0.9389\n",
      "Epoch 772/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0941 - accuracy: 0.9635 - val_loss: 0.1712 - val_accuracy: 0.9390\n",
      "Epoch 773/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0941 - accuracy: 0.9633 - val_loss: 0.1698 - val_accuracy: 0.9400\n",
      "Epoch 774/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0939 - accuracy: 0.9633 - val_loss: 0.1703 - val_accuracy: 0.9396\n",
      "Epoch 775/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0938 - accuracy: 0.9633 - val_loss: 0.1722 - val_accuracy: 0.9389\n",
      "Epoch 776/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0937 - accuracy: 0.9635 - val_loss: 0.1727 - val_accuracy: 0.9393\n",
      "Epoch 777/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0937 - accuracy: 0.9637 - val_loss: 0.1716 - val_accuracy: 0.9388\n",
      "Epoch 778/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0936 - accuracy: 0.9632 - val_loss: 0.1710 - val_accuracy: 0.9397\n",
      "Epoch 779/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0936 - accuracy: 0.9637 - val_loss: 0.1732 - val_accuracy: 0.9385\n",
      "Epoch 780/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0935 - accuracy: 0.9635 - val_loss: 0.1717 - val_accuracy: 0.9396\n",
      "Epoch 781/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0934 - accuracy: 0.9636 - val_loss: 0.1717 - val_accuracy: 0.9399\n",
      "Epoch 782/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0933 - accuracy: 0.9636 - val_loss: 0.1730 - val_accuracy: 0.9387\n",
      "Epoch 783/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0933 - accuracy: 0.9635 - val_loss: 0.1714 - val_accuracy: 0.9394\n",
      "Epoch 784/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0933 - accuracy: 0.9635 - val_loss: 0.1726 - val_accuracy: 0.9391\n",
      "Epoch 785/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0931 - accuracy: 0.9638 - val_loss: 0.1714 - val_accuracy: 0.9389\n",
      "Epoch 786/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0930 - accuracy: 0.9637 - val_loss: 0.1721 - val_accuracy: 0.9401\n",
      "Epoch 787/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0929 - accuracy: 0.9637 - val_loss: 0.1729 - val_accuracy: 0.9392\n",
      "Epoch 788/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0931 - accuracy: 0.9637 - val_loss: 0.1728 - val_accuracy: 0.9386\n",
      "Epoch 789/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0930 - accuracy: 0.9639 - val_loss: 0.1733 - val_accuracy: 0.9391\n",
      "Epoch 790/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0928 - accuracy: 0.9638 - val_loss: 0.1728 - val_accuracy: 0.9390\n",
      "Epoch 791/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0927 - accuracy: 0.9638 - val_loss: 0.1725 - val_accuracy: 0.9399\n",
      "Epoch 792/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0927 - accuracy: 0.9637 - val_loss: 0.1732 - val_accuracy: 0.9393\n",
      "Epoch 793/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0924 - accuracy: 0.9641 - val_loss: 0.1731 - val_accuracy: 0.9393\n",
      "Epoch 794/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0923 - accuracy: 0.9639 - val_loss: 0.1723 - val_accuracy: 0.9392\n",
      "Epoch 795/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0924 - accuracy: 0.9639 - val_loss: 0.1741 - val_accuracy: 0.9387\n",
      "Epoch 796/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0921 - accuracy: 0.9640 - val_loss: 0.1734 - val_accuracy: 0.9390\n",
      "Epoch 797/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0922 - accuracy: 0.9642 - val_loss: 0.1734 - val_accuracy: 0.9390\n",
      "Epoch 798/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0923 - accuracy: 0.9642 - val_loss: 0.1732 - val_accuracy: 0.9390\n",
      "Epoch 799/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.0920 - accuracy: 0.9640 - val_loss: 0.1746 - val_accuracy: 0.9377\n",
      "Epoch 800/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0920 - accuracy: 0.9640 - val_loss: 0.1735 - val_accuracy: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:122: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Train on 6000 samples, validate on 2001 samples\n",
      "Epoch 1/800\n",
      "6000/6000 [==============================] - 1s 95us/step - loss: 0.3443 - accuracy: 0.8909 - val_loss: 0.2654 - val_accuracy: 0.9247\n",
      "Epoch 2/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.2496 - accuracy: 0.9304 - val_loss: 0.2647 - val_accuracy: 0.9247\n",
      "Epoch 3/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2491 - accuracy: 0.9304 - val_loss: 0.2643 - val_accuracy: 0.9247\n",
      "Epoch 4/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2487 - accuracy: 0.9304 - val_loss: 0.2641 - val_accuracy: 0.9247\n",
      "Epoch 5/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2484 - accuracy: 0.9304 - val_loss: 0.2636 - val_accuracy: 0.9247\n",
      "Epoch 6/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2481 - accuracy: 0.9304 - val_loss: 0.2633 - val_accuracy: 0.9247\n",
      "Epoch 7/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2478 - accuracy: 0.9304 - val_loss: 0.2629 - val_accuracy: 0.9247\n",
      "Epoch 8/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2474 - accuracy: 0.9304 - val_loss: 0.2627 - val_accuracy: 0.9247\n",
      "Epoch 9/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2470 - accuracy: 0.9304 - val_loss: 0.2623 - val_accuracy: 0.9247\n",
      "Epoch 10/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2466 - accuracy: 0.9304 - val_loss: 0.2617 - val_accuracy: 0.9247\n",
      "Epoch 11/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2462 - accuracy: 0.9304 - val_loss: 0.2613 - val_accuracy: 0.9247\n",
      "Epoch 12/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2457 - accuracy: 0.9304 - val_loss: 0.2609 - val_accuracy: 0.9247\n",
      "Epoch 13/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.2452 - accuracy: 0.9304 - val_loss: 0.2602 - val_accuracy: 0.9247\n",
      "Epoch 14/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2446 - accuracy: 0.9304 - val_loss: 0.2595 - val_accuracy: 0.9247\n",
      "Epoch 15/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2440 - accuracy: 0.9304 - val_loss: 0.2591 - val_accuracy: 0.9247\n",
      "Epoch 16/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2433 - accuracy: 0.9304 - val_loss: 0.2582 - val_accuracy: 0.9247\n",
      "Epoch 17/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2425 - accuracy: 0.9304 - val_loss: 0.2572 - val_accuracy: 0.9247\n",
      "Epoch 18/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2417 - accuracy: 0.9304 - val_loss: 0.2563 - val_accuracy: 0.9247\n",
      "Epoch 19/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2407 - accuracy: 0.9304 - val_loss: 0.2551 - val_accuracy: 0.9247\n",
      "Epoch 20/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2397 - accuracy: 0.9304 - val_loss: 0.2543 - val_accuracy: 0.9247\n",
      "Epoch 21/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2385 - accuracy: 0.9304 - val_loss: 0.2530 - val_accuracy: 0.9247\n",
      "Epoch 22/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2373 - accuracy: 0.9304 - val_loss: 0.2517 - val_accuracy: 0.9247\n",
      "Epoch 23/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2361 - accuracy: 0.9304 - val_loss: 0.2505 - val_accuracy: 0.9247\n",
      "Epoch 24/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2347 - accuracy: 0.9304 - val_loss: 0.2491 - val_accuracy: 0.9247\n",
      "Epoch 25/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2334 - accuracy: 0.9304 - val_loss: 0.2479 - val_accuracy: 0.9247\n",
      "Epoch 26/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2320 - accuracy: 0.9304 - val_loss: 0.2463 - val_accuracy: 0.9247\n",
      "Epoch 27/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2307 - accuracy: 0.9304 - val_loss: 0.2449 - val_accuracy: 0.9247\n",
      "Epoch 28/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2292 - accuracy: 0.9304 - val_loss: 0.2435 - val_accuracy: 0.9247\n",
      "Epoch 29/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2278 - accuracy: 0.9304 - val_loss: 0.2423 - val_accuracy: 0.9247\n",
      "Epoch 30/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2263 - accuracy: 0.9304 - val_loss: 0.2407 - val_accuracy: 0.9247\n",
      "Epoch 31/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2249 - accuracy: 0.9304 - val_loss: 0.2393 - val_accuracy: 0.9247\n",
      "Epoch 32/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2234 - accuracy: 0.9304 - val_loss: 0.2380 - val_accuracy: 0.9248\n",
      "Epoch 33/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2220 - accuracy: 0.9305 - val_loss: 0.2362 - val_accuracy: 0.9248\n",
      "Epoch 34/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2205 - accuracy: 0.9305 - val_loss: 0.2348 - val_accuracy: 0.9248\n",
      "Epoch 35/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2189 - accuracy: 0.9306 - val_loss: 0.2334 - val_accuracy: 0.9248\n",
      "Epoch 36/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2174 - accuracy: 0.9307 - val_loss: 0.2317 - val_accuracy: 0.9249\n",
      "Epoch 37/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2158 - accuracy: 0.9308 - val_loss: 0.2302 - val_accuracy: 0.9250\n",
      "Epoch 38/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2143 - accuracy: 0.9309 - val_loss: 0.2288 - val_accuracy: 0.9250\n",
      "Epoch 39/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2127 - accuracy: 0.9310 - val_loss: 0.2272 - val_accuracy: 0.9252\n",
      "Epoch 40/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2112 - accuracy: 0.9311 - val_loss: 0.2259 - val_accuracy: 0.9253\n",
      "Epoch 41/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2097 - accuracy: 0.9312 - val_loss: 0.2245 - val_accuracy: 0.9255\n",
      "Epoch 42/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.2082 - accuracy: 0.9313 - val_loss: 0.2230 - val_accuracy: 0.9254\n",
      "Epoch 43/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.2068 - accuracy: 0.9315 - val_loss: 0.2216 - val_accuracy: 0.9258\n",
      "Epoch 44/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2054 - accuracy: 0.9316 - val_loss: 0.2204 - val_accuracy: 0.9258\n",
      "Epoch 45/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.2040 - accuracy: 0.9317 - val_loss: 0.2193 - val_accuracy: 0.9259\n",
      "Epoch 46/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.2026 - accuracy: 0.9317 - val_loss: 0.2181 - val_accuracy: 0.9258\n",
      "Epoch 47/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2013 - accuracy: 0.9319 - val_loss: 0.2171 - val_accuracy: 0.9262\n",
      "Epoch 48/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.2001 - accuracy: 0.9320 - val_loss: 0.2158 - val_accuracy: 0.9260\n",
      "Epoch 49/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1989 - accuracy: 0.9322 - val_loss: 0.2144 - val_accuracy: 0.9261\n",
      "Epoch 50/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1977 - accuracy: 0.9321 - val_loss: 0.2136 - val_accuracy: 0.9264\n",
      "Epoch 51/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1966 - accuracy: 0.9323 - val_loss: 0.2131 - val_accuracy: 0.9263\n",
      "Epoch 52/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1956 - accuracy: 0.9325 - val_loss: 0.2117 - val_accuracy: 0.9264\n",
      "Epoch 53/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1946 - accuracy: 0.9324 - val_loss: 0.2108 - val_accuracy: 0.9267\n",
      "Epoch 54/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1937 - accuracy: 0.9326 - val_loss: 0.2101 - val_accuracy: 0.9268\n",
      "Epoch 55/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1929 - accuracy: 0.9328 - val_loss: 0.2092 - val_accuracy: 0.9265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1921 - accuracy: 0.9328 - val_loss: 0.2088 - val_accuracy: 0.9270\n",
      "Epoch 57/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1914 - accuracy: 0.9328 - val_loss: 0.2084 - val_accuracy: 0.9268\n",
      "Epoch 58/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1907 - accuracy: 0.9331 - val_loss: 0.2077 - val_accuracy: 0.9270\n",
      "Epoch 59/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1901 - accuracy: 0.9331 - val_loss: 0.2069 - val_accuracy: 0.9273\n",
      "Epoch 60/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1894 - accuracy: 0.9332 - val_loss: 0.2066 - val_accuracy: 0.9271\n",
      "Epoch 61/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1888 - accuracy: 0.9334 - val_loss: 0.2059 - val_accuracy: 0.9273\n",
      "Epoch 62/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1882 - accuracy: 0.9333 - val_loss: 0.2055 - val_accuracy: 0.9276\n",
      "Epoch 63/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1876 - accuracy: 0.9336 - val_loss: 0.2048 - val_accuracy: 0.9273\n",
      "Epoch 64/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1871 - accuracy: 0.9335 - val_loss: 0.2045 - val_accuracy: 0.9273\n",
      "Epoch 65/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1864 - accuracy: 0.9336 - val_loss: 0.2040 - val_accuracy: 0.9277\n",
      "Epoch 66/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1858 - accuracy: 0.9339 - val_loss: 0.2033 - val_accuracy: 0.9274\n",
      "Epoch 67/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1852 - accuracy: 0.9340 - val_loss: 0.2030 - val_accuracy: 0.9276\n",
      "Epoch 68/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1846 - accuracy: 0.9340 - val_loss: 0.2022 - val_accuracy: 0.9278\n",
      "Epoch 69/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1840 - accuracy: 0.9342 - val_loss: 0.2019 - val_accuracy: 0.9279\n",
      "Epoch 70/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1833 - accuracy: 0.9343 - val_loss: 0.2018 - val_accuracy: 0.9275\n",
      "Epoch 71/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1827 - accuracy: 0.9346 - val_loss: 0.2006 - val_accuracy: 0.9285\n",
      "Epoch 72/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1821 - accuracy: 0.9346 - val_loss: 0.2003 - val_accuracy: 0.9280\n",
      "Epoch 73/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1815 - accuracy: 0.9347 - val_loss: 0.1995 - val_accuracy: 0.9280\n",
      "Epoch 74/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1809 - accuracy: 0.9348 - val_loss: 0.1990 - val_accuracy: 0.9285\n",
      "Epoch 75/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1803 - accuracy: 0.9352 - val_loss: 0.1983 - val_accuracy: 0.9287\n",
      "Epoch 76/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1799 - accuracy: 0.9352 - val_loss: 0.1984 - val_accuracy: 0.9285\n",
      "Epoch 77/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1793 - accuracy: 0.9353 - val_loss: 0.1980 - val_accuracy: 0.9284\n",
      "Epoch 78/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1789 - accuracy: 0.9355 - val_loss: 0.1972 - val_accuracy: 0.9284\n",
      "Epoch 79/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1784 - accuracy: 0.9356 - val_loss: 0.1967 - val_accuracy: 0.9292\n",
      "Epoch 80/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1780 - accuracy: 0.9358 - val_loss: 0.1963 - val_accuracy: 0.9290\n",
      "Epoch 81/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1776 - accuracy: 0.9360 - val_loss: 0.1961 - val_accuracy: 0.9289\n",
      "Epoch 82/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1772 - accuracy: 0.9358 - val_loss: 0.1960 - val_accuracy: 0.9288\n",
      "Epoch 83/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1768 - accuracy: 0.9361 - val_loss: 0.1954 - val_accuracy: 0.9293\n",
      "Epoch 84/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1764 - accuracy: 0.9361 - val_loss: 0.1953 - val_accuracy: 0.9294\n",
      "Epoch 85/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1761 - accuracy: 0.9363 - val_loss: 0.1949 - val_accuracy: 0.9293\n",
      "Epoch 86/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1757 - accuracy: 0.9363 - val_loss: 0.1946 - val_accuracy: 0.9294\n",
      "Epoch 87/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1754 - accuracy: 0.9367 - val_loss: 0.1941 - val_accuracy: 0.9295\n",
      "Epoch 88/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1750 - accuracy: 0.9368 - val_loss: 0.1938 - val_accuracy: 0.9299\n",
      "Epoch 89/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1746 - accuracy: 0.9369 - val_loss: 0.1935 - val_accuracy: 0.9298\n",
      "Epoch 90/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1742 - accuracy: 0.9369 - val_loss: 0.1932 - val_accuracy: 0.9299\n",
      "Epoch 91/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1739 - accuracy: 0.9370 - val_loss: 0.1927 - val_accuracy: 0.9298\n",
      "Epoch 92/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1735 - accuracy: 0.9372 - val_loss: 0.1926 - val_accuracy: 0.9304\n",
      "Epoch 93/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1732 - accuracy: 0.9372 - val_loss: 0.1922 - val_accuracy: 0.9302\n",
      "Epoch 94/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1728 - accuracy: 0.9373 - val_loss: 0.1920 - val_accuracy: 0.9301\n",
      "Epoch 95/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1725 - accuracy: 0.9374 - val_loss: 0.1918 - val_accuracy: 0.9305\n",
      "Epoch 96/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1721 - accuracy: 0.9376 - val_loss: 0.1911 - val_accuracy: 0.9302\n",
      "Epoch 97/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1717 - accuracy: 0.9378 - val_loss: 0.1904 - val_accuracy: 0.9306\n",
      "Epoch 98/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1713 - accuracy: 0.9379 - val_loss: 0.1905 - val_accuracy: 0.9306\n",
      "Epoch 99/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1709 - accuracy: 0.9380 - val_loss: 0.1899 - val_accuracy: 0.9306\n",
      "Epoch 100/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1705 - accuracy: 0.9380 - val_loss: 0.1898 - val_accuracy: 0.9306\n",
      "Epoch 101/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1702 - accuracy: 0.9381 - val_loss: 0.1893 - val_accuracy: 0.9309\n",
      "Epoch 102/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1698 - accuracy: 0.9382 - val_loss: 0.1892 - val_accuracy: 0.9308\n",
      "Epoch 103/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1694 - accuracy: 0.9385 - val_loss: 0.1887 - val_accuracy: 0.9308\n",
      "Epoch 104/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1691 - accuracy: 0.9385 - val_loss: 0.1885 - val_accuracy: 0.9310\n",
      "Epoch 105/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1688 - accuracy: 0.9384 - val_loss: 0.1881 - val_accuracy: 0.9312\n",
      "Epoch 106/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1684 - accuracy: 0.9386 - val_loss: 0.1877 - val_accuracy: 0.9312\n",
      "Epoch 107/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1681 - accuracy: 0.9388 - val_loss: 0.1877 - val_accuracy: 0.9312\n",
      "Epoch 108/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1678 - accuracy: 0.9390 - val_loss: 0.1870 - val_accuracy: 0.9313\n",
      "Epoch 109/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1674 - accuracy: 0.9388 - val_loss: 0.1868 - val_accuracy: 0.9314\n",
      "Epoch 110/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1672 - accuracy: 0.9389 - val_loss: 0.1866 - val_accuracy: 0.9317\n",
      "Epoch 111/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1668 - accuracy: 0.9391 - val_loss: 0.1862 - val_accuracy: 0.9319\n",
      "Epoch 112/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1665 - accuracy: 0.9392 - val_loss: 0.1862 - val_accuracy: 0.9321\n",
      "Epoch 113/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1661 - accuracy: 0.9394 - val_loss: 0.1855 - val_accuracy: 0.9318\n",
      "Epoch 114/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1659 - accuracy: 0.9394 - val_loss: 0.1852 - val_accuracy: 0.9321\n",
      "Epoch 115/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1656 - accuracy: 0.9394 - val_loss: 0.1848 - val_accuracy: 0.9321\n",
      "Epoch 116/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1652 - accuracy: 0.9396 - val_loss: 0.1849 - val_accuracy: 0.9321\n",
      "Epoch 117/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1649 - accuracy: 0.9397 - val_loss: 0.1850 - val_accuracy: 0.9320\n",
      "Epoch 118/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1647 - accuracy: 0.9398 - val_loss: 0.1841 - val_accuracy: 0.9316\n",
      "Epoch 119/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1643 - accuracy: 0.9398 - val_loss: 0.1843 - val_accuracy: 0.9323\n",
      "Epoch 120/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1640 - accuracy: 0.9401 - val_loss: 0.1837 - val_accuracy: 0.9325\n",
      "Epoch 121/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1637 - accuracy: 0.9401 - val_loss: 0.1836 - val_accuracy: 0.9321\n",
      "Epoch 122/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1634 - accuracy: 0.9401 - val_loss: 0.1831 - val_accuracy: 0.9326\n",
      "Epoch 123/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1631 - accuracy: 0.9403 - val_loss: 0.1832 - val_accuracy: 0.9326\n",
      "Epoch 124/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1628 - accuracy: 0.9404 - val_loss: 0.1824 - val_accuracy: 0.9326\n",
      "Epoch 125/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1624 - accuracy: 0.9403 - val_loss: 0.1825 - val_accuracy: 0.9328\n",
      "Epoch 126/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1622 - accuracy: 0.9406 - val_loss: 0.1818 - val_accuracy: 0.9328\n",
      "Epoch 127/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1619 - accuracy: 0.9407 - val_loss: 0.1814 - val_accuracy: 0.9332\n",
      "Epoch 128/800\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.1616 - accuracy: 0.9408 - val_loss: 0.1815 - val_accuracy: 0.9330\n",
      "Epoch 129/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1613 - accuracy: 0.9408 - val_loss: 0.1817 - val_accuracy: 0.9333\n",
      "Epoch 130/800\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.1611 - accuracy: 0.9409 - val_loss: 0.1808 - val_accuracy: 0.9333\n",
      "Epoch 131/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1608 - accuracy: 0.9410 - val_loss: 0.1807 - val_accuracy: 0.9333\n",
      "Epoch 132/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1606 - accuracy: 0.9411 - val_loss: 0.1802 - val_accuracy: 0.9333\n",
      "Epoch 133/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1603 - accuracy: 0.9413 - val_loss: 0.1802 - val_accuracy: 0.9332\n",
      "Epoch 134/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1600 - accuracy: 0.9411 - val_loss: 0.1804 - val_accuracy: 0.9333\n",
      "Epoch 135/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1597 - accuracy: 0.9414 - val_loss: 0.1798 - val_accuracy: 0.9339\n",
      "Epoch 136/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1594 - accuracy: 0.9414 - val_loss: 0.1800 - val_accuracy: 0.9335\n",
      "Epoch 137/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1591 - accuracy: 0.9416 - val_loss: 0.1792 - val_accuracy: 0.9339\n",
      "Epoch 138/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1589 - accuracy: 0.9418 - val_loss: 0.1789 - val_accuracy: 0.9341\n",
      "Epoch 139/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1586 - accuracy: 0.9418 - val_loss: 0.1786 - val_accuracy: 0.9339\n",
      "Epoch 140/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1583 - accuracy: 0.9418 - val_loss: 0.1784 - val_accuracy: 0.9337\n",
      "Epoch 141/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1580 - accuracy: 0.9420 - val_loss: 0.1780 - val_accuracy: 0.9344\n",
      "Epoch 142/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1577 - accuracy: 0.9421 - val_loss: 0.1782 - val_accuracy: 0.9345\n",
      "Epoch 143/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1574 - accuracy: 0.9422 - val_loss: 0.1777 - val_accuracy: 0.9344\n",
      "Epoch 144/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1572 - accuracy: 0.9423 - val_loss: 0.1776 - val_accuracy: 0.9343\n",
      "Epoch 145/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1568 - accuracy: 0.9424 - val_loss: 0.1773 - val_accuracy: 0.9347\n",
      "Epoch 146/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1566 - accuracy: 0.9424 - val_loss: 0.1770 - val_accuracy: 0.9348\n",
      "Epoch 147/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1563 - accuracy: 0.9426 - val_loss: 0.1764 - val_accuracy: 0.9346\n",
      "Epoch 148/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1561 - accuracy: 0.9427 - val_loss: 0.1766 - val_accuracy: 0.9343\n",
      "Epoch 149/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1558 - accuracy: 0.9428 - val_loss: 0.1766 - val_accuracy: 0.9349\n",
      "Epoch 150/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.1762 - val_accuracy: 0.9350\n",
      "Epoch 151/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1553 - accuracy: 0.9430 - val_loss: 0.1756 - val_accuracy: 0.9349\n",
      "Epoch 152/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1550 - accuracy: 0.9430 - val_loss: 0.1754 - val_accuracy: 0.9352\n",
      "Epoch 153/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1548 - accuracy: 0.9432 - val_loss: 0.1758 - val_accuracy: 0.9355\n",
      "Epoch 154/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1546 - accuracy: 0.9432 - val_loss: 0.1753 - val_accuracy: 0.9358\n",
      "Epoch 155/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1544 - accuracy: 0.9433 - val_loss: 0.1752 - val_accuracy: 0.9356\n",
      "Epoch 156/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1540 - accuracy: 0.9434 - val_loss: 0.1745 - val_accuracy: 0.9359\n",
      "Epoch 157/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1538 - accuracy: 0.9434 - val_loss: 0.1751 - val_accuracy: 0.9364\n",
      "Epoch 158/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1536 - accuracy: 0.9435 - val_loss: 0.1741 - val_accuracy: 0.9357\n",
      "Epoch 159/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1534 - accuracy: 0.9436 - val_loss: 0.1739 - val_accuracy: 0.9359\n",
      "Epoch 160/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1531 - accuracy: 0.9436 - val_loss: 0.1736 - val_accuracy: 0.9357\n",
      "Epoch 161/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1529 - accuracy: 0.9437 - val_loss: 0.1737 - val_accuracy: 0.9358\n",
      "Epoch 162/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1527 - accuracy: 0.9438 - val_loss: 0.1736 - val_accuracy: 0.9364\n",
      "Epoch 163/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1525 - accuracy: 0.9440 - val_loss: 0.1732 - val_accuracy: 0.9361\n",
      "Epoch 164/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1522 - accuracy: 0.9439 - val_loss: 0.1733 - val_accuracy: 0.9359\n",
      "Epoch 165/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1521 - accuracy: 0.9440 - val_loss: 0.1731 - val_accuracy: 0.9362\n",
      "Epoch 166/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1519 - accuracy: 0.9442 - val_loss: 0.1724 - val_accuracy: 0.9361\n",
      "Epoch 167/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1516 - accuracy: 0.9442 - val_loss: 0.1726 - val_accuracy: 0.9359\n",
      "Epoch 168/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1514 - accuracy: 0.9443 - val_loss: 0.1734 - val_accuracy: 0.9368\n",
      "Epoch 169/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1512 - accuracy: 0.9443 - val_loss: 0.1722 - val_accuracy: 0.9368\n",
      "Epoch 170/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1512 - accuracy: 0.9443 - val_loss: 0.1720 - val_accuracy: 0.9369\n",
      "Epoch 171/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1509 - accuracy: 0.9444 - val_loss: 0.1720 - val_accuracy: 0.9369\n",
      "Epoch 172/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1507 - accuracy: 0.9444 - val_loss: 0.1715 - val_accuracy: 0.9368\n",
      "Epoch 173/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1504 - accuracy: 0.9445 - val_loss: 0.1713 - val_accuracy: 0.9370\n",
      "Epoch 174/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1504 - accuracy: 0.9444 - val_loss: 0.1716 - val_accuracy: 0.9369\n",
      "Epoch 175/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1502 - accuracy: 0.9446 - val_loss: 0.1710 - val_accuracy: 0.9365\n",
      "Epoch 176/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1500 - accuracy: 0.9447 - val_loss: 0.1711 - val_accuracy: 0.9367\n",
      "Epoch 177/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1499 - accuracy: 0.9448 - val_loss: 0.1706 - val_accuracy: 0.9367\n",
      "Epoch 178/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1496 - accuracy: 0.9447 - val_loss: 0.1707 - val_accuracy: 0.9368\n",
      "Epoch 179/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1494 - accuracy: 0.9447 - val_loss: 0.1707 - val_accuracy: 0.9369\n",
      "Epoch 180/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1493 - accuracy: 0.9450 - val_loss: 0.1709 - val_accuracy: 0.9365\n",
      "Epoch 181/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1491 - accuracy: 0.9450 - val_loss: 0.1701 - val_accuracy: 0.9374\n",
      "Epoch 182/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1489 - accuracy: 0.9450 - val_loss: 0.1706 - val_accuracy: 0.9366\n",
      "Epoch 183/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1488 - accuracy: 0.9451 - val_loss: 0.1706 - val_accuracy: 0.9368\n",
      "Epoch 184/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1486 - accuracy: 0.9452 - val_loss: 0.1702 - val_accuracy: 0.9370\n",
      "Epoch 185/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1484 - accuracy: 0.9450 - val_loss: 0.1698 - val_accuracy: 0.9373\n",
      "Epoch 186/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1483 - accuracy: 0.9451 - val_loss: 0.1702 - val_accuracy: 0.9374\n",
      "Epoch 187/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1481 - accuracy: 0.9454 - val_loss: 0.1695 - val_accuracy: 0.9372\n",
      "Epoch 188/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1480 - accuracy: 0.9451 - val_loss: 0.1693 - val_accuracy: 0.9375\n",
      "Epoch 189/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1478 - accuracy: 0.9454 - val_loss: 0.1696 - val_accuracy: 0.9371\n",
      "Epoch 190/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1476 - accuracy: 0.9453 - val_loss: 0.1697 - val_accuracy: 0.9373\n",
      "Epoch 191/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1475 - accuracy: 0.9455 - val_loss: 0.1692 - val_accuracy: 0.9375\n",
      "Epoch 192/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1473 - accuracy: 0.9456 - val_loss: 0.1694 - val_accuracy: 0.9372\n",
      "Epoch 193/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1472 - accuracy: 0.9455 - val_loss: 0.1689 - val_accuracy: 0.9378\n",
      "Epoch 194/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1470 - accuracy: 0.9455 - val_loss: 0.1696 - val_accuracy: 0.9372\n",
      "Epoch 195/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1468 - accuracy: 0.9457 - val_loss: 0.1687 - val_accuracy: 0.9371\n",
      "Epoch 196/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1467 - accuracy: 0.9457 - val_loss: 0.1688 - val_accuracy: 0.9376\n",
      "Epoch 197/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1465 - accuracy: 0.9459 - val_loss: 0.1685 - val_accuracy: 0.9373\n",
      "Epoch 198/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1464 - accuracy: 0.9459 - val_loss: 0.1682 - val_accuracy: 0.9378\n",
      "Epoch 199/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1463 - accuracy: 0.9457 - val_loss: 0.1685 - val_accuracy: 0.9378\n",
      "Epoch 200/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1460 - accuracy: 0.9458 - val_loss: 0.1686 - val_accuracy: 0.9377\n",
      "Epoch 201/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1459 - accuracy: 0.9459 - val_loss: 0.1681 - val_accuracy: 0.9379\n",
      "Epoch 202/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1457 - accuracy: 0.9458 - val_loss: 0.1682 - val_accuracy: 0.9380\n",
      "Epoch 203/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1456 - accuracy: 0.9460 - val_loss: 0.1687 - val_accuracy: 0.9377\n",
      "Epoch 204/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1454 - accuracy: 0.9462 - val_loss: 0.1676 - val_accuracy: 0.9383\n",
      "Epoch 205/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1452 - accuracy: 0.9461 - val_loss: 0.1677 - val_accuracy: 0.9376\n",
      "Epoch 206/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1451 - accuracy: 0.9463 - val_loss: 0.1677 - val_accuracy: 0.9381\n",
      "Epoch 207/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1450 - accuracy: 0.9461 - val_loss: 0.1678 - val_accuracy: 0.9378\n",
      "Epoch 208/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1449 - accuracy: 0.9463 - val_loss: 0.1672 - val_accuracy: 0.9377\n",
      "Epoch 209/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1446 - accuracy: 0.9463 - val_loss: 0.1674 - val_accuracy: 0.9382\n",
      "Epoch 210/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1446 - accuracy: 0.9463 - val_loss: 0.1674 - val_accuracy: 0.9377\n",
      "Epoch 211/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1444 - accuracy: 0.9465 - val_loss: 0.1673 - val_accuracy: 0.9378\n",
      "Epoch 212/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1443 - accuracy: 0.9466 - val_loss: 0.1670 - val_accuracy: 0.9381\n",
      "Epoch 213/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1441 - accuracy: 0.9463 - val_loss: 0.1664 - val_accuracy: 0.9381\n",
      "Epoch 214/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1439 - accuracy: 0.9464 - val_loss: 0.1669 - val_accuracy: 0.9382\n",
      "Epoch 215/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1438 - accuracy: 0.9466 - val_loss: 0.1669 - val_accuracy: 0.9378\n",
      "Epoch 216/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1437 - accuracy: 0.9466 - val_loss: 0.1664 - val_accuracy: 0.9383\n",
      "Epoch 217/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1436 - accuracy: 0.9468 - val_loss: 0.1665 - val_accuracy: 0.9387\n",
      "Epoch 218/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1433 - accuracy: 0.9468 - val_loss: 0.1664 - val_accuracy: 0.9385\n",
      "Epoch 219/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1432 - accuracy: 0.9468 - val_loss: 0.1667 - val_accuracy: 0.9380\n",
      "Epoch 220/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1431 - accuracy: 0.9469 - val_loss: 0.1660 - val_accuracy: 0.9381\n",
      "Epoch 221/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1430 - accuracy: 0.9469 - val_loss: 0.1659 - val_accuracy: 0.9383\n",
      "Epoch 222/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1428 - accuracy: 0.9471 - val_loss: 0.1659 - val_accuracy: 0.9385\n",
      "Epoch 223/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1427 - accuracy: 0.9471 - val_loss: 0.1659 - val_accuracy: 0.9386\n",
      "Epoch 224/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1425 - accuracy: 0.9471 - val_loss: 0.1656 - val_accuracy: 0.9385\n",
      "Epoch 225/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1424 - accuracy: 0.9470 - val_loss: 0.1661 - val_accuracy: 0.9387\n",
      "Epoch 226/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1423 - accuracy: 0.9472 - val_loss: 0.1662 - val_accuracy: 0.9385\n",
      "Epoch 227/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1421 - accuracy: 0.9473 - val_loss: 0.1652 - val_accuracy: 0.9383\n",
      "Epoch 228/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1420 - accuracy: 0.9473 - val_loss: 0.1654 - val_accuracy: 0.9383\n",
      "Epoch 229/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1419 - accuracy: 0.9473 - val_loss: 0.1656 - val_accuracy: 0.9383\n",
      "Epoch 230/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1417 - accuracy: 0.9473 - val_loss: 0.1655 - val_accuracy: 0.9385\n",
      "Epoch 231/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1416 - accuracy: 0.9473 - val_loss: 0.1655 - val_accuracy: 0.9383\n",
      "Epoch 232/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1414 - accuracy: 0.9476 - val_loss: 0.1655 - val_accuracy: 0.9384\n",
      "Epoch 233/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1413 - accuracy: 0.9475 - val_loss: 0.1653 - val_accuracy: 0.9383\n",
      "Epoch 234/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1411 - accuracy: 0.9475 - val_loss: 0.1650 - val_accuracy: 0.9383\n",
      "Epoch 235/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1410 - accuracy: 0.9475 - val_loss: 0.1649 - val_accuracy: 0.9385\n",
      "Epoch 236/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1408 - accuracy: 0.9477 - val_loss: 0.1647 - val_accuracy: 0.9386\n",
      "Epoch 237/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1407 - accuracy: 0.9476 - val_loss: 0.1647 - val_accuracy: 0.9386\n",
      "Epoch 238/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1406 - accuracy: 0.9476 - val_loss: 0.1646 - val_accuracy: 0.9388\n",
      "Epoch 239/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1405 - accuracy: 0.9477 - val_loss: 0.1644 - val_accuracy: 0.9383\n",
      "Epoch 240/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1403 - accuracy: 0.9478 - val_loss: 0.1644 - val_accuracy: 0.9386\n",
      "Epoch 241/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1401 - accuracy: 0.9478 - val_loss: 0.1642 - val_accuracy: 0.9387\n",
      "Epoch 242/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1401 - accuracy: 0.9480 - val_loss: 0.1645 - val_accuracy: 0.9382\n",
      "Epoch 243/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1399 - accuracy: 0.9480 - val_loss: 0.1643 - val_accuracy: 0.9387\n",
      "Epoch 244/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1398 - accuracy: 0.9479 - val_loss: 0.1642 - val_accuracy: 0.9388\n",
      "Epoch 245/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1396 - accuracy: 0.9480 - val_loss: 0.1643 - val_accuracy: 0.9387\n",
      "Epoch 246/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1396 - accuracy: 0.9481 - val_loss: 0.1636 - val_accuracy: 0.9389\n",
      "Epoch 247/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1394 - accuracy: 0.9481 - val_loss: 0.1641 - val_accuracy: 0.9388\n",
      "Epoch 248/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1393 - accuracy: 0.9482 - val_loss: 0.1639 - val_accuracy: 0.9387\n",
      "Epoch 249/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1392 - accuracy: 0.9482 - val_loss: 0.1636 - val_accuracy: 0.9391\n",
      "Epoch 250/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1390 - accuracy: 0.9483 - val_loss: 0.1635 - val_accuracy: 0.9393\n",
      "Epoch 251/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1389 - accuracy: 0.9482 - val_loss: 0.1634 - val_accuracy: 0.9391\n",
      "Epoch 252/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1388 - accuracy: 0.9483 - val_loss: 0.1636 - val_accuracy: 0.9393\n",
      "Epoch 253/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1386 - accuracy: 0.9483 - val_loss: 0.1640 - val_accuracy: 0.9387\n",
      "Epoch 254/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1385 - accuracy: 0.9484 - val_loss: 0.1634 - val_accuracy: 0.9386\n",
      "Epoch 255/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1383 - accuracy: 0.9485 - val_loss: 0.1631 - val_accuracy: 0.9387\n",
      "Epoch 256/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1382 - accuracy: 0.9485 - val_loss: 0.1635 - val_accuracy: 0.9386\n",
      "Epoch 257/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1380 - accuracy: 0.9486 - val_loss: 0.1633 - val_accuracy: 0.9392\n",
      "Epoch 258/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1381 - accuracy: 0.9484 - val_loss: 0.1640 - val_accuracy: 0.9391\n",
      "Epoch 259/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1379 - accuracy: 0.9486 - val_loss: 0.1635 - val_accuracy: 0.9387\n",
      "Epoch 260/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1377 - accuracy: 0.9485 - val_loss: 0.1632 - val_accuracy: 0.9391\n",
      "Epoch 261/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1377 - accuracy: 0.9486 - val_loss: 0.1628 - val_accuracy: 0.9394\n",
      "Epoch 262/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1375 - accuracy: 0.9487 - val_loss: 0.1627 - val_accuracy: 0.9394\n",
      "Epoch 263/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1374 - accuracy: 0.9487 - val_loss: 0.1625 - val_accuracy: 0.9392\n",
      "Epoch 264/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1373 - accuracy: 0.9489 - val_loss: 0.1624 - val_accuracy: 0.9394\n",
      "Epoch 265/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1371 - accuracy: 0.9487 - val_loss: 0.1627 - val_accuracy: 0.9391\n",
      "Epoch 266/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1370 - accuracy: 0.9488 - val_loss: 0.1622 - val_accuracy: 0.9390\n",
      "Epoch 267/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1369 - accuracy: 0.9489 - val_loss: 0.1625 - val_accuracy: 0.9396\n",
      "Epoch 268/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1368 - accuracy: 0.9490 - val_loss: 0.1626 - val_accuracy: 0.9391\n",
      "Epoch 269/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1367 - accuracy: 0.9489 - val_loss: 0.1622 - val_accuracy: 0.9394\n",
      "Epoch 270/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1366 - accuracy: 0.9490 - val_loss: 0.1624 - val_accuracy: 0.9392\n",
      "Epoch 271/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1364 - accuracy: 0.9492 - val_loss: 0.1624 - val_accuracy: 0.9389\n",
      "Epoch 272/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1362 - accuracy: 0.9490 - val_loss: 0.1630 - val_accuracy: 0.9390\n",
      "Epoch 273/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1362 - accuracy: 0.9490 - val_loss: 0.1625 - val_accuracy: 0.9389\n",
      "Epoch 274/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1361 - accuracy: 0.9491 - val_loss: 0.1616 - val_accuracy: 0.9395\n",
      "Epoch 275/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1360 - accuracy: 0.9492 - val_loss: 0.1619 - val_accuracy: 0.9396\n",
      "Epoch 276/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1358 - accuracy: 0.9494 - val_loss: 0.1615 - val_accuracy: 0.9393\n",
      "Epoch 277/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1357 - accuracy: 0.9493 - val_loss: 0.1624 - val_accuracy: 0.9391\n",
      "Epoch 278/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1356 - accuracy: 0.9494 - val_loss: 0.1621 - val_accuracy: 0.9393\n",
      "Epoch 279/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1355 - accuracy: 0.9495 - val_loss: 0.1622 - val_accuracy: 0.9395\n",
      "Epoch 280/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1354 - accuracy: 0.9493 - val_loss: 0.1616 - val_accuracy: 0.9394\n",
      "Epoch 281/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1353 - accuracy: 0.9494 - val_loss: 0.1613 - val_accuracy: 0.9395\n",
      "Epoch 282/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1352 - accuracy: 0.9494 - val_loss: 0.1613 - val_accuracy: 0.9398\n",
      "Epoch 283/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1351 - accuracy: 0.9495 - val_loss: 0.1621 - val_accuracy: 0.9398\n",
      "Epoch 284/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1349 - accuracy: 0.9495 - val_loss: 0.1613 - val_accuracy: 0.9395\n",
      "Epoch 285/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1348 - accuracy: 0.9496 - val_loss: 0.1611 - val_accuracy: 0.9397\n",
      "Epoch 286/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1347 - accuracy: 0.9495 - val_loss: 0.1607 - val_accuracy: 0.9396\n",
      "Epoch 287/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1346 - accuracy: 0.9496 - val_loss: 0.1612 - val_accuracy: 0.9396\n",
      "Epoch 288/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1345 - accuracy: 0.9496 - val_loss: 0.1613 - val_accuracy: 0.9393\n",
      "Epoch 289/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1344 - accuracy: 0.9496 - val_loss: 0.1612 - val_accuracy: 0.9389\n",
      "Epoch 290/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1343 - accuracy: 0.9498 - val_loss: 0.1616 - val_accuracy: 0.9400\n",
      "Epoch 291/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1342 - accuracy: 0.9499 - val_loss: 0.1612 - val_accuracy: 0.9394\n",
      "Epoch 292/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1340 - accuracy: 0.9498 - val_loss: 0.1610 - val_accuracy: 0.9399\n",
      "Epoch 293/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1339 - accuracy: 0.9498 - val_loss: 0.1610 - val_accuracy: 0.9399\n",
      "Epoch 294/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1338 - accuracy: 0.9501 - val_loss: 0.1607 - val_accuracy: 0.9401\n",
      "Epoch 295/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1337 - accuracy: 0.9498 - val_loss: 0.1608 - val_accuracy: 0.9399\n",
      "Epoch 296/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1336 - accuracy: 0.9500 - val_loss: 0.1607 - val_accuracy: 0.9400\n",
      "Epoch 297/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1336 - accuracy: 0.9500 - val_loss: 0.1606 - val_accuracy: 0.9396\n",
      "Epoch 298/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1334 - accuracy: 0.9502 - val_loss: 0.1606 - val_accuracy: 0.9397\n",
      "Epoch 299/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1332 - accuracy: 0.9501 - val_loss: 0.1605 - val_accuracy: 0.9396\n",
      "Epoch 300/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1332 - accuracy: 0.9503 - val_loss: 0.1607 - val_accuracy: 0.9401\n",
      "Epoch 301/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1331 - accuracy: 0.9499 - val_loss: 0.1604 - val_accuracy: 0.9398\n",
      "Epoch 302/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1329 - accuracy: 0.9503 - val_loss: 0.1608 - val_accuracy: 0.9399\n",
      "Epoch 303/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1328 - accuracy: 0.9502 - val_loss: 0.1601 - val_accuracy: 0.9396\n",
      "Epoch 304/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1328 - accuracy: 0.9502 - val_loss: 0.1602 - val_accuracy: 0.9403\n",
      "Epoch 305/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1327 - accuracy: 0.9502 - val_loss: 0.1600 - val_accuracy: 0.9398\n",
      "Epoch 306/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1326 - accuracy: 0.9503 - val_loss: 0.1602 - val_accuracy: 0.9400\n",
      "Epoch 307/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1325 - accuracy: 0.9504 - val_loss: 0.1603 - val_accuracy: 0.9399\n",
      "Epoch 308/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1324 - accuracy: 0.9503 - val_loss: 0.1601 - val_accuracy: 0.9398\n",
      "Epoch 309/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1323 - accuracy: 0.9503 - val_loss: 0.1602 - val_accuracy: 0.9404\n",
      "Epoch 310/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1322 - accuracy: 0.9504 - val_loss: 0.1601 - val_accuracy: 0.9399\n",
      "Epoch 311/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1321 - accuracy: 0.9504 - val_loss: 0.1601 - val_accuracy: 0.9400\n",
      "Epoch 312/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1319 - accuracy: 0.9504 - val_loss: 0.1599 - val_accuracy: 0.9406\n",
      "Epoch 313/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1318 - accuracy: 0.9504 - val_loss: 0.1599 - val_accuracy: 0.9397\n",
      "Epoch 314/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1317 - accuracy: 0.9505 - val_loss: 0.1605 - val_accuracy: 0.9396\n",
      "Epoch 315/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1315 - accuracy: 0.9507 - val_loss: 0.1598 - val_accuracy: 0.9399\n",
      "Epoch 316/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1315 - accuracy: 0.9507 - val_loss: 0.1599 - val_accuracy: 0.9397\n",
      "Epoch 317/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1314 - accuracy: 0.9507 - val_loss: 0.1604 - val_accuracy: 0.9396\n",
      "Epoch 318/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1313 - accuracy: 0.9507 - val_loss: 0.1597 - val_accuracy: 0.9402\n",
      "Epoch 319/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1313 - accuracy: 0.9507 - val_loss: 0.1594 - val_accuracy: 0.9399\n",
      "Epoch 320/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1311 - accuracy: 0.9509 - val_loss: 0.1596 - val_accuracy: 0.9399\n",
      "Epoch 321/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1310 - accuracy: 0.9507 - val_loss: 0.1592 - val_accuracy: 0.9405\n",
      "Epoch 322/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1309 - accuracy: 0.9508 - val_loss: 0.1593 - val_accuracy: 0.9405\n",
      "Epoch 323/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1308 - accuracy: 0.9508 - val_loss: 0.1598 - val_accuracy: 0.9405\n",
      "Epoch 324/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1308 - accuracy: 0.9508 - val_loss: 0.1594 - val_accuracy: 0.9404\n",
      "Epoch 325/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.1594 - val_accuracy: 0.9406\n",
      "Epoch 326/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1306 - accuracy: 0.9510 - val_loss: 0.1592 - val_accuracy: 0.9401\n",
      "Epoch 327/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1305 - accuracy: 0.9508 - val_loss: 0.1591 - val_accuracy: 0.9405\n",
      "Epoch 328/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1304 - accuracy: 0.9511 - val_loss: 0.1589 - val_accuracy: 0.9404\n",
      "Epoch 329/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1302 - accuracy: 0.9510 - val_loss: 0.1590 - val_accuracy: 0.9404\n",
      "Epoch 330/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1302 - accuracy: 0.9510 - val_loss: 0.1600 - val_accuracy: 0.9405\n",
      "Epoch 331/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1300 - accuracy: 0.9510 - val_loss: 0.1591 - val_accuracy: 0.9399\n",
      "Epoch 332/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1300 - accuracy: 0.9510 - val_loss: 0.1590 - val_accuracy: 0.9406\n",
      "Epoch 333/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1299 - accuracy: 0.9513 - val_loss: 0.1589 - val_accuracy: 0.9405\n",
      "Epoch 334/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1298 - accuracy: 0.9510 - val_loss: 0.1586 - val_accuracy: 0.9404\n",
      "Epoch 335/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1297 - accuracy: 0.9511 - val_loss: 0.1591 - val_accuracy: 0.9402\n",
      "Epoch 336/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1296 - accuracy: 0.9512 - val_loss: 0.1596 - val_accuracy: 0.9396\n",
      "Epoch 337/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1295 - accuracy: 0.9513 - val_loss: 0.1588 - val_accuracy: 0.9403\n",
      "Epoch 338/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1294 - accuracy: 0.9514 - val_loss: 0.1595 - val_accuracy: 0.9400\n",
      "Epoch 339/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1294 - accuracy: 0.9514 - val_loss: 0.1589 - val_accuracy: 0.9406\n",
      "Epoch 340/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1292 - accuracy: 0.9513 - val_loss: 0.1593 - val_accuracy: 0.9404\n",
      "Epoch 341/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1292 - accuracy: 0.9514 - val_loss: 0.1590 - val_accuracy: 0.9406\n",
      "Epoch 342/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1290 - accuracy: 0.9513 - val_loss: 0.1590 - val_accuracy: 0.9405\n",
      "Epoch 343/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1290 - accuracy: 0.9513 - val_loss: 0.1586 - val_accuracy: 0.9405\n",
      "Epoch 344/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1289 - accuracy: 0.9515 - val_loss: 0.1582 - val_accuracy: 0.9402\n",
      "Epoch 345/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1288 - accuracy: 0.9515 - val_loss: 0.1588 - val_accuracy: 0.9402\n",
      "Epoch 346/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1287 - accuracy: 0.9516 - val_loss: 0.1588 - val_accuracy: 0.9406\n",
      "Epoch 347/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1286 - accuracy: 0.9516 - val_loss: 0.1588 - val_accuracy: 0.9402\n",
      "Epoch 348/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1285 - accuracy: 0.9515 - val_loss: 0.1585 - val_accuracy: 0.9408\n",
      "Epoch 349/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1284 - accuracy: 0.9515 - val_loss: 0.1584 - val_accuracy: 0.9403\n",
      "Epoch 350/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1284 - accuracy: 0.9516 - val_loss: 0.1588 - val_accuracy: 0.9403\n",
      "Epoch 351/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1283 - accuracy: 0.9516 - val_loss: 0.1585 - val_accuracy: 0.9405\n",
      "Epoch 352/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1281 - accuracy: 0.9517 - val_loss: 0.1584 - val_accuracy: 0.9405\n",
      "Epoch 353/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1282 - accuracy: 0.9516 - val_loss: 0.1585 - val_accuracy: 0.9409\n",
      "Epoch 354/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1280 - accuracy: 0.9518 - val_loss: 0.1580 - val_accuracy: 0.9410\n",
      "Epoch 355/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1280 - accuracy: 0.9517 - val_loss: 0.1586 - val_accuracy: 0.9400\n",
      "Epoch 356/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1279 - accuracy: 0.9518 - val_loss: 0.1592 - val_accuracy: 0.9405\n",
      "Epoch 357/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1278 - accuracy: 0.9517 - val_loss: 0.1588 - val_accuracy: 0.9407\n",
      "Epoch 358/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1276 - accuracy: 0.9519 - val_loss: 0.1585 - val_accuracy: 0.9408\n",
      "Epoch 359/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1275 - accuracy: 0.9519 - val_loss: 0.1579 - val_accuracy: 0.9408\n",
      "Epoch 360/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1275 - accuracy: 0.9518 - val_loss: 0.1587 - val_accuracy: 0.9407\n",
      "Epoch 361/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1275 - accuracy: 0.9519 - val_loss: 0.1584 - val_accuracy: 0.9410\n",
      "Epoch 362/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1274 - accuracy: 0.9518 - val_loss: 0.1581 - val_accuracy: 0.9407\n",
      "Epoch 363/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1272 - accuracy: 0.9517 - val_loss: 0.1578 - val_accuracy: 0.9406\n",
      "Epoch 364/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1271 - accuracy: 0.9521 - val_loss: 0.1589 - val_accuracy: 0.9404\n",
      "Epoch 365/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1270 - accuracy: 0.9520 - val_loss: 0.1578 - val_accuracy: 0.9405\n",
      "Epoch 366/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1270 - accuracy: 0.9520 - val_loss: 0.1579 - val_accuracy: 0.9407\n",
      "Epoch 367/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1269 - accuracy: 0.9520 - val_loss: 0.1581 - val_accuracy: 0.9404\n",
      "Epoch 368/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1268 - accuracy: 0.9521 - val_loss: 0.1583 - val_accuracy: 0.9408\n",
      "Epoch 369/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1268 - accuracy: 0.9519 - val_loss: 0.1583 - val_accuracy: 0.9405\n",
      "Epoch 370/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1267 - accuracy: 0.9521 - val_loss: 0.1582 - val_accuracy: 0.9405\n",
      "Epoch 371/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1266 - accuracy: 0.9522 - val_loss: 0.1580 - val_accuracy: 0.9399\n",
      "Epoch 372/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1265 - accuracy: 0.9521 - val_loss: 0.1580 - val_accuracy: 0.9409\n",
      "Epoch 373/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1264 - accuracy: 0.9522 - val_loss: 0.1583 - val_accuracy: 0.9409\n",
      "Epoch 374/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1262 - accuracy: 0.9522 - val_loss: 0.1583 - val_accuracy: 0.9408\n",
      "Epoch 375/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1263 - accuracy: 0.9522 - val_loss: 0.1579 - val_accuracy: 0.9406\n",
      "Epoch 376/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1261 - accuracy: 0.9523 - val_loss: 0.1587 - val_accuracy: 0.9402\n",
      "Epoch 377/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1260 - accuracy: 0.9523 - val_loss: 0.1584 - val_accuracy: 0.9410\n",
      "Epoch 378/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1260 - accuracy: 0.9523 - val_loss: 0.1579 - val_accuracy: 0.9409\n",
      "Epoch 379/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1260 - accuracy: 0.9524 - val_loss: 0.1581 - val_accuracy: 0.9405\n",
      "Epoch 380/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1258 - accuracy: 0.9524 - val_loss: 0.1590 - val_accuracy: 0.9401\n",
      "Epoch 381/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1258 - accuracy: 0.9524 - val_loss: 0.1581 - val_accuracy: 0.9410\n",
      "Epoch 382/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1257 - accuracy: 0.9522 - val_loss: 0.1578 - val_accuracy: 0.9404\n",
      "Epoch 383/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1256 - accuracy: 0.9524 - val_loss: 0.1584 - val_accuracy: 0.9406\n",
      "Epoch 384/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1256 - accuracy: 0.9521 - val_loss: 0.1572 - val_accuracy: 0.9401\n",
      "Epoch 385/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1254 - accuracy: 0.9525 - val_loss: 0.1584 - val_accuracy: 0.9410\n",
      "Epoch 386/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1254 - accuracy: 0.9525 - val_loss: 0.1575 - val_accuracy: 0.9405\n",
      "Epoch 387/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1254 - accuracy: 0.9525 - val_loss: 0.1577 - val_accuracy: 0.9407\n",
      "Epoch 388/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1252 - accuracy: 0.9525 - val_loss: 0.1578 - val_accuracy: 0.9411\n",
      "Epoch 389/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1252 - accuracy: 0.9524 - val_loss: 0.1577 - val_accuracy: 0.9405\n",
      "Epoch 390/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1251 - accuracy: 0.9526 - val_loss: 0.1576 - val_accuracy: 0.9405\n",
      "Epoch 391/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1250 - accuracy: 0.9525 - val_loss: 0.1581 - val_accuracy: 0.9400\n",
      "Epoch 392/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1250 - accuracy: 0.9525 - val_loss: 0.1578 - val_accuracy: 0.9411\n",
      "Epoch 393/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1248 - accuracy: 0.9527 - val_loss: 0.1573 - val_accuracy: 0.9408\n",
      "Epoch 394/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1248 - accuracy: 0.9529 - val_loss: 0.1577 - val_accuracy: 0.9409\n",
      "Epoch 395/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1247 - accuracy: 0.9527 - val_loss: 0.1579 - val_accuracy: 0.9411\n",
      "Epoch 396/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1246 - accuracy: 0.9526 - val_loss: 0.1575 - val_accuracy: 0.9409\n",
      "Epoch 397/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1245 - accuracy: 0.9529 - val_loss: 0.1577 - val_accuracy: 0.9406\n",
      "Epoch 398/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1244 - accuracy: 0.9525 - val_loss: 0.1581 - val_accuracy: 0.9405\n",
      "Epoch 399/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1245 - accuracy: 0.9525 - val_loss: 0.1574 - val_accuracy: 0.9412\n",
      "Epoch 400/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1243 - accuracy: 0.9527 - val_loss: 0.1574 - val_accuracy: 0.9407\n",
      "Epoch 401/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1243 - accuracy: 0.9527 - val_loss: 0.1575 - val_accuracy: 0.9412\n",
      "Epoch 402/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1242 - accuracy: 0.9527 - val_loss: 0.1572 - val_accuracy: 0.9407\n",
      "Epoch 403/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1241 - accuracy: 0.9528 - val_loss: 0.1578 - val_accuracy: 0.9407\n",
      "Epoch 404/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1241 - accuracy: 0.9528 - val_loss: 0.1575 - val_accuracy: 0.9408\n",
      "Epoch 405/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1240 - accuracy: 0.9529 - val_loss: 0.1576 - val_accuracy: 0.9406\n",
      "Epoch 406/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1239 - accuracy: 0.9528 - val_loss: 0.1581 - val_accuracy: 0.9403\n",
      "Epoch 407/800\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.1238 - accuracy: 0.9530 - val_loss: 0.1568 - val_accuracy: 0.9411\n",
      "Epoch 408/800\n",
      "6000/6000 [==============================] - 1s 93us/step - loss: 0.1238 - accuracy: 0.9529 - val_loss: 0.1574 - val_accuracy: 0.9406\n",
      "Epoch 409/800\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.1236 - accuracy: 0.9530 - val_loss: 0.1575 - val_accuracy: 0.9409\n",
      "Epoch 410/800\n",
      "6000/6000 [==============================] - 1s 88us/step - loss: 0.1235 - accuracy: 0.9530 - val_loss: 0.1579 - val_accuracy: 0.9410\n",
      "Epoch 411/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1235 - accuracy: 0.9530 - val_loss: 0.1572 - val_accuracy: 0.9407\n",
      "Epoch 412/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1234 - accuracy: 0.9530 - val_loss: 0.1576 - val_accuracy: 0.9409\n",
      "Epoch 413/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1234 - accuracy: 0.9529 - val_loss: 0.1576 - val_accuracy: 0.9406\n",
      "Epoch 414/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1233 - accuracy: 0.9528 - val_loss: 0.1578 - val_accuracy: 0.9406\n",
      "Epoch 415/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1232 - accuracy: 0.9531 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
      "Epoch 416/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1232 - accuracy: 0.9531 - val_loss: 0.1581 - val_accuracy: 0.9409\n",
      "Epoch 417/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1231 - accuracy: 0.9532 - val_loss: 0.1575 - val_accuracy: 0.9407\n",
      "Epoch 418/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1230 - accuracy: 0.9532 - val_loss: 0.1574 - val_accuracy: 0.9406\n",
      "Epoch 419/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1230 - accuracy: 0.9531 - val_loss: 0.1578 - val_accuracy: 0.9401\n",
      "Epoch 420/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1229 - accuracy: 0.9532 - val_loss: 0.1575 - val_accuracy: 0.9404\n",
      "Epoch 421/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1228 - accuracy: 0.9532 - val_loss: 0.1575 - val_accuracy: 0.9409\n",
      "Epoch 422/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1228 - accuracy: 0.9532 - val_loss: 0.1572 - val_accuracy: 0.9405\n",
      "Epoch 423/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1226 - accuracy: 0.9532 - val_loss: 0.1576 - val_accuracy: 0.9411\n",
      "Epoch 424/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1225 - accuracy: 0.9531 - val_loss: 0.1570 - val_accuracy: 0.9407\n",
      "Epoch 425/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1225 - accuracy: 0.9532 - val_loss: 0.1577 - val_accuracy: 0.9408\n",
      "Epoch 426/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1224 - accuracy: 0.9531 - val_loss: 0.1574 - val_accuracy: 0.9412\n",
      "Epoch 427/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1224 - accuracy: 0.9532 - val_loss: 0.1574 - val_accuracy: 0.9409\n",
      "Epoch 428/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1223 - accuracy: 0.9534 - val_loss: 0.1576 - val_accuracy: 0.9400\n",
      "Epoch 429/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1222 - accuracy: 0.9532 - val_loss: 0.1570 - val_accuracy: 0.9408\n",
      "Epoch 430/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1222 - accuracy: 0.9534 - val_loss: 0.1577 - val_accuracy: 0.9404\n",
      "Epoch 431/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1220 - accuracy: 0.9534 - val_loss: 0.1582 - val_accuracy: 0.9409\n",
      "Epoch 432/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1221 - accuracy: 0.9534 - val_loss: 0.1577 - val_accuracy: 0.9407\n",
      "Epoch 433/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1220 - accuracy: 0.9533 - val_loss: 0.1573 - val_accuracy: 0.9407\n",
      "Epoch 434/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1219 - accuracy: 0.9535 - val_loss: 0.1574 - val_accuracy: 0.9409\n",
      "Epoch 435/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1218 - accuracy: 0.9532 - val_loss: 0.1574 - val_accuracy: 0.9406\n",
      "Epoch 436/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1217 - accuracy: 0.9535 - val_loss: 0.1579 - val_accuracy: 0.9403\n",
      "Epoch 437/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1217 - accuracy: 0.9536 - val_loss: 0.1573 - val_accuracy: 0.9411\n",
      "Epoch 438/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1216 - accuracy: 0.9535 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
      "Epoch 439/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1215 - accuracy: 0.9534 - val_loss: 0.1575 - val_accuracy: 0.9414\n",
      "Epoch 440/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1214 - accuracy: 0.9536 - val_loss: 0.1574 - val_accuracy: 0.9408\n",
      "Epoch 441/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1213 - accuracy: 0.9536 - val_loss: 0.1573 - val_accuracy: 0.9404\n",
      "Epoch 442/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1214 - accuracy: 0.9536 - val_loss: 0.1575 - val_accuracy: 0.9405\n",
      "Epoch 443/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1212 - accuracy: 0.9538 - val_loss: 0.1582 - val_accuracy: 0.9408\n",
      "Epoch 444/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1212 - accuracy: 0.9534 - val_loss: 0.1572 - val_accuracy: 0.9407\n",
      "Epoch 445/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1211 - accuracy: 0.9537 - val_loss: 0.1580 - val_accuracy: 0.9407\n",
      "Epoch 446/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1210 - accuracy: 0.9538 - val_loss: 0.1573 - val_accuracy: 0.9409\n",
      "Epoch 447/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1210 - accuracy: 0.9538 - val_loss: 0.1579 - val_accuracy: 0.9406\n",
      "Epoch 448/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1209 - accuracy: 0.9536 - val_loss: 0.1569 - val_accuracy: 0.9409\n",
      "Epoch 449/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1209 - accuracy: 0.9537 - val_loss: 0.1581 - val_accuracy: 0.9408\n",
      "Epoch 450/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1208 - accuracy: 0.9538 - val_loss: 0.1583 - val_accuracy: 0.9402\n",
      "Epoch 451/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1207 - accuracy: 0.9538 - val_loss: 0.1573 - val_accuracy: 0.9412\n",
      "Epoch 452/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1207 - accuracy: 0.9536 - val_loss: 0.1579 - val_accuracy: 0.9409\n",
      "Epoch 453/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1205 - accuracy: 0.9536 - val_loss: 0.1583 - val_accuracy: 0.9404\n",
      "Epoch 454/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1205 - accuracy: 0.9537 - val_loss: 0.1572 - val_accuracy: 0.9409\n",
      "Epoch 455/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1205 - accuracy: 0.9538 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
      "Epoch 456/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1204 - accuracy: 0.9538 - val_loss: 0.1577 - val_accuracy: 0.9403\n",
      "Epoch 457/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1203 - accuracy: 0.9540 - val_loss: 0.1572 - val_accuracy: 0.9406\n",
      "Epoch 458/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1204 - accuracy: 0.9538 - val_loss: 0.1578 - val_accuracy: 0.9405\n",
      "Epoch 459/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1202 - accuracy: 0.9539 - val_loss: 0.1584 - val_accuracy: 0.9409\n",
      "Epoch 460/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1200 - accuracy: 0.9539 - val_loss: 0.1580 - val_accuracy: 0.9401\n",
      "Epoch 461/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1200 - accuracy: 0.9540 - val_loss: 0.1577 - val_accuracy: 0.9404\n",
      "Epoch 462/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1200 - accuracy: 0.9539 - val_loss: 0.1577 - val_accuracy: 0.9408\n",
      "Epoch 463/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1199 - accuracy: 0.9541 - val_loss: 0.1569 - val_accuracy: 0.9407\n",
      "Epoch 464/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1198 - accuracy: 0.9540 - val_loss: 0.1576 - val_accuracy: 0.9402\n",
      "Epoch 465/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1198 - accuracy: 0.9540 - val_loss: 0.1570 - val_accuracy: 0.9411\n",
      "Epoch 466/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1197 - accuracy: 0.9540 - val_loss: 0.1585 - val_accuracy: 0.9404\n",
      "Epoch 467/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1196 - accuracy: 0.9542 - val_loss: 0.1577 - val_accuracy: 0.9404\n",
      "Epoch 468/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1196 - accuracy: 0.9541 - val_loss: 0.1575 - val_accuracy: 0.9410\n",
      "Epoch 469/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1195 - accuracy: 0.9543 - val_loss: 0.1583 - val_accuracy: 0.9407\n",
      "Epoch 470/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1195 - accuracy: 0.9542 - val_loss: 0.1575 - val_accuracy: 0.9407\n",
      "Epoch 471/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1194 - accuracy: 0.9542 - val_loss: 0.1576 - val_accuracy: 0.9403\n",
      "Epoch 472/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1193 - accuracy: 0.9539 - val_loss: 0.1574 - val_accuracy: 0.9411\n",
      "Epoch 473/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1191 - accuracy: 0.9541 - val_loss: 0.1572 - val_accuracy: 0.9406\n",
      "Epoch 474/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1191 - accuracy: 0.9540 - val_loss: 0.1574 - val_accuracy: 0.9406\n",
      "Epoch 475/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1191 - accuracy: 0.9541 - val_loss: 0.1580 - val_accuracy: 0.9408\n",
      "Epoch 476/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1191 - accuracy: 0.9541 - val_loss: 0.1577 - val_accuracy: 0.9406\n",
      "Epoch 477/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1191 - accuracy: 0.9542 - val_loss: 0.1575 - val_accuracy: 0.9402\n",
      "Epoch 478/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1189 - accuracy: 0.9542 - val_loss: 0.1575 - val_accuracy: 0.9406\n",
      "Epoch 479/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1188 - accuracy: 0.9543 - val_loss: 0.1569 - val_accuracy: 0.9410\n",
      "Epoch 480/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1188 - accuracy: 0.9543 - val_loss: 0.1573 - val_accuracy: 0.9411\n",
      "Epoch 481/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1188 - accuracy: 0.9544 - val_loss: 0.1579 - val_accuracy: 0.9408\n",
      "Epoch 482/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1186 - accuracy: 0.9543 - val_loss: 0.1588 - val_accuracy: 0.9403\n",
      "Epoch 483/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1186 - accuracy: 0.9546 - val_loss: 0.1575 - val_accuracy: 0.9408\n",
      "Epoch 484/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1185 - accuracy: 0.9543 - val_loss: 0.1572 - val_accuracy: 0.9407\n",
      "Epoch 485/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1183 - accuracy: 0.9546 - val_loss: 0.1577 - val_accuracy: 0.9406\n",
      "Epoch 486/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1184 - accuracy: 0.9543 - val_loss: 0.1574 - val_accuracy: 0.9406\n",
      "Epoch 487/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1184 - accuracy: 0.9543 - val_loss: 0.1579 - val_accuracy: 0.9405\n",
      "Epoch 488/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1183 - accuracy: 0.9545 - val_loss: 0.1578 - val_accuracy: 0.9409\n",
      "Epoch 489/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1182 - accuracy: 0.9545 - val_loss: 0.1574 - val_accuracy: 0.9407\n",
      "Epoch 490/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1181 - accuracy: 0.9544 - val_loss: 0.1573 - val_accuracy: 0.9405\n",
      "Epoch 491/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1181 - accuracy: 0.9544 - val_loss: 0.1570 - val_accuracy: 0.9409\n",
      "Epoch 492/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1180 - accuracy: 0.9547 - val_loss: 0.1583 - val_accuracy: 0.9405\n",
      "Epoch 493/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1179 - accuracy: 0.9547 - val_loss: 0.1576 - val_accuracy: 0.9404\n",
      "Epoch 494/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1180 - accuracy: 0.9545 - val_loss: 0.1577 - val_accuracy: 0.9402\n",
      "Epoch 495/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1179 - accuracy: 0.9545 - val_loss: 0.1577 - val_accuracy: 0.9408\n",
      "Epoch 496/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1177 - accuracy: 0.9547 - val_loss: 0.1573 - val_accuracy: 0.9409\n",
      "Epoch 497/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1178 - accuracy: 0.9547 - val_loss: 0.1584 - val_accuracy: 0.9408\n",
      "Epoch 498/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1176 - accuracy: 0.9547 - val_loss: 0.1571 - val_accuracy: 0.9406\n",
      "Epoch 499/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1176 - accuracy: 0.9545 - val_loss: 0.1575 - val_accuracy: 0.9406\n",
      "Epoch 500/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1175 - accuracy: 0.9547 - val_loss: 0.1580 - val_accuracy: 0.9410\n",
      "Epoch 501/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1175 - accuracy: 0.9547 - val_loss: 0.1576 - val_accuracy: 0.9412\n",
      "Epoch 502/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1174 - accuracy: 0.9547 - val_loss: 0.1580 - val_accuracy: 0.9415\n",
      "Epoch 503/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1172 - accuracy: 0.9549 - val_loss: 0.1577 - val_accuracy: 0.9411\n",
      "Epoch 504/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1171 - accuracy: 0.9548 - val_loss: 0.1580 - val_accuracy: 0.9405\n",
      "Epoch 505/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1173 - accuracy: 0.9548 - val_loss: 0.1576 - val_accuracy: 0.9404\n",
      "Epoch 506/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1172 - accuracy: 0.9546 - val_loss: 0.1583 - val_accuracy: 0.9402\n",
      "Epoch 507/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1171 - accuracy: 0.9550 - val_loss: 0.1580 - val_accuracy: 0.9409\n",
      "Epoch 508/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1170 - accuracy: 0.9548 - val_loss: 0.1575 - val_accuracy: 0.9408\n",
      "Epoch 509/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1169 - accuracy: 0.9550 - val_loss: 0.1581 - val_accuracy: 0.9406\n",
      "Epoch 510/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1168 - accuracy: 0.9550 - val_loss: 0.1576 - val_accuracy: 0.9414\n",
      "Epoch 511/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1168 - accuracy: 0.9550 - val_loss: 0.1580 - val_accuracy: 0.9410\n",
      "Epoch 512/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1168 - accuracy: 0.9548 - val_loss: 0.1589 - val_accuracy: 0.9406\n",
      "Epoch 513/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1167 - accuracy: 0.9551 - val_loss: 0.1587 - val_accuracy: 0.9405\n",
      "Epoch 514/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1166 - accuracy: 0.9549 - val_loss: 0.1585 - val_accuracy: 0.9399\n",
      "Epoch 515/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1165 - accuracy: 0.9551 - val_loss: 0.1578 - val_accuracy: 0.9408\n",
      "Epoch 516/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1165 - accuracy: 0.9551 - val_loss: 0.1577 - val_accuracy: 0.9408\n",
      "Epoch 517/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1164 - accuracy: 0.9550 - val_loss: 0.1590 - val_accuracy: 0.9406\n",
      "Epoch 518/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1164 - accuracy: 0.9550 - val_loss: 0.1581 - val_accuracy: 0.9407\n",
      "Epoch 519/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1163 - accuracy: 0.9550 - val_loss: 0.1585 - val_accuracy: 0.9401\n",
      "Epoch 520/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1162 - accuracy: 0.9552 - val_loss: 0.1576 - val_accuracy: 0.9410\n",
      "Epoch 521/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1162 - accuracy: 0.9551 - val_loss: 0.1586 - val_accuracy: 0.9407\n",
      "Epoch 522/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1161 - accuracy: 0.9551 - val_loss: 0.1582 - val_accuracy: 0.9407\n",
      "Epoch 523/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1161 - accuracy: 0.9552 - val_loss: 0.1580 - val_accuracy: 0.9409\n",
      "Epoch 524/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1160 - accuracy: 0.9551 - val_loss: 0.1583 - val_accuracy: 0.9404\n",
      "Epoch 525/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1158 - accuracy: 0.9554 - val_loss: 0.1579 - val_accuracy: 0.9409\n",
      "Epoch 526/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1158 - accuracy: 0.9553 - val_loss: 0.1589 - val_accuracy: 0.9398\n",
      "Epoch 527/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1158 - accuracy: 0.9552 - val_loss: 0.1594 - val_accuracy: 0.9404\n",
      "Epoch 528/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1156 - accuracy: 0.9554 - val_loss: 0.1582 - val_accuracy: 0.9405\n",
      "Epoch 529/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1157 - accuracy: 0.9554 - val_loss: 0.1581 - val_accuracy: 0.9399\n",
      "Epoch 530/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1157 - accuracy: 0.9550 - val_loss: 0.1581 - val_accuracy: 0.9409\n",
      "Epoch 531/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1153 - accuracy: 0.9554 - val_loss: 0.1591 - val_accuracy: 0.9402\n",
      "Epoch 532/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1154 - accuracy: 0.9557 - val_loss: 0.1588 - val_accuracy: 0.9399\n",
      "Epoch 533/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1154 - accuracy: 0.9554 - val_loss: 0.1588 - val_accuracy: 0.9404\n",
      "Epoch 534/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1154 - accuracy: 0.9556 - val_loss: 0.1580 - val_accuracy: 0.9410\n",
      "Epoch 535/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1153 - accuracy: 0.9554 - val_loss: 0.1587 - val_accuracy: 0.9407\n",
      "Epoch 536/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1153 - accuracy: 0.9554 - val_loss: 0.1583 - val_accuracy: 0.9401\n",
      "Epoch 537/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1152 - accuracy: 0.9556 - val_loss: 0.1577 - val_accuracy: 0.9409\n",
      "Epoch 538/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1151 - accuracy: 0.9553 - val_loss: 0.1581 - val_accuracy: 0.9401\n",
      "Epoch 539/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1151 - accuracy: 0.9554 - val_loss: 0.1590 - val_accuracy: 0.9403\n",
      "Epoch 540/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1149 - accuracy: 0.9556 - val_loss: 0.1589 - val_accuracy: 0.9401\n",
      "Epoch 541/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1149 - accuracy: 0.9554 - val_loss: 0.1579 - val_accuracy: 0.9406\n",
      "Epoch 542/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1148 - accuracy: 0.9558 - val_loss: 0.1592 - val_accuracy: 0.9405\n",
      "Epoch 543/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1149 - accuracy: 0.9556 - val_loss: 0.1585 - val_accuracy: 0.9406\n",
      "Epoch 544/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1147 - accuracy: 0.9556 - val_loss: 0.1589 - val_accuracy: 0.9407\n",
      "Epoch 545/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1147 - accuracy: 0.9556 - val_loss: 0.1586 - val_accuracy: 0.9406\n",
      "Epoch 546/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1147 - accuracy: 0.9555 - val_loss: 0.1579 - val_accuracy: 0.9407\n",
      "Epoch 547/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1145 - accuracy: 0.9556 - val_loss: 0.1582 - val_accuracy: 0.9406\n",
      "Epoch 548/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1145 - accuracy: 0.9559 - val_loss: 0.1586 - val_accuracy: 0.9402\n",
      "Epoch 549/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1144 - accuracy: 0.9556 - val_loss: 0.1586 - val_accuracy: 0.9407\n",
      "Epoch 550/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1143 - accuracy: 0.9557 - val_loss: 0.1588 - val_accuracy: 0.9407\n",
      "Epoch 551/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1142 - accuracy: 0.9558 - val_loss: 0.1584 - val_accuracy: 0.9404\n",
      "Epoch 552/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1142 - accuracy: 0.9556 - val_loss: 0.1587 - val_accuracy: 0.9402\n",
      "Epoch 553/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1142 - accuracy: 0.9558 - val_loss: 0.1589 - val_accuracy: 0.9401\n",
      "Epoch 554/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1141 - accuracy: 0.9557 - val_loss: 0.1588 - val_accuracy: 0.9405\n",
      "Epoch 555/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1140 - accuracy: 0.9558 - val_loss: 0.1588 - val_accuracy: 0.9405\n",
      "Epoch 556/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1139 - accuracy: 0.9559 - val_loss: 0.1589 - val_accuracy: 0.9400\n",
      "Epoch 557/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1139 - accuracy: 0.9558 - val_loss: 0.1584 - val_accuracy: 0.9405\n",
      "Epoch 558/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1139 - accuracy: 0.9558 - val_loss: 0.1579 - val_accuracy: 0.9407\n",
      "Epoch 559/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1137 - accuracy: 0.9561 - val_loss: 0.1594 - val_accuracy: 0.9410\n",
      "Epoch 560/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1137 - accuracy: 0.9561 - val_loss: 0.1597 - val_accuracy: 0.9405\n",
      "Epoch 561/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1136 - accuracy: 0.9560 - val_loss: 0.1591 - val_accuracy: 0.9397\n",
      "Epoch 562/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1136 - accuracy: 0.9559 - val_loss: 0.1590 - val_accuracy: 0.9401\n",
      "Epoch 563/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1135 - accuracy: 0.9558 - val_loss: 0.1586 - val_accuracy: 0.9400\n",
      "Epoch 564/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1135 - accuracy: 0.9561 - val_loss: 0.1591 - val_accuracy: 0.9404\n",
      "Epoch 565/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1133 - accuracy: 0.9561 - val_loss: 0.1598 - val_accuracy: 0.9407\n",
      "Epoch 566/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1134 - accuracy: 0.9560 - val_loss: 0.1590 - val_accuracy: 0.9399\n",
      "Epoch 567/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1133 - accuracy: 0.9560 - val_loss: 0.1596 - val_accuracy: 0.9403\n",
      "Epoch 568/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1132 - accuracy: 0.9562 - val_loss: 0.1603 - val_accuracy: 0.9408\n",
      "Epoch 569/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1131 - accuracy: 0.9561 - val_loss: 0.1592 - val_accuracy: 0.9407\n",
      "Epoch 570/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1132 - accuracy: 0.9563 - val_loss: 0.1592 - val_accuracy: 0.9399\n",
      "Epoch 571/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1131 - accuracy: 0.9559 - val_loss: 0.1593 - val_accuracy: 0.9408\n",
      "Epoch 572/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1130 - accuracy: 0.9562 - val_loss: 0.1602 - val_accuracy: 0.9402\n",
      "Epoch 573/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1129 - accuracy: 0.9564 - val_loss: 0.1586 - val_accuracy: 0.9406\n",
      "Epoch 574/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1129 - accuracy: 0.9562 - val_loss: 0.1597 - val_accuracy: 0.9401\n",
      "Epoch 575/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1127 - accuracy: 0.9564 - val_loss: 0.1596 - val_accuracy: 0.9402\n",
      "Epoch 576/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1128 - accuracy: 0.9564 - val_loss: 0.1595 - val_accuracy: 0.9408\n",
      "Epoch 577/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1127 - accuracy: 0.9562 - val_loss: 0.1594 - val_accuracy: 0.9397\n",
      "Epoch 578/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1126 - accuracy: 0.9562 - val_loss: 0.1591 - val_accuracy: 0.9401\n",
      "Epoch 579/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1126 - accuracy: 0.9563 - val_loss: 0.1594 - val_accuracy: 0.9408\n",
      "Epoch 580/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1125 - accuracy: 0.9561 - val_loss: 0.1600 - val_accuracy: 0.9399\n",
      "Epoch 581/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1125 - accuracy: 0.9563 - val_loss: 0.1597 - val_accuracy: 0.9405\n",
      "Epoch 582/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1124 - accuracy: 0.9565 - val_loss: 0.1592 - val_accuracy: 0.9404\n",
      "Epoch 583/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1124 - accuracy: 0.9562 - val_loss: 0.1603 - val_accuracy: 0.9404\n",
      "Epoch 584/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1123 - accuracy: 0.9563 - val_loss: 0.1587 - val_accuracy: 0.9404\n",
      "Epoch 585/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1121 - accuracy: 0.9564 - val_loss: 0.1592 - val_accuracy: 0.9406\n",
      "Epoch 586/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1120 - accuracy: 0.9568 - val_loss: 0.1593 - val_accuracy: 0.9400\n",
      "Epoch 587/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1119 - accuracy: 0.9567 - val_loss: 0.1597 - val_accuracy: 0.9401\n",
      "Epoch 588/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1120 - accuracy: 0.9565 - val_loss: 0.1593 - val_accuracy: 0.9398\n",
      "Epoch 589/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1119 - accuracy: 0.9565 - val_loss: 0.1599 - val_accuracy: 0.9398\n",
      "Epoch 590/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1118 - accuracy: 0.9564 - val_loss: 0.1595 - val_accuracy: 0.9402\n",
      "Epoch 591/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1118 - accuracy: 0.9564 - val_loss: 0.1597 - val_accuracy: 0.9400\n",
      "Epoch 592/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1118 - accuracy: 0.9564 - val_loss: 0.1594 - val_accuracy: 0.9401\n",
      "Epoch 593/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1117 - accuracy: 0.9566 - val_loss: 0.1597 - val_accuracy: 0.9409\n",
      "Epoch 594/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1117 - accuracy: 0.9566 - val_loss: 0.1600 - val_accuracy: 0.9402\n",
      "Epoch 595/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1115 - accuracy: 0.9569 - val_loss: 0.1598 - val_accuracy: 0.9398\n",
      "Epoch 596/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1115 - accuracy: 0.9568 - val_loss: 0.1603 - val_accuracy: 0.9401\n",
      "Epoch 597/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1115 - accuracy: 0.9568 - val_loss: 0.1600 - val_accuracy: 0.9400\n",
      "Epoch 598/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1113 - accuracy: 0.9568 - val_loss: 0.1597 - val_accuracy: 0.9400\n",
      "Epoch 599/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1113 - accuracy: 0.9564 - val_loss: 0.1599 - val_accuracy: 0.9404\n",
      "Epoch 600/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1113 - accuracy: 0.9569 - val_loss: 0.1601 - val_accuracy: 0.9399\n",
      "Epoch 601/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1112 - accuracy: 0.9569 - val_loss: 0.1596 - val_accuracy: 0.9402\n",
      "Epoch 602/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1110 - accuracy: 0.9571 - val_loss: 0.1599 - val_accuracy: 0.9404\n",
      "Epoch 603/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1110 - accuracy: 0.9568 - val_loss: 0.1597 - val_accuracy: 0.9402\n",
      "Epoch 604/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1109 - accuracy: 0.9568 - val_loss: 0.1607 - val_accuracy: 0.9398\n",
      "Epoch 605/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1109 - accuracy: 0.9568 - val_loss: 0.1604 - val_accuracy: 0.9402\n",
      "Epoch 606/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1109 - accuracy: 0.9570 - val_loss: 0.1597 - val_accuracy: 0.9403\n",
      "Epoch 607/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1107 - accuracy: 0.9570 - val_loss: 0.1601 - val_accuracy: 0.9397\n",
      "Epoch 608/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1108 - accuracy: 0.9569 - val_loss: 0.1605 - val_accuracy: 0.9402\n",
      "Epoch 609/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1108 - accuracy: 0.9567 - val_loss: 0.1602 - val_accuracy: 0.9393\n",
      "Epoch 610/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1106 - accuracy: 0.9570 - val_loss: 0.1609 - val_accuracy: 0.9403\n",
      "Epoch 611/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1105 - accuracy: 0.9570 - val_loss: 0.1601 - val_accuracy: 0.9400\n",
      "Epoch 612/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1105 - accuracy: 0.9570 - val_loss: 0.1602 - val_accuracy: 0.9401\n",
      "Epoch 613/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1105 - accuracy: 0.9569 - val_loss: 0.1609 - val_accuracy: 0.9398\n",
      "Epoch 614/800\n",
      "6000/6000 [==============================] - 1s 91us/step - loss: 0.1103 - accuracy: 0.9570 - val_loss: 0.1603 - val_accuracy: 0.9406\n",
      "Epoch 615/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1102 - accuracy: 0.9571 - val_loss: 0.1601 - val_accuracy: 0.9404\n",
      "Epoch 616/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1103 - accuracy: 0.9571 - val_loss: 0.1609 - val_accuracy: 0.9406\n",
      "Epoch 617/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1102 - accuracy: 0.9570 - val_loss: 0.1613 - val_accuracy: 0.9394\n",
      "Epoch 618/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1102 - accuracy: 0.9573 - val_loss: 0.1614 - val_accuracy: 0.9398\n",
      "Epoch 619/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1101 - accuracy: 0.9573 - val_loss: 0.1601 - val_accuracy: 0.9395\n",
      "Epoch 620/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1100 - accuracy: 0.9571 - val_loss: 0.1610 - val_accuracy: 0.9395\n",
      "Epoch 621/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1100 - accuracy: 0.9571 - val_loss: 0.1611 - val_accuracy: 0.9407\n",
      "Epoch 622/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1098 - accuracy: 0.9571 - val_loss: 0.1617 - val_accuracy: 0.9398\n",
      "Epoch 623/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1099 - accuracy: 0.9574 - val_loss: 0.1611 - val_accuracy: 0.9400\n",
      "Epoch 624/800\n",
      "6000/6000 [==============================] - 0s 71us/step - loss: 0.1098 - accuracy: 0.9573 - val_loss: 0.1607 - val_accuracy: 0.9401\n",
      "Epoch 625/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1097 - accuracy: 0.9574 - val_loss: 0.1604 - val_accuracy: 0.9408\n",
      "Epoch 626/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1095 - accuracy: 0.9574 - val_loss: 0.1609 - val_accuracy: 0.9398\n",
      "Epoch 627/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1095 - accuracy: 0.9576 - val_loss: 0.1618 - val_accuracy: 0.9403\n",
      "Epoch 628/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1095 - accuracy: 0.9572 - val_loss: 0.1621 - val_accuracy: 0.9405\n",
      "Epoch 629/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1094 - accuracy: 0.9575 - val_loss: 0.1609 - val_accuracy: 0.9402\n",
      "Epoch 630/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1094 - accuracy: 0.9576 - val_loss: 0.1611 - val_accuracy: 0.9401\n",
      "Epoch 631/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1094 - accuracy: 0.9575 - val_loss: 0.1613 - val_accuracy: 0.9401\n",
      "Epoch 632/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1093 - accuracy: 0.9573 - val_loss: 0.1619 - val_accuracy: 0.9397\n",
      "Epoch 633/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1092 - accuracy: 0.9576 - val_loss: 0.1619 - val_accuracy: 0.9399\n",
      "Epoch 634/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1092 - accuracy: 0.9576 - val_loss: 0.1620 - val_accuracy: 0.9394\n",
      "Epoch 635/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1091 - accuracy: 0.9575 - val_loss: 0.1609 - val_accuracy: 0.9398\n",
      "Epoch 636/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1090 - accuracy: 0.9577 - val_loss: 0.1611 - val_accuracy: 0.9395\n",
      "Epoch 637/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1090 - accuracy: 0.9576 - val_loss: 0.1611 - val_accuracy: 0.9396\n",
      "Epoch 638/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1087 - accuracy: 0.9577 - val_loss: 0.1623 - val_accuracy: 0.9394\n",
      "Epoch 639/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1088 - accuracy: 0.9578 - val_loss: 0.1612 - val_accuracy: 0.9403\n",
      "Epoch 640/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1089 - accuracy: 0.9577 - val_loss: 0.1611 - val_accuracy: 0.9403\n",
      "Epoch 641/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1087 - accuracy: 0.9579 - val_loss: 0.1614 - val_accuracy: 0.9400\n",
      "Epoch 642/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1086 - accuracy: 0.9577 - val_loss: 0.1624 - val_accuracy: 0.9401\n",
      "Epoch 643/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1085 - accuracy: 0.9577 - val_loss: 0.1617 - val_accuracy: 0.9397\n",
      "Epoch 644/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1085 - accuracy: 0.9578 - val_loss: 0.1615 - val_accuracy: 0.9406\n",
      "Epoch 645/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1085 - accuracy: 0.9575 - val_loss: 0.1616 - val_accuracy: 0.9400\n",
      "Epoch 646/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1083 - accuracy: 0.9578 - val_loss: 0.1620 - val_accuracy: 0.9396\n",
      "Epoch 647/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1083 - accuracy: 0.9578 - val_loss: 0.1625 - val_accuracy: 0.9399\n",
      "Epoch 648/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1082 - accuracy: 0.9578 - val_loss: 0.1623 - val_accuracy: 0.9397\n",
      "Epoch 649/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1081 - accuracy: 0.9580 - val_loss: 0.1619 - val_accuracy: 0.9396\n",
      "Epoch 650/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1081 - accuracy: 0.9578 - val_loss: 0.1618 - val_accuracy: 0.9397\n",
      "Epoch 651/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1080 - accuracy: 0.9577 - val_loss: 0.1619 - val_accuracy: 0.9403\n",
      "Epoch 652/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1080 - accuracy: 0.9581 - val_loss: 0.1627 - val_accuracy: 0.9395\n",
      "Epoch 653/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1080 - accuracy: 0.9579 - val_loss: 0.1620 - val_accuracy: 0.9399\n",
      "Epoch 654/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1079 - accuracy: 0.9578 - val_loss: 0.1617 - val_accuracy: 0.9395\n",
      "Epoch 655/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1078 - accuracy: 0.9580 - val_loss: 0.1631 - val_accuracy: 0.9396\n",
      "Epoch 656/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1078 - accuracy: 0.9579 - val_loss: 0.1628 - val_accuracy: 0.9396\n",
      "Epoch 657/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1076 - accuracy: 0.9581 - val_loss: 0.1632 - val_accuracy: 0.9398\n",
      "Epoch 658/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1077 - accuracy: 0.9580 - val_loss: 0.1629 - val_accuracy: 0.9393\n",
      "Epoch 659/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1076 - accuracy: 0.9580 - val_loss: 0.1627 - val_accuracy: 0.9391\n",
      "Epoch 660/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1075 - accuracy: 0.9581 - val_loss: 0.1621 - val_accuracy: 0.9398\n",
      "Epoch 661/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1074 - accuracy: 0.9583 - val_loss: 0.1627 - val_accuracy: 0.9396\n",
      "Epoch 662/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1073 - accuracy: 0.9582 - val_loss: 0.1621 - val_accuracy: 0.9400\n",
      "Epoch 663/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1074 - accuracy: 0.9581 - val_loss: 0.1632 - val_accuracy: 0.9393\n",
      "Epoch 664/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1072 - accuracy: 0.9584 - val_loss: 0.1645 - val_accuracy: 0.9389\n",
      "Epoch 665/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1072 - accuracy: 0.9582 - val_loss: 0.1632 - val_accuracy: 0.9393\n",
      "Epoch 666/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1070 - accuracy: 0.9580 - val_loss: 0.1635 - val_accuracy: 0.9402\n",
      "Epoch 667/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1069 - accuracy: 0.9584 - val_loss: 0.1624 - val_accuracy: 0.9391\n",
      "Epoch 668/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.1630 - val_accuracy: 0.9397\n",
      "Epoch 669/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1068 - accuracy: 0.9584 - val_loss: 0.1649 - val_accuracy: 0.9384\n",
      "Epoch 670/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1069 - accuracy: 0.9585 - val_loss: 0.1628 - val_accuracy: 0.9398\n",
      "Epoch 671/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1068 - accuracy: 0.9585 - val_loss: 0.1637 - val_accuracy: 0.9392\n",
      "Epoch 672/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1067 - accuracy: 0.9582 - val_loss: 0.1636 - val_accuracy: 0.9396\n",
      "Epoch 673/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1067 - accuracy: 0.9584 - val_loss: 0.1637 - val_accuracy: 0.9388\n",
      "Epoch 674/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1066 - accuracy: 0.9582 - val_loss: 0.1628 - val_accuracy: 0.9401\n",
      "Epoch 675/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1065 - accuracy: 0.9583 - val_loss: 0.1629 - val_accuracy: 0.9398\n",
      "Epoch 676/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1064 - accuracy: 0.9586 - val_loss: 0.1655 - val_accuracy: 0.9379\n",
      "Epoch 677/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1064 - accuracy: 0.9586 - val_loss: 0.1644 - val_accuracy: 0.9401\n",
      "Epoch 678/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.1626 - val_accuracy: 0.9396\n",
      "Epoch 679/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1062 - accuracy: 0.9588 - val_loss: 0.1639 - val_accuracy: 0.9401\n",
      "Epoch 680/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1062 - accuracy: 0.9585 - val_loss: 0.1627 - val_accuracy: 0.9396\n",
      "Epoch 681/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1063 - accuracy: 0.9585 - val_loss: 0.1633 - val_accuracy: 0.9395\n",
      "Epoch 682/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1061 - accuracy: 0.9587 - val_loss: 0.1640 - val_accuracy: 0.9401\n",
      "Epoch 683/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1060 - accuracy: 0.9586 - val_loss: 0.1638 - val_accuracy: 0.9390\n",
      "Epoch 684/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1059 - accuracy: 0.9587 - val_loss: 0.1634 - val_accuracy: 0.9396\n",
      "Epoch 685/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1059 - accuracy: 0.9585 - val_loss: 0.1658 - val_accuracy: 0.9398\n",
      "Epoch 686/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1059 - accuracy: 0.9588 - val_loss: 0.1638 - val_accuracy: 0.9392\n",
      "Epoch 687/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1058 - accuracy: 0.9588 - val_loss: 0.1639 - val_accuracy: 0.9395\n",
      "Epoch 688/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1057 - accuracy: 0.9588 - val_loss: 0.1650 - val_accuracy: 0.9398\n",
      "Epoch 689/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1056 - accuracy: 0.9586 - val_loss: 0.1644 - val_accuracy: 0.9398\n",
      "Epoch 690/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1057 - accuracy: 0.9589 - val_loss: 0.1641 - val_accuracy: 0.9390\n",
      "Epoch 691/800\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.1054 - accuracy: 0.9587 - val_loss: 0.1639 - val_accuracy: 0.9394\n",
      "Epoch 692/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1054 - accuracy: 0.9588 - val_loss: 0.1632 - val_accuracy: 0.9391\n",
      "Epoch 693/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1054 - accuracy: 0.9589 - val_loss: 0.1638 - val_accuracy: 0.9388\n",
      "Epoch 694/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.1053 - accuracy: 0.9589 - val_loss: 0.1638 - val_accuracy: 0.9395\n",
      "Epoch 695/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1053 - accuracy: 0.9590 - val_loss: 0.1659 - val_accuracy: 0.9381\n",
      "Epoch 696/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1052 - accuracy: 0.9588 - val_loss: 0.1653 - val_accuracy: 0.9394\n",
      "Epoch 697/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1051 - accuracy: 0.9588 - val_loss: 0.1640 - val_accuracy: 0.9389\n",
      "Epoch 698/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1051 - accuracy: 0.9588 - val_loss: 0.1647 - val_accuracy: 0.9390\n",
      "Epoch 699/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1049 - accuracy: 0.9591 - val_loss: 0.1653 - val_accuracy: 0.9395\n",
      "Epoch 700/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1050 - accuracy: 0.9589 - val_loss: 0.1645 - val_accuracy: 0.9390\n",
      "Epoch 701/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1049 - accuracy: 0.9590 - val_loss: 0.1649 - val_accuracy: 0.9392\n",
      "Epoch 702/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1048 - accuracy: 0.9591 - val_loss: 0.1646 - val_accuracy: 0.9387\n",
      "Epoch 703/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1049 - accuracy: 0.9591 - val_loss: 0.1648 - val_accuracy: 0.9392\n",
      "Epoch 704/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1047 - accuracy: 0.9592 - val_loss: 0.1644 - val_accuracy: 0.9399\n",
      "Epoch 705/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1046 - accuracy: 0.9589 - val_loss: 0.1642 - val_accuracy: 0.9386\n",
      "Epoch 706/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1045 - accuracy: 0.9590 - val_loss: 0.1648 - val_accuracy: 0.9399\n",
      "Epoch 707/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1044 - accuracy: 0.9593 - val_loss: 0.1663 - val_accuracy: 0.9393\n",
      "Epoch 708/800\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.1043 - accuracy: 0.9594 - val_loss: 0.1655 - val_accuracy: 0.9388\n",
      "Epoch 709/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1044 - accuracy: 0.9591 - val_loss: 0.1653 - val_accuracy: 0.9393\n",
      "Epoch 710/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1043 - accuracy: 0.9593 - val_loss: 0.1649 - val_accuracy: 0.9393\n",
      "Epoch 711/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1042 - accuracy: 0.9592 - val_loss: 0.1650 - val_accuracy: 0.9395\n",
      "Epoch 712/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1043 - accuracy: 0.9593 - val_loss: 0.1656 - val_accuracy: 0.9393\n",
      "Epoch 713/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1040 - accuracy: 0.9595 - val_loss: 0.1651 - val_accuracy: 0.9390\n",
      "Epoch 714/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1041 - accuracy: 0.9593 - val_loss: 0.1658 - val_accuracy: 0.9395\n",
      "Epoch 715/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1040 - accuracy: 0.9592 - val_loss: 0.1658 - val_accuracy: 0.9394\n",
      "Epoch 716/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1038 - accuracy: 0.9594 - val_loss: 0.1650 - val_accuracy: 0.9394\n",
      "Epoch 717/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1038 - accuracy: 0.9593 - val_loss: 0.1658 - val_accuracy: 0.9393\n",
      "Epoch 718/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1037 - accuracy: 0.9592 - val_loss: 0.1649 - val_accuracy: 0.9388\n",
      "Epoch 719/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1036 - accuracy: 0.9595 - val_loss: 0.1662 - val_accuracy: 0.9387\n",
      "Epoch 720/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1036 - accuracy: 0.9597 - val_loss: 0.1661 - val_accuracy: 0.9384\n",
      "Epoch 721/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1037 - accuracy: 0.9598 - val_loss: 0.1648 - val_accuracy: 0.9392\n",
      "Epoch 722/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1035 - accuracy: 0.9597 - val_loss: 0.1652 - val_accuracy: 0.9391\n",
      "Epoch 723/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1034 - accuracy: 0.9596 - val_loss: 0.1661 - val_accuracy: 0.9387\n",
      "Epoch 724/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1033 - accuracy: 0.9597 - val_loss: 0.1670 - val_accuracy: 0.9392\n",
      "Epoch 725/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1033 - accuracy: 0.9598 - val_loss: 0.1666 - val_accuracy: 0.9391\n",
      "Epoch 726/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1032 - accuracy: 0.9596 - val_loss: 0.1667 - val_accuracy: 0.9390\n",
      "Epoch 727/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1031 - accuracy: 0.9599 - val_loss: 0.1670 - val_accuracy: 0.9390\n",
      "Epoch 728/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1031 - accuracy: 0.9595 - val_loss: 0.1663 - val_accuracy: 0.9386\n",
      "Epoch 729/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1029 - accuracy: 0.9600 - val_loss: 0.1663 - val_accuracy: 0.9386\n",
      "Epoch 730/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1029 - accuracy: 0.9598 - val_loss: 0.1666 - val_accuracy: 0.9381\n",
      "Epoch 731/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1028 - accuracy: 0.9599 - val_loss: 0.1674 - val_accuracy: 0.9395\n",
      "Epoch 732/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1028 - accuracy: 0.9598 - val_loss: 0.1667 - val_accuracy: 0.9388\n",
      "Epoch 733/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1027 - accuracy: 0.9598 - val_loss: 0.1659 - val_accuracy: 0.9398\n",
      "Epoch 734/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1026 - accuracy: 0.9599 - val_loss: 0.1675 - val_accuracy: 0.9390\n",
      "Epoch 735/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1026 - accuracy: 0.9598 - val_loss: 0.1666 - val_accuracy: 0.9382\n",
      "Epoch 736/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1025 - accuracy: 0.9599 - val_loss: 0.1658 - val_accuracy: 0.9384\n",
      "Epoch 737/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1024 - accuracy: 0.9602 - val_loss: 0.1672 - val_accuracy: 0.9389\n",
      "Epoch 738/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1025 - accuracy: 0.9599 - val_loss: 0.1671 - val_accuracy: 0.9390\n",
      "Epoch 739/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1023 - accuracy: 0.9599 - val_loss: 0.1674 - val_accuracy: 0.9380\n",
      "Epoch 740/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1024 - accuracy: 0.9600 - val_loss: 0.1681 - val_accuracy: 0.9385\n",
      "Epoch 741/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1022 - accuracy: 0.9601 - val_loss: 0.1671 - val_accuracy: 0.9393\n",
      "Epoch 742/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1022 - accuracy: 0.9599 - val_loss: 0.1668 - val_accuracy: 0.9386\n",
      "Epoch 743/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1020 - accuracy: 0.9599 - val_loss: 0.1679 - val_accuracy: 0.9387\n",
      "Epoch 744/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1019 - accuracy: 0.9599 - val_loss: 0.1674 - val_accuracy: 0.9383\n",
      "Epoch 745/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1019 - accuracy: 0.9603 - val_loss: 0.1676 - val_accuracy: 0.9384\n",
      "Epoch 746/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1018 - accuracy: 0.9601 - val_loss: 0.1678 - val_accuracy: 0.9375\n",
      "Epoch 747/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1018 - accuracy: 0.9600 - val_loss: 0.1679 - val_accuracy: 0.9386\n",
      "Epoch 748/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1018 - accuracy: 0.9602 - val_loss: 0.1667 - val_accuracy: 0.9391\n",
      "Epoch 749/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1018 - accuracy: 0.9602 - val_loss: 0.1674 - val_accuracy: 0.9384\n",
      "Epoch 750/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1015 - accuracy: 0.9605 - val_loss: 0.1690 - val_accuracy: 0.9385\n",
      "Epoch 751/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1016 - accuracy: 0.9603 - val_loss: 0.1675 - val_accuracy: 0.9390\n",
      "Epoch 752/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1015 - accuracy: 0.9601 - val_loss: 0.1675 - val_accuracy: 0.9380\n",
      "Epoch 753/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.1013 - accuracy: 0.9603 - val_loss: 0.1683 - val_accuracy: 0.9383\n",
      "Epoch 754/800\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.1013 - accuracy: 0.9604 - val_loss: 0.1682 - val_accuracy: 0.9389\n",
      "Epoch 755/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1013 - accuracy: 0.9604 - val_loss: 0.1687 - val_accuracy: 0.9388\n",
      "Epoch 756/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1012 - accuracy: 0.9605 - val_loss: 0.1684 - val_accuracy: 0.9394\n",
      "Epoch 757/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1012 - accuracy: 0.9604 - val_loss: 0.1669 - val_accuracy: 0.9385\n",
      "Epoch 758/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1011 - accuracy: 0.9604 - val_loss: 0.1691 - val_accuracy: 0.9379\n",
      "Epoch 759/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.1010 - accuracy: 0.9603 - val_loss: 0.1692 - val_accuracy: 0.9384\n",
      "Epoch 760/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.1010 - accuracy: 0.9604 - val_loss: 0.1678 - val_accuracy: 0.9384\n",
      "Epoch 761/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.1009 - accuracy: 0.9604 - val_loss: 0.1697 - val_accuracy: 0.9385\n",
      "Epoch 762/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1009 - accuracy: 0.9607 - val_loss: 0.1693 - val_accuracy: 0.9387\n",
      "Epoch 763/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1007 - accuracy: 0.9607 - val_loss: 0.1685 - val_accuracy: 0.9383\n",
      "Epoch 764/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1007 - accuracy: 0.9607 - val_loss: 0.1681 - val_accuracy: 0.9386\n",
      "Epoch 765/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.1007 - accuracy: 0.9607 - val_loss: 0.1687 - val_accuracy: 0.9387\n",
      "Epoch 766/800\n",
      "6000/6000 [==============================] - 1s 84us/step - loss: 0.1007 - accuracy: 0.9604 - val_loss: 0.1682 - val_accuracy: 0.9383\n",
      "Epoch 767/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1006 - accuracy: 0.9606 - val_loss: 0.1700 - val_accuracy: 0.9392\n",
      "Epoch 768/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1003 - accuracy: 0.9608 - val_loss: 0.1697 - val_accuracy: 0.9386\n",
      "Epoch 769/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1004 - accuracy: 0.9609 - val_loss: 0.1702 - val_accuracy: 0.9385\n",
      "Epoch 770/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1002 - accuracy: 0.9607 - val_loss: 0.1693 - val_accuracy: 0.9386\n",
      "Epoch 771/800\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.1001 - accuracy: 0.9610 - val_loss: 0.1704 - val_accuracy: 0.9383\n",
      "Epoch 772/800\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.1001 - accuracy: 0.9605 - val_loss: 0.1699 - val_accuracy: 0.9388\n",
      "Epoch 773/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1701 - val_accuracy: 0.9370\n",
      "Epoch 774/800\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.1000 - accuracy: 0.9608 - val_loss: 0.1705 - val_accuracy: 0.9394\n",
      "Epoch 775/800\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.0999 - accuracy: 0.9608 - val_loss: 0.1713 - val_accuracy: 0.9391\n",
      "Epoch 776/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0999 - accuracy: 0.9609 - val_loss: 0.1690 - val_accuracy: 0.9383\n",
      "Epoch 777/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0998 - accuracy: 0.9608 - val_loss: 0.1698 - val_accuracy: 0.9382\n",
      "Epoch 778/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0997 - accuracy: 0.9610 - val_loss: 0.1694 - val_accuracy: 0.9379\n",
      "Epoch 779/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0998 - accuracy: 0.9611 - val_loss: 0.1692 - val_accuracy: 0.9383\n",
      "Epoch 780/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0996 - accuracy: 0.9609 - val_loss: 0.1702 - val_accuracy: 0.9384\n",
      "Epoch 781/800\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0995 - accuracy: 0.9611 - val_loss: 0.1698 - val_accuracy: 0.9380\n",
      "Epoch 782/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0995 - accuracy: 0.9609 - val_loss: 0.1703 - val_accuracy: 0.9383\n",
      "Epoch 783/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0994 - accuracy: 0.9611 - val_loss: 0.1706 - val_accuracy: 0.9382\n",
      "Epoch 784/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0993 - accuracy: 0.9609 - val_loss: 0.1693 - val_accuracy: 0.9383\n",
      "Epoch 785/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0991 - accuracy: 0.9611 - val_loss: 0.1703 - val_accuracy: 0.9378\n",
      "Epoch 786/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0993 - accuracy: 0.9613 - val_loss: 0.1706 - val_accuracy: 0.9384\n",
      "Epoch 787/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0992 - accuracy: 0.9613 - val_loss: 0.1716 - val_accuracy: 0.9377\n",
      "Epoch 788/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0989 - accuracy: 0.9612 - val_loss: 0.1708 - val_accuracy: 0.9388\n",
      "Epoch 789/800\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0989 - accuracy: 0.9614 - val_loss: 0.1711 - val_accuracy: 0.9378\n",
      "Epoch 790/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0989 - accuracy: 0.9612 - val_loss: 0.1719 - val_accuracy: 0.9379\n",
      "Epoch 791/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0987 - accuracy: 0.9613 - val_loss: 0.1713 - val_accuracy: 0.9381\n",
      "Epoch 792/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0986 - accuracy: 0.9614 - val_loss: 0.1700 - val_accuracy: 0.9383\n",
      "Epoch 793/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0987 - accuracy: 0.9613 - val_loss: 0.1712 - val_accuracy: 0.9382\n",
      "Epoch 794/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0986 - accuracy: 0.9614 - val_loss: 0.1726 - val_accuracy: 0.9381\n",
      "Epoch 795/800\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0987 - accuracy: 0.9610 - val_loss: 0.1721 - val_accuracy: 0.9378\n",
      "Epoch 796/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0986 - accuracy: 0.9613 - val_loss: 0.1709 - val_accuracy: 0.9376\n",
      "Epoch 797/800\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0985 - accuracy: 0.9613 - val_loss: 0.1721 - val_accuracy: 0.9369\n",
      "Epoch 798/800\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1718 - val_accuracy: 0.9380\n",
      "Epoch 799/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0983 - accuracy: 0.9616 - val_loss: 0.1716 - val_accuracy: 0.9371\n",
      "Epoch 800/800\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0984 - accuracy: 0.9612 - val_loss: 0.1724 - val_accuracy: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:122: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[1999, 1]\n",
      "[0, 0]\n",
      "[1776, 45]\n",
      "[132, 47]\n",
      "[1759, 62]\n",
      "[20, 159]\n",
      "[1810, 56]\n",
      "[96, 38]\n",
      "[1793, 73]\n",
      "[49, 85]\n",
      "[1901, 35]\n",
      "[36, 28]\n",
      "[1837, 99]\n",
      "[24, 40]\n",
      "[1892, 30]\n",
      "[50, 28]\n",
      "[1800, 122]\n",
      "[49, 29]\n",
      "[1700, 131]\n",
      "[96, 73]\n",
      "[1790, 41]\n",
      "[36, 133]\n",
      "[1702, 109]\n",
      "[101, 88]\n",
      "[1773, 38]\n",
      "[55, 134]\n",
      "[1840, 67]\n",
      "[52, 41]\n",
      "[1829, 78]\n",
      "[30, 63]\n",
      "[1856, 50]\n",
      "[42, 52]\n",
      "[1829, 77]\n",
      "[34, 60]\n",
      "[1806, 67]\n",
      "[68, 59]\n",
      "[1790, 83]\n",
      "[59, 68]\n",
      "[1775, 51]\n",
      "[84, 90]\n",
      "[1761, 65]\n",
      "[67, 107]\n",
      "[1763, 45]\n",
      "[77, 115]\n",
      "[1755, 53]\n",
      "[66, 126]\n",
      "[1792, 72]\n",
      "[72, 64]\n",
      "[1767, 97]\n",
      "[70, 66]\n",
      "[1844, 41]\n",
      "[59, 56]\n",
      "[1806, 79]\n",
      "[45, 70]\n",
      "[1837, 48]\n",
      "[55, 60]\n",
      "[1802, 83]\n",
      "[31, 84]\n",
      "[1744, 121]\n",
      "[65, 70]\n",
      "[1828, 37]\n",
      "[22, 113]\n",
      "[1753, 77]\n",
      "[57, 113]\n",
      "[1769, 61]\n",
      "[77, 93]\n",
      "[1815, 66]\n",
      "[57, 62]\n",
      "[1810, 71]\n",
      "[42, 77]\n",
      "[1865, 39]\n",
      "[42, 54]\n",
      "[1826, 78]\n",
      "[35, 61]\n",
      "[1866, 35]\n",
      "[52, 47]\n",
      "[1818, 83]\n",
      "[46, 53]\n",
      "[1793, 30]\n",
      "[59, 118]\n",
      "[1776, 47]\n",
      "[53, 124]\n",
      "[1726, 79]\n",
      "[118, 77]\n",
      "[1721, 84]\n",
      "[102, 93]\n",
      "[1921, 20]\n",
      "[29, 30]\n",
      "[1832, 109]\n",
      "[19, 40]\n",
      "[1932, 10]\n",
      "[18, 40]\n",
      "[1849, 93]\n",
      "[19, 39]\n",
      "[1912, 22]\n",
      "[32, 34]\n",
      "[1840, 94]\n",
      "[29, 37]\n",
      "[1891, 29]\n",
      "[44, 36]\n",
      "[1849, 71]\n",
      "[24, 56]\n",
      "[1714, 115]\n",
      "[78, 93]\n",
      "[1776, 53]\n",
      "[51, 120]\n",
      "[1807, 33]\n",
      "[47, 113]\n",
      "[1788, 52]\n",
      "[41, 119]\n",
      "[1734, 53]\n",
      "[107, 106]\n",
      "[1730, 57]\n",
      "[145, 68]\n",
      "[1773, 54]\n",
      "[80, 93]\n",
      "[1762, 65]\n",
      "[69, 104]\n",
      "[1896, 22]\n",
      "[52, 30]\n",
      "[1842, 76]\n",
      "[34, 48]\n",
      "[1899, 43]\n",
      "[39, 19]\n",
      "[1842, 100]\n",
      "[19, 39]\n",
      "[1828, 74]\n",
      "[54, 44]\n",
      "[1846, 56]\n",
      "[39, 59]\n",
      "[1844, 51]\n",
      "[55, 50]\n",
      "[1828, 67]\n",
      "[18, 87]\n",
      "[1853, 31]\n",
      "[63, 53]\n",
      "[1793, 91]\n",
      "[46, 70]\n",
      "[1915, 27]\n",
      "[34, 24]\n",
      "[1843, 99]\n",
      "[17, 41]\n",
      "[1894, 26]\n",
      "[51, 29]\n",
      "[1820, 100]\n",
      "[41, 39]\n",
      "[1886, 37]\n",
      "[36, 41]\n",
      "[1845, 78]\n",
      "[21, 56]\n",
      "[1843, 48]\n",
      "[67, 42]\n",
      "[1800, 91]\n",
      "[56, 53]\n",
      "[1853, 49]\n",
      "[49, 49]\n",
      "[1833, 69]\n",
      "[43, 55]\n",
      "[1917, 14]\n",
      "[40, 29]\n",
      "[1828, 103]\n",
      "[30, 39]\n",
      "[1889, 47]\n",
      "[44, 20]\n",
      "[1854, 82]\n",
      "[24, 40]\n",
      "[1862, 39]\n",
      "[65, 34]\n",
      "[1813, 88]\n",
      "[54, 45]\n",
      "[1832, 50]\n",
      "[64, 54]\n",
      "[1810, 72]\n",
      "[37, 81]\n",
      "[1837, 48]\n",
      "[57, 58]\n",
      "[1801, 84]\n",
      "[45, 70]\n",
      "[1927, 12]\n",
      "[33, 28]\n",
      "[1835, 104]\n",
      "[27, 34]\n",
      "[1868, 48]\n",
      "[55, 29]\n",
      "[1817, 99]\n",
      "[44, 40]\n",
      "[1734, 96]\n",
      "[95, 75]\n",
      "[1791, 39]\n",
      "[40, 130]\n",
      "[1799, 29]\n",
      "[62, 110]\n",
      "[1785, 43]\n",
      "[61, 111]\n",
      "[1713, 123]\n",
      "[90, 74]\n",
      "[1782, 54]\n",
      "[43, 121]\n",
      "[1738, 76]\n",
      "[120, 66]\n",
      "[1735, 79]\n",
      "[102, 84]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9449721903754156 (+- 0.0012008469880829212)\n",
      "> F1: 0.6095281306100763(+- 0.007920987632584923)\n",
      "> Time: 104.8546061599995 (+- 0.812170981605861)\n",
      "##############################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9261858041567732 (+- 0.0003996761518919594)\n",
      "> F1: 0.143544227540668(+- 0.003692416495514664)\n",
      "> Time: 0.035407919999999996 (+- 0.001497005067994095)\n",
      "##############################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9425959428375889 (+- 0.0009438247501950641)\n",
      "> F1: 0.556508364632404(+- 0.0022116015274208484)\n",
      "> Time: 0.07701702000000002 (+- 0.0020009400774635916)\n",
      "##############################################################################\n",
      "> AUC for class : 0.3624312156078039 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X00: 0.5547543009867761 (+- 0.015760521141615694)\n",
      "X^2 for MWPM and NN: 43.75141242937853\n",
      "X^2 for PLUT and NN: 20.5\n",
      "> AUC for class X01: 0.8729854522846064 (+- 0.01324323566297572)\n",
      "X^2 for MWPM and NN: 11.05921052631579\n",
      "X^2 for PLUT and NN: 4.336065573770492\n",
      "> AUC for class X02: 0.9745246305670594 (+- 0.0032229882350464562)\n",
      "X^2 for MWPM and NN: 0.056338028169014086\n",
      "X^2 for PLUT and NN: 44.520325203252035\n",
      "> AUC for class X03: 0.9732156219746884 (+- 0.007501264687307033)\n",
      "X^2 for MWPM and NN: 5.5125\n",
      "X^2 for PLUT and NN: 30.31578947368421\n",
      "> AUC for class X04: 0.8746464331675181 (+- 0.012950141553579654)\n",
      "X^2 for MWPM and NN: 5.092511013215859\n",
      "X^2 for PLUT and NN: 0.2077922077922078\n",
      "> AUC for class X10: 0.8344062743623224 (+- 0.01700641327547282)\n",
      "X^2 for MWPM and NN: 0.23333333333333334\n",
      "X^2 for PLUT and NN: 3.4838709677419355\n",
      "> AUC for class X11: 0.9367955710568967 (+- 0.010923888941294077)\n",
      "X^2 for MWPM and NN: 1.6470588235294117\n",
      "X^2 for PLUT and NN: 20.453703703703702\n",
      "> AUC for class X12: 0.9370293633342298 (+- 0.008773877598918026)\n",
      "X^2 for MWPM and NN: 0.532608695652174\n",
      "X^2 for PLUT and NN: 15.891891891891891\n",
      "> AUC for class X13: 0.9326360259538401 (+- 0.009744629725662503)\n",
      "X^2 for MWPM and NN: 0.02962962962962963\n",
      "X^2 for PLUT and NN: 3.7253521126760565\n",
      "> AUC for class X14: 0.8620302949832525 (+- 0.01136349392522856)\n",
      "X^2 for MWPM and NN: 8.562962962962963\n",
      "X^2 for PLUT and NN: 0.06818181818181818\n",
      "> AUC for class X20: 0.843195089862849 (+- 0.021911398613496454)\n",
      "X^2 for MWPM and NN: 8.926229508196721\n",
      "X^2 for PLUT and NN: 1.6470588235294117\n",
      "> AUC for class X21: 0.9429428120039554 (+- 0.005104016846459212)\n",
      "X^2 for MWPM and NN: 0.006944444444444444\n",
      "X^2 for PLUT and NN: 4.047904191616767\n",
      "> AUC for class X22: 0.9492698450409577 (+- 0.008018683979318568)\n",
      "X^2 for MWPM and NN: 3.61\n",
      "X^2 for PLUT and NN: 8.78225806451613\n",
      "> AUC for class X23: 0.9296956540869854 (+- 0.008145083384804764)\n",
      "X^2 for MWPM and NN: 0.6213592233009708\n",
      "X^2 for PLUT and NN: 22.81578947368421\n",
      "> AUC for class X24: 0.8284729983702459 (+- 0.009390227335150917)\n",
      "X^2 for MWPM and NN: 16.263440860215052\n",
      "X^2 for PLUT and NN: 3.3220338983050848\n",
      "> AUC for class X30: 0.8815475563807775 (+- 0.01584652880635687)\n",
      "X^2 for MWPM and NN: 2.6940298507462686\n",
      "X^2 for PLUT and NN: 2.0942028985507246\n",
      "> AUC for class X31: 0.9465375570470327 (+- 0.007292224714701271)\n",
      "X^2 for MWPM and NN: 0.5203252032520326\n",
      "X^2 for PLUT and NN: 6.938053097345133\n",
      "> AUC for class X32: 0.9591973782896839 (+- 0.01004995583724082)\n",
      "X^2 for MWPM and NN: 0.19753086419753085\n",
      "X^2 for PLUT and NN: 15.610619469026549\n",
      "> AUC for class X33: 0.9536827983968641 (+- 0.004369753792298182)\n",
      "X^2 for MWPM and NN: 3.7241379310344827\n",
      "X^2 for PLUT and NN: 10.046511627906977\n",
      "> AUC for class X34: 0.835287146207772 (+- 0.0089327198970881)\n",
      "X^2 for MWPM and NN: 10.112359550561798\n",
      "X^2 for PLUT and NN: 0.49\n",
      "> AUC for class X40: 0.8870060444980489 (+- 0.011999420709042533)\n",
      "X^2 for MWPM and NN: 8.121827411167512\n",
      "X^2 for PLUT and NN: 1.9408602150537635\n",
      "> AUC for class X41: 0.9799142253144462 (+- 0.0020666781766609)\n",
      "X^2 for MWPM and NN: 2.0408163265306123\n",
      "X^2 for PLUT and NN: 61.8828125\n",
      "> AUC for class X42: 0.9821517895815205 (+- 0.0025664911532096873)\n",
      "X^2 for MWPM and NN: 2.892857142857143\n",
      "X^2 for PLUT and NN: 47.580357142857146\n",
      "> AUC for class X43: 0.9742522276042198 (+- 0.0037665223592774277)\n",
      "X^2 for MWPM and NN: 2.240740740740741\n",
      "X^2 for PLUT and NN: 33.300813008130085\n",
      "> AUC for class X44: 0.9534922859448084 (+- 0.008550495476167036)\n",
      "X^2 for MWPM and NN: 3.506849315068493\n",
      "X^2 for PLUT and NN: 22.273684210526316\n",
      "> AUC for class Z00: 0.8765330968879363 (+- 0.02357343229148252)\n",
      "X^2 for MWPM and NN: 6.715025906735751\n",
      "X^2 for PLUT and NN: 0.009615384615384616\n",
      "> AUC for class Z01: 0.8583054579929458 (+- 0.011123690280563238)\n",
      "X^2 for MWPM and NN: 2.8125\n",
      "X^2 for PLUT and NN: 1.075268817204301\n",
      "> AUC for class Z02: 0.841904743928168 (+- 0.02077459868119449)\n",
      "X^2 for MWPM and NN: 18.90625\n",
      "X^2 for PLUT and NN: 39.21287128712871\n",
      "> AUC for class Z03: 0.8501297508657167 (+- 0.006626586028794121)\n",
      "X^2 for MWPM and NN: 5.440298507462686\n",
      "X^2 for PLUT and NN: 0.1865671641791045\n",
      "> AUC for class Z04: 0.9590514671528483 (+- 0.009571314494155221)\n",
      "X^2 for MWPM and NN: 12.986486486486486\n",
      "X^2 for PLUT and NN: 15.281818181818181\n",
      "> AUC for class Z10: 0.9818487679943981 (+- 0.00277131842481071)\n",
      "X^2 for MWPM and NN: 0.10975609756097561\n",
      "X^2 for PLUT and NN: 53.78151260504202\n",
      "> AUC for class Z11: 0.9372461057920303 (+- 0.009607968469876656)\n",
      "X^2 for MWPM and NN: 2.8203125\n",
      "X^2 for PLUT and NN: 2.694736842105263\n",
      "> AUC for class Z12: 0.9382842266000523 (+- 0.008771600441397269)\n",
      "X^2 for MWPM and NN: 0.2358490566037736\n",
      "X^2 for PLUT and NN: 27.105882352941176\n",
      "> AUC for class Z13: 0.9495461548968868 (+- 0.006180687316553269)\n",
      "X^2 for MWPM and NN: 11.585106382978724\n",
      "X^2 for PLUT and NN: 14.13138686131387\n",
      "> AUC for class Z14: 0.971176115854069 (+- 0.003701061759929132)\n",
      "X^2 for MWPM and NN: 1.0491803278688525\n",
      "X^2 for PLUT and NN: 56.560344827586206\n",
      "> AUC for class Z20: 0.9817473890938103 (+- 0.0029897920972031125)\n",
      "X^2 for MWPM and NN: 8.779220779220779\n",
      "X^2 for PLUT and NN: 23.858156028368793\n",
      "> AUC for class Z21: 0.9525964127546803 (+- 0.007714544053340851)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 31.67676767676768\n",
      "> AUC for class Z22: 0.9336309718066976 (+- 0.005253567521954813)\n",
      "X^2 for MWPM and NN: 3.4782608695652173\n",
      "X^2 for PLUT and NN: 7.863945578231292\n",
      "> AUC for class Z23: 0.9488293624318386 (+- 0.0031585090204163655)\n",
      "X^2 for MWPM and NN: 0.01020408163265306\n",
      "X^2 for PLUT and NN: 5.580357142857143\n",
      "> AUC for class Z24: 0.9784011874723031 (+- 0.005048348770367595)\n",
      "X^2 for MWPM and NN: 13.5\n",
      "X^2 for PLUT and NN: 38.97744360902256\n",
      "> AUC for class Z30: 0.9755771394240632 (+- 0.00371877789950292)\n",
      "X^2 for MWPM and NN: 0.04395604395604396\n",
      "X^2 for PLUT and NN: 30.650943396226417\n",
      "> AUC for class Z31: 0.9500120929003673 (+- 0.010645618149952858)\n",
      "X^2 for MWPM and NN: 7.009615384615385\n",
      "X^2 for PLUT and NN: 7.669014084507042\n",
      "> AUC for class Z32: 0.929541747705041 (+- 0.0069754200941563515)\n",
      "X^2 for MWPM and NN: 1.9736842105263157\n",
      "X^2 for PLUT and NN: 10.605504587155963\n",
      "> AUC for class Z33: 0.9376508816354885 (+- 0.00854380775959684)\n",
      "X^2 for MWPM and NN: 0.9523809523809523\n",
      "X^2 for PLUT and NN: 11.193798449612403\n",
      "> AUC for class Z34: 0.9796554761808867 (+- 0.002905575347540848)\n",
      "X^2 for MWPM and NN: 10.755555555555556\n",
      "X^2 for PLUT and NN: 44.091603053435115\n",
      "> AUC for class Z40: 0.9621649223942657 (+- 0.007417845567120806)\n",
      "X^2 for MWPM and NN: 0.6213592233009708\n",
      "X^2 for PLUT and NN: 20.39160839160839\n",
      "> AUC for class Z41: 0.845287591911695 (+- 0.008234297104035657)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.05063291139240506\n",
      "> AUC for class Z42: 0.8371041911808959 (+- 0.01497993134283774)\n",
      "X^2 for MWPM and NN: 12.703296703296703\n",
      "X^2 for PLUT and NN: 3.4711538461538463\n",
      "> AUC for class Z43: 0.859203200844874 (+- 0.014210553901748933)\n",
      "X^2 for MWPM and NN: 4.807511737089202\n",
      "X^2 for PLUT and NN: 1.0309278350515463\n",
      "> AUC for class Z44: 0.8740791735989912 (+- 0.010606690233077615)\n",
      "X^2 for MWPM and NN: 10.331632653061224\n",
      "X^2 for PLUT and NN: 3.18232044198895\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.5556516724336793, 0.5590668080593849, 0.5589112873881086, 0.5556763022531334, 0.5532357530277138]\n",
      "TOTAL F1 PLUT: [0.14168377823408626, 0.14470108695652176, 0.14984604858022577, 0.13874673035369045, 0.14274349357881577]\n",
      "TOTAL F1 MWPM: [0.6100850798920937, 0.5945308162132148, 0.6109080685971324, 0.6168611475523027, 0.6152555407956373]\n",
      "TOTAL ACC NN: [0.9424797892570496, 0.942480206489563, 0.9439607858657837, 0.9430099129676819, 0.941049019607866]\n",
      "TOTAL ACC PLUT: [0.9262721580386557, 0.925941176470615, 0.9269117647059101, 0.925754901960813, 0.926049019607872]\n",
      "TOTAL ACC MWPM: [0.9447629126613318, 0.9427254901960936, 0.9457254901960929, 0.9455980392157013, 0.9460490196078579]\n",
      "TOTAL TIME NN: [0.0760161, 0.0810189, 0.0760167, 0.0760175, 0.0760159]\n",
      "TOTAL TIME PLUT: [0.0340073, 0.0360084, 0.0340076, 0.035008, 0.0380083]\n",
      "TOTAL TIME MWPM: [103.48378619999957, 104.48240849999944, 105.08248069999924, 105.81301839999968, 105.41133699999955]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAEiCAYAAAAyFTwvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACUQElEQVR4nOzdeVyU1f7A8c+ZGXZHdlcQVMBhUVJxy9yrqy3mcu2XdvPaYotZmWZm11v32q20vN2rpZXa4lK5lKlle9lVs0zNVEDADRcQXBBkGWCW8/vjmaEREUFBEM779RphnufM83xnQOY755zne4SUEkVRFEVRFKX6dHUdgKIoiqIoyrVKJVKKoiiKoiiXSSVSiqIoiqIol0klUoqiKIqiKJdJJVKKoiiKoiiXSSVSiqIoiqIol0klUkqlhBD9hRBSCDHOZVu4Y9s/qniM94UQtVJnQwjxD0cs4bVxfEUjhLhOCPG9EOJsdX721wLH83m/ruNQFOXa1CgTKSGEtxBikhBisxAiRwhhEUJkCyG+EEKME0IY6jrG6hBCbBdClAohgitp00QIUSCESL2asdUEIcSw+vzG7ZJsut4KhBC/CSGerOz3SQjRVwixWgiR6fgZnnT8Hg67xDmjhBALhBApQohCIYRZCJEmhFgohOhWw8/PAHwCRAJ/B+4B1lTSfly518IihDjjeD3eEkL0rsn4qsKRcA+r5eOX/x1w3p6q5jGsQghTBfv7V3Q8l/N8cJHj/iiEKLi8Z6YoyqVcUwlDTRBCRAAbgCjgO+Bl4DTQDLgReA+IAZ6uqxgvwzvAm8BfgP9cpM2dgA/a87tSRwAvwFoDx6qKYcBfgX9UsO9fwCyg5CrFUpmPgC8AAbQAxgKvAdHAg+UbCyFeAqajvZ7vAIcdjxsDfCqEWAbcK6W0lXvc/Wg/72LHOX9H+1lEASOB8UKIWCllcg09r3aO2xQp5RvVeNw8YDvaBzZfIA4YATwkhPgQ7bmV1lCMl/I8sARYW8vneRLt74mrndU8hh7t79Lwaj5utBDiVSnl79V8nKIoV6BRJVJCCC/gc7Q3hZFSyvKfqmc7Ps1X+oleCGGUUubXUpiX4yO0N+x7uXgidS9gQ3szuSJSK4dffKXHqQlSSitXL6G7lN+klMudd4QQC4AU4AEhxN+klKdc9t2PlkR9B9whpSxy2fcKWmI1FkgHnnPZdyOwEEgG/iSlzHQNQAgxHXishp9XC8fXnGo+brOU8mPXDUKISWjPbQxwDnjkiqOrX9ZKKdOv8Bg7gGFCiF5Syp+r+Ji9aIn0bOBPV3h+RVGqobEN7T0AdAD+XUESBYCUcruUcoHzvhAi3dE13lkI8bUQIg/Y47K/rxDiWyFEnmN45TfHm+R5hBCxjiGcDCFEiRAiSwixUQhxq0sbT0f3fqoQokgIkSuE2CuEeLWyJyWlzAM+BjoKIRIqOHckcAPwpZTyhBCilRDi30KI34U256VYCJEshJgmhNBf6kUUF5kj5Yj/VccwlVkI8asQ4uaLHKO70OZOpTmea74Q4ichxPBy7X5E641yHcIom7MlLjJHyhHjMqEN2ZYIIQ4KIV4SQniXa+d8fAfH/uOO9ruFELdc6rWojJSyEPgFrYeqvcs53dF60gqAu12TKMfjrMBDwFHgKXH+kO1sx/H+r3wS5XyslPI/VemNqspr5Hj9/+e4+57L6x9eldeggvjMwDjgEFrP2XnHEUK0FEK8KYQ4KrShzkyhDVc2K9fO+XOLFULMc/x/MgshtgkhBpV7js75eX91/R2q4PXoJYT4n9CGSs8IIRYLIZpU9zkKIZqKK5se8E+gCHilGo85CiwAbnZ9/oqi1L5G1SMF/NnxdWE1H9cG+AFYjTZXpAmAEOJ24FMgC/g3kA/cBSwWQrSTUv7N0S7Q8XiAt9CGcoKABKAH2lAjwHzgPmApWg+TAW1eysAqxPgu2tyVe9E+0bq61/H1HcfXTmhDLJ8CBwE3YDDaEFk7tDfxy/ER2jDcZ8DXaMnDGrQhq/KGAyZgFdrrEYiWMK0RQtwtpfzQ0e5FtIS/j+P5OW29WBBCiDDgV7ThpAXAfqA/Wg9QbyHEIEey4moJYAHmAO7AJGCtECLqCnsYnAmUa29Ob7Reng+klCcrepCUslgIsRx4FrgFWCKEaAt0QevpuaJhu2q8Ri8CPzniWAhsdhziVPljVpWUslRow5bPo/WevO2IqQ3wM9rr/w7a72YEWq/VACFEguNDg6ulaD2tswEj2u/uV0KIIVLK7xxx3gMsc8R+sf/716H1Vr8HfOh4Le4H7FQwLFuJPY44bEKIX4EXpJRfVuPxoP09+Q/wNyHEUCnl+io+7kW0vx+zhRDdpFpIVVGuDillo7kBZ4C8aj4mHZDAA+W269ESgFyglct2d7Q3HhsQ6dg21HGMOy9xrhzgi8t8bgI44DiGh8t2HXAcyAYMjm1egKjgGMsccbd02dbfEfs4l23hjm3/cNl2s2Pb++WOOcyxXZbb7lPB+b2BVCC53Pb3yz/eZd8/HMcPd9n2gWPbLeXavurYfn8Fj//c9TVBG96VwMtVeO2dr9FzaAlyMNARLTGWwLZy7R9zbJ98ieOOcLSb47h/u+P+vBr4v1Cd1+iC34FLHHuco/2fq/Dc/u2ybR1wEggp1zYBbfjW9ffN+XPbBri7bA9B6+nbV+4YF/xulttnB3qU274BLbluUoXnPAktIfwr2v/3qUCG47hVfd2czykBaIqWBCYC+nI/h6cqiP9zx/fPOu7f5bL/R6DgSn9n1E3d1K3iW2Mb2muK1mtUXTlcOEm7K1pP1bvSZYhFapNnX0FLYO5wbHZ+ih4ihGhayXnygFghRFx1A5RSSrReKX+05MXpZqA1sFQ6emGklGZHe4QQ7kKIACFEEFovkg7tD3l1Oc953jCklHItWnJUPt5C5/dCu4oyEC2R+gGIvsTrdFFCCB3aG9kuKeUX5Xa/jPbGVtEk3rnO18QR33a0N+TIapz+n2hvfifReiYmoPXI3VGunfO5le9dKe+c46tvucedq6BtlV3Ba1STnM+hqSMmX+A2YD1QLIQIct7QPswcQPtdLu8/0mXCupTyOFqSaBJCRFcjnp+llNvKbfsBrVc4/FIPllL+V0r5kJRyiZRyvZTyVbSe32zgP9UdIpRSnkMb/o3FMbRdRf8FMoF/CSHcqnNORVEuT2NLpM6hdbtX10FZ7sopoK3ja1IF7Z3b2gFIKf+HNgQxDjjtmAv0TyFETLnHTUJLhPY65qssFkLc4XjjA8CR9LRwvbk8/n20HqX7XLY5v3/X5RgGIcQMIUQa2qTxM2gJwDJHE/8KX4XKtUN7A06rYN++8huEEM0cc1+ygUK0K51OAQ87mvhdRgyg9QY1oYKfi5QyBzjhiLW8QxVsO4M25FhVC4Gb0IbipqEl4CFcODG/fIJ0MeUTLufjLud32NXlvkY1qXxS2AHt79H9aL8H5W8dgOYVHOeC3y20ifhQvedwsZ8/VO93oIyU8gzaUL4fcP1lHOJNtGHxfwohPKt4ziK0nq32/PF/SVGUWtTYEqlEoKkQorpvEkWXblI5KeVf0YZ7/ob2B3oKsEcIMdGlzTq0T7/3oH0aHoR2ufaPjgnKoPVwnCh3cz4+E61X6UYhRIgQIgCt5+FnKaXrG85rwAvAb2jzp25BSwCmOfbX6u+FEEIA36B90l4C/B/aHK2b0Oan1HoMFSifKDuJahxjv5TyOynll1LKV9CG4rqhvZm6SnR87XKJ4zn37y33uM7ViKm+6uT46uytdL7Oy9F+Dyq6ja3FeC7283eN7XKkO74GVfeBjp62v6Ml409U46Hvol0tOkMIcaVJt6Iol9DYJpt/AvRFu3rv2Ss8lvMTbGwF+2LKtQFASpmI9mb4qhDCD21+xywhxHznsJKjR2A5sNyRcMxCq2l1B9pk9ylU3mP0Dlpi9Fe0ngwPXHqjHO4BNkkp73LdKLQaW5frEFryE8WFPR3lh1g6AfHATCnl8+VieKCCY1dn0uwptOHbC34uQgh/oCVa3aVaJ6Xc6phUPVYIMU9K6ZwgvxVtyOcOIUSQlLJ83SEcPRB/QevN+tJxvMNCiF1ok8FNUsqUywytTl8jx4eCe9CSl68dmw+g/ZzdpTZJvKqigd3ltlX4/6+OOIeGsy/z8R+i/Z9/hvN7mi9KSmkTWhmMT4EqFQNVFOXyNbYeqcVon4CfEkKUn7cCgBCiqxBiQhWO9RvaJcf3ug6vOeYlTEV7U1jn2BbgOjwHIKXMReu29wY8hRB6R3Ll2kYCuxx3Axzbdjp6Pcpu5eL6DO2NchzaH95CYGW5NjbKfcoWQvigFRO8XOscX6eWO+4wtGGZ8uenghjiqHhuToFjf8ClgpBS2tFeg85CiMHldj+D9jv/6aWOU4NeQHu+M50bpJQlaBPTm6AlzF6uDxBaCYoFQBjwqjz/yj5nr+GKcsO6ZY8VWtX+8sPGZeryNXI81/fRht3ellIeccR0Bq2Y6QghRM8KHidExZX7n3TprUUIEYJWoyq1XC9sAY7/QzXNMVR+wTCtECIU7YrDM1RylWllHH8DnkEbHpxejcetdZxzMlqxYUVRakmj6pGSUhYJIW5DuxpnrRDiG+BbtD90wcAAtMuxL1m/xfGpbyLaG852IcRCtE/5/wf0BF6SUu53NB+L9gf/U7RP3hagn+Ncq6SUZkcSdUIIsR4teTqJNg/rEeAs2htfVZ6jRQixFO1TLGhXKpWfYP8xWnXplWgFIZujJV1nuExSyq+FEJ+h1eoJAL5Cm6fxEFovnOsE+n1ovVZPC61mUSpaT9ZDaMNYXcsd/hdgIrBACOG8kmqblLKisgqg9TbehPYzXoD2mvdF+9lsogaKklaVlPKAEGIFcLcQoo+UcrNj+0JHD+BUINnxM0tHK4swGm0YeDnaBHbX430rhHgQbf5MqhDCtbJ5BFpl8/ac/3pX5Gq8Rn0cPWuC8yubBzue26Ry7R8BtgCbHK/HLrSkrh1aj+xSLqxubwA2O14HI9q8IC/g8XLtfkEb8p6G9gFISilXXPlTBLSE+LAQYi3a7/ZZtA8PDzj2jZZa/azLIqX8RgjxPdpQf3VMQyv5EI32gUpRlNpQ15cN1sUNrRfoSbQ/2mfR3piz0RKse3Bcbuxomw78WMmx+qElY+fQhmF24XLpuKPNdWhvTAfQ/qCdQxuOmIKjVAFa2YSX0Wr7nEFb8iQdbVgusprPLxpHyQGgz0We/6to5RuK0WoIPYP2h7p8qYP+FWwLp1z5A8d2L7R6WlmA2fFcbqaC8gVovS2r0XrPihxth1NxOQMdWn2n42i9O2XxVNTesb0t2uT5k0Ap2jDPS4B3uXYVPr4qP/sKXqOnLrI/2hH3xos89hO0uW6ljtfjS2D4Jc7ZAS2ZSnO8fsVoCenbQOcq/p5U9TW64HfgEscd5/L7J9GSvLNo/zfeAq6v5LFBjt9N54UQuWjJ9VwgpoKfWyzwuuN3rtjxe3RTBceNRJuXd84Zl8u+CksjuDyP/pd4vh5ovd17+ePvyQm0Dyzdq/H/1vmcEirY1xXtYo5Kyx9U8Lh1jv2q/IG6qVst3YSU1Zl+oiiKUveEVlX/eaCtvPIlWRRFUS5bY5sjpSiKoiiKUmNUIqUoiqIoinKZVCKlKIqiKIpymepsjpQQ4l20JSFOSikvuMLIUUNpLlpNpCK0ia6/Xd0oFUVRFEVRLq4uyx+8D7yBdklzRYagXWkTCfRAu0Kpx6UOGhQUJMPDw2smQkVRlEZi586dp6WUFdXqUhSlEnWWSEkpNwkhwitpcgfaQrsS+EUI4SeEaCmlPFHJYwgPD2fHjh01GaqiKA2ElNrNZgO7Xfvqer+ym9Wq3Wy2Cx/vbOM8luvxnOeFC4/puq18jM6bs43rNud9VxUds/yxpJTY7dpNSujUSU/fvtpjhBBHaudVV5SGrT4X5GwNHHO5f9yx7YJEylGg8EGANm3aXJXgFEXR3pytVigt1W4Wi3YrLtZuZrOkuBhKzHbMJZIis8RcLCkpgZJSbX9pKVhKocQiKSmxY7VK7TildkotYLOCdCQGNrvEbpNYrRKbTWK3C2xWLTGwWSU2q8Bmk9is2uMsVrBYBHYb2O0CKQUgHUmILPdcHPfLNsvzvpfOJ3zBQ2XZcYVzl5Tn1+0v1/68bytsJyvcfTHS5d8Lz3dxo8f60LevX9UaK4pSofqcSFWZlHIhsBAgISFBFcZSFLT3cmdSU1T0R3KjJTjarbBIS2wK8iV5uTYK8u0UFtgoMksKCmyUlArMRXZKS6G42I6lVFBSIiktlpSUCixWgd0mHTmH1u0htXKXjvtlKQgAQjprdJZF6dj2x32gLCEpSyKEa0Ihz2shcGkoXNMO4bgvQTjaIUBI9DqJTkh0OhA6iU6AEHb0eu1Qer1EILV9eh06nUSnk+j1YNBLdHrte50ehA50AvQ67XRCpx3X4CYc9x1nFs7jaNt1OolO79jnOIY2NfSPbUIHep22TeiEFq8QCCERQqArO7ZjHyD0Qoul7Dw6QKAvKsRn8yas4WFYO3ZECOja1btav1OKolyoPidSGUCoy/0QxzZFadAsFsjLk+Sek+TnS/ILJOfyIT9fUlggycuzU1BkJ/+cjYJzNoqKJMVFNkpKwFwkKSyAggKB2azHbtMSGmm3az0u0tFr4uyRkc47zgTGJdFxJi8uuYkQf2Q02hu6wE3YcfewYzCA3mDHw03i4WHD00Pi6WHHw8OOuxt4uEkMeomHtw53Dx3u7hI3N4G7px43Tz2enm64eehx9zRo290Fbm4Cg0GHwaAlEzq9QG8QGBw3nQ4tuTEI9Hpwc/vjq7s7LsegrL0oS1jKPa8Kvq9s2zVDSli7Fv77XzAXQk5zeHItuLnVcWCK0jDU50RqPTDRsU5ZDyDvUvOjFKU+sdu1hOjkaUnOWTu5eXD6jIWTJ22cOmEh55SVMzmSoiJBQSEUFkFRoZ7SEoF0THARjiUIAEfCY9c6WNAmw/yR6MiyN3utU0OHTlhw09vx8rDh7m7D013i5Q2e7nY8PMHTC+2+tw7vJnp8jHqaNBH4+Ljj7aPD01OPp5fA01OHl5cOTy/tq4cHeHgIvLwEnp5a4uJMqv6IQVzbyUdDcfw4/Otf4Jw32rcvPPOMSqIUpQbVWSLlWGS0PxAkhDiOttyDG4CU8i20leBvQVufrgi4t24iVZQ/WCxw5ozkxEkLh46Xkn3KSuZJC2dzBGdPSc6eluSehYJcHQUFeuz2P2YiC0dipI00SaSQ6NC+F47uHx0WPHSSJt6lePtY8PGRNPG24uMD3t7QxEfgYzTg7SNp0tSAsakbPk3d8fLW4+XjhrePG0199TRtKmjSRODhIRzDPjqV3DQmdjt89BEsWAAlJeDvD1Onwk03lRv+VBTlStXlVXujL7FfAo9epXAUhdJSOHUKjmaUcuh4CWnpZk5k2sg6buVsto6c0wYK8vWOy6a0XiKdBCHtlM1sEdrQmU5Y8RDQxLMEP2MJvk2s+PpKfP0FTQN0BARIAoL0BAZA0wBPmgY2oYlRT9OmOnyM7o55ObqyJEib56IoVWS3w4YNWhJ1yy0weTL4+dV1VIrSINXnoT1FqTF2O5w8ZWP/kRIOHSvh2BErmcesZGXaOJklyTntRlGBXpsYbdcmResQCLsOnXDXJv4CTQxW/JqUEOBbTEBTM/5+FgICJX7NPQgM0hMUKPAPNBAY7I5/kDteRj907t7odDr0er3qFVJqj/PKAqMRDAb4xz/g5Em44Ya6jkxRGjSVSCnXPKvNTm6+nQNHSjl8rJT0DDNHjpeQnSnIyYSzp/Tkn/XAZgNhl+jsEiEFCD06oXdcLSXx0dsIbGomqKmZ4GALzZvbaNZKR6sQPS2aS1q2cieohTcGTx90Bj90bl7o9Hr0en1dvwRKY5eUBP/8J7RvDy+/rG2LitJuiqLUKpVIKfWW3S4ptdkpKLZwrriYQ0dLOJFlITNTkpFhI+OY4MQxPWey3THn6xF2qd2kGzpp0IbYkAgBnjqJr08JQX6FtAgsomUbQbNgK61aCVq2kLQMa0Jgc2/cjUHo3T3R6/VqOE2p/4qL4c03tflQzqqh585B06Z1HZmiNBoqkVLqBYvNwtnifA5l5ZFywMa+VDsHD9g5nelGToYneac8webmSJK0y/T1UiKkHZ2w42+w0MyvgGYBRTQLNNOieQnNmtlo2UZPqxA3WoQ2pYmfEZ1PC/QePhgMBjXEplzbduyAF16AjAzQ6WDsWHjoIfDwqOvIFKVRUYmUclVZ7BbOlRRwuiiPgxkF7E22kZYmObrfk1PpRnJP+KKzg84uyhImnbTTRNho5ldIM/9CgpsWEBRYTKtmhYS00xMe5Unz0Ka4Nw3C4NMKvcFNu6khN6UhklIbvluzRrsfGQl//zvExNRtXIrSSKlESqlVBSWFHMk7Q0bOOX7+7Rz79rqTldqUM+lNKcxtgl4KR9KkVcRuorfROjCPdq3O0D40j5BWZtq0sxPSwR9jUy8M3n7ovcPRu2tzlVSypDQ6QoC7u1YL6oEH4K9/1SaXK4pSJ9T/PqXGWG12zhaZOZKXze8HTvPbLitH93ly9oA/J480Q2drgU4KnKuS+XrYaNM8l3atzxIRdg5TbAlRHQRNAgJwC2yLwbspbm6qZ0lRyMnRrsAzmbT7EybAiBHQrl3dxqUoikqklMtnsdk5W1TKueJSdqQdZ/PWEtIT3chMaUrhqXbo7WhXx+n0uOsEIa0KiG13iuj2Z+gUfZbWoQa8W7XDwzcEtyaBGAwGNcFbUVxJCV99BXPmgI8PrFihVWb19lZJlKLUEyqRUqqloMTK6QIze49ls+3XUvbv0XFotydnM4PR20CPDnQGmnjaiQw7R5wpl+jQTOIiTuHXOgjP5u3xDOiJu6eXSpoUpTLZ2dpcqC1btPsdOmgrTXurhYYVpT5RiZRSqWKLjVMFRaTnnGRbYi6//+rGkd1NyNrviyiV6OwCgQ6jJ8RE5RAXdYbO0dlERRbjExyMl38L3Jt2w90YoBInRakKux0+/RTmzoWiIq3A5pNPwu23q+VdFKUeUomUcp5ii41T+SVk5+eRdiqDPbvtpPzmxdHfAik46Y+w2hFSh5teEBmaR+eYbK4znSI2rgS/kNZ4BrTCw/c63Nw9VHkBRbkcM2bAN99o3w8YANOmQVBQ3cakKMpFqURKwW6XZOYVcCjnJGlZp0nbLUje0YQju9pgyTcgrIBdENDUSkL8aXrFH6VbxzP4hbbGKzgMd7/rcHd3V4mTotSEP/1JqxE1bRoMHKh6oRSlnlOJVCN2zlxK0skM9h0/xY5tNg5ta8GxvZHYS0DYJFLqCGlRTM/Op7ih6xHi2p/Bs0V7vJtF4xkQoobqFKUmpKVBYqJ2FR5Av37QrZuaC6Uo1wiVSDUy+cUW0nNy2J1xjJ07zez/pTmHdkUiinRIiw2Ejqi2Zrpfd4p+nQ8Q1s6Gt28gXi3i8fRvjU6VIlCUmlFaCu+8A++/r12dFxurTSgHlUQpyjVEJVKNgMVmJ+1kDsknj5C4/zSpW1uQtrUN5hxPhFUibXaiwvPoe/0pBnY7QPOAQjx8m+ERkoBXoOp5UpQat2ePtrzL4cPa/TvvhNDQuo1JUZTLohKpBqzUamfn8aPsyzpK8m8G9v0YzqG9HRAlNqTVTsvgAvp3z+TGXodoE27Fq1kY3s1uwMOvpUqeFKU2mM2wYIFWD0pKCAvTlne57rq6jkxRlMukEqkGyG6XJGZl82NKKrs3+ZH0Q0fOnXJHllhx15XQp0sWtww+SqeIU3gEheLTui+evs1U8qQote2117TSBjodjBsH48dry70oinLNUolUA1JqtXP07Fk++/UAmzYY2f9zZ+xFOrDaaWHM4dYbD3PLTYfxC/bBu2Uk3q0G4uauVopXlKvmgQfgyBGYPPmP5V4URbmmqUSqAZBScuhUHh9vTeXHdX4c3NYBg0WHLLUR3y6T4b120+OGArzCOmBsNRgvv2aqVIGiXA0//ghffAGzZmm9UM2bw8KFdR2Voig1SCVS1zCrzc7hM7ms//kwX65swtGdHTBYJO52C32uO87//Wkf7bsa8QntQZPAVrirIQRFuTpycuCVV+C777T733wDgwfXbUyKotQKlUhdo5Kyj/HZz0f48eMADv0ShsEKnrpSBnU+xJ13nSQ8rhlNQ27Fy8eo5j4pytUipdYD9e9/w7lz4OUFjz0GN99c15EpilJLVCJ1DSmyFHHo7HG+35XFVx804/DP0egt4GkQDOp9lP8bfozwbib8ml+Hp6dnXYerKI3LiRPw0kvw88/a/V694NlnoWXLuo1LUZRapRKpa0CJpZQdmWn8nJrBlo9CSNkcg94mcDPo6d8rkzFDU2nbrT3+oTfj4aEmjytKndi4UUuimjbVJpPfeqta3kVRGoFqJVJCiFDgn8DNQDNgsJTyByFEMDAbeFNKub3mw2y8dp84wpYD+9jxRSjb13dDFkkMBjd6X5/L6Bt3EtHZj8DIm/DyaVLXoSpK41NSAs4PL3fdBbm58H//B4GBdRqWoihXT5UTKSFEW+AXwNPxtay/Wkp5SgiRADwAqESqBpw157M5PYlvvrWx48Ou5J80gBR0va6Av9zxG9FtTuEfez0+zcLUFXiKcrVZrbB8OXz4IXzwAQQHa1flTZhQ15EpinKVVadH6kXADsQBZuBkuf1fALfXUFyNlsVmZ3fmcX5I2cv2D+NI2tQUYReEhlp5cNRuunU8gV94HD6t+6BzU/OgFOWqS02FmTO1r6CVOBg1qk5DUhSl7lQnkboReF1KeUwIUVG/9REgpGbCapzyiy18fyCJn389y6Z3e5OXCW5ugrF3ZTPi+i0YQyPxi7oLg0FNbVOUq660FBYv1hYZttuhVSv429+gR4+6jkxRlDpUnXfkpsCJSva7V/N4ioujZwr57sAufl4XyNa13ZFmK+3bFPLUpMNEBR3GaLoJn6AQNYynKHUhORmeew7S07UJ5HfdpQ3jeXvXdWSKotSx6iQ+x4DYSvb3BA5cWTiNj80uScnK5fvEZD5/K4zjiX6I0lJG3XKCv96ZSNMmbvjF3I7B27euQ1WUxu3oUQgP1xKqTp3qOhpFUeqJ6iRSa4CHhRDv8EfPlAQQQowERgHP12x4DVuxxcamA+ls/vUUn883UXLagK+3mWcmHyAh+gDG8OswhsYhVEFNRbn60tIgKkr7PiYG5s6Frl3VIsOKopynupPNbwO2AZvQkqhnhBAvAd2B34F/13SADVVBiZUtB4/w5Re5fPteDLLQTnyHszzz2G6aB1kIiL0VD6O6hFpRrrpz5+C11+Dzz7Wvfftq23v1qtu4FEWpl6qcSEkpzwkhegEvAGMAAdwE5AILgL9JKYtrI8iGpqDYwpcpe1m7xJ3fNrRHlNoYdmMmE0b/inebOPzbXofO4FbXYSpK4/PDD9oCwzk5Ws/T6dN1HZGiKPVctSaHSynPAU8ATziKcArglJRS1kZwDVFRqYUPd21h/fxwDm3zx2CXPHR/BsP67sI39maMQa3rOkRFaXxOn9YWGf7hB+1+587w979DmzZ1G5eiKPVedQpyPgeskVImglaEs9z+WGCklHJmzYbYcJhLLaz+7VdWz2lHxm4/vN2sPDtpP71i0vDveDOefi3qOkRFaXz27IEnnoD8fO0qvMcfhxEjtAKbiqIol1CdvxT/ACq7VCWOak42F0IMFkKkCiEOCCGeqWB/GyHERiHELiHEHiHELdU5fn1yLPcsi7du4r2ZoRzf2ZSmnhZeefZXel13gsDrblNJlKLUlYgILYG6/npYtQr+/GeVRCmKUmU1WffJE7BWtbEQQg/MR5tndRzYLoRYL6VMdmk2A1glpXxTCBGDVj09vOZCvjrSTubyRdJ2vnjtOrKT3QgKgtl/T6R9iI3ATrdhcFPzoRTlqrHbYf16+NOfwMtLS6Lefx+CgtQiw4qiVFuliZQQoing57IpUAhR0aSBAOButFpTVdUdOCClPOQ41wrgDsA1kZJohUABfIHMahy/XsjKK+J/h3bx3fw4jiW60yLYwuyZybT1zyag41D0KolSlKvn8GF44QVtOO/QIZg8WdseHFy3cSmKcs26VI/Uk8Bzju8l8F/HrSICeLoa527N+YnXcaD8Wgv/AL4RQjwG+KAtU3PhiYV4EHgQoE09mhxaUFLC12k72LCgHQd3+hDoZ2HWzGTa+p8goPPt6N296jpERWkcrFZYuhQWLQKLRet96tq1rqOq93bu3NnMYDAsRpu6ocY7lcbIDiRardYHunbtWn6NYeDSidSPjq8CLaH6FNhTro0ECoBfpJRbLz/WCo0G3pdS/ttRemGZECJOSmk/LwApFwILARISEurFFYSlVgur9m7iu/cjSN3qTxMvCy/+PZmIwJP4x9+GzsOnrkNUlMYhJUVbZDgtTbs/bJg2udxorNOwrgUGg2FxixYtooODg8/qdLp68bdVUa4mu90uTp06FZOVlbUYGFpRm0oTKSnl/4D/AQghwoC3pJTbaii+DCDU5X6IY5ur+4HBjlh+FkJ4AkFAhVlhfWGxW/jywC9sXdOG379rhpss4Z9PJ2JqfRq/TkPQeTSp6xAVpXE4dAjGjv1jkeEZM6B797qO6loSp5IopTHT6XQyODg4LysrK+5ibapTkPPemgmrzHYgUgjRFi2Bugut0Kero8Ag4H0hRDTahPZT1GNWu5VvD/7Mxi/8+OmTEHSlpfztsX1cF5lJQMdb0XuqJEpRrpp27WDgQGjWDB55RJtcrlSHTiVRSmPn+D9w0aHtal+157jazgT4V3RgKeWmqhxHSmkVQkwEvgb0wLtSyiQhxExgh5RyPTAFWCSEeBJtCHFcfS/+mXLmANu3efHDu+2g2MKDYzPok3CQwOvuwM276aUPoCjK5SsshPnzteE75zp5L72kyhkoilJrqvXXRQgxDTiNNk/qf8DGCm5VJqX8QkoZJaVsL6V80bHtOUcShZQyWUrZW0oZL6W8Tkr5TXWOf7XlFufy3W8ZfPpaByz5Vm6/+TQjB23Hr0MfPHx86zo8RWnYtm6FO+/UakG9/DI4P3OpJOqaptfru5pMppjIyMjYgQMHRpw+fVrv3Ldjxw7Pnj17RoWHh8eFhYXFTZ06taXd/scU2lWrVjWNi4uLbt++fWx0dHTM+PHjQ8of32w2i+uvvz7KZDLFLFq0yP9icXTv3r3Dpk2bvMtvnzdvXuDYsWMvuMrJbrczbty40DZt2sRFRUXFbNmy5YLHAhQUFIhu3bp1sFr/qB40c+bMZh4eHl3OnDlT9lwrOo9rTHl5eboxY8aEhYaGxsXGxkZ37969ww8//HBFk3Gr+hwWLVrkHxUVFRMRERH7yCOPlC3P8corrwRHRUXFmEymmK5du3bYuXOnJ8Cvv/7qNXLkyPAria0+qfJfGCHE/cDLaIsTz0CbgP5f4FUgB9gB3FfjEV4jLDYLX6f8zqrZ8RSfhe7X5fHoXT/RJKov3s3b1nV4itJw5ebCc89pFcmzsyEmBv72N1UTqoHw8PCwp6SkJO/fvz/Jz8/P+uqrrwaDloAMHz484umnn85KT09PTExMTN62bVuT2bNnBwNs377dc8qUKW2WLVt2+ODBg0l79+5NjoiIKCl//K1bt3oDpKSkJI8fP/5sTcW9evVq30OHDnmmp6cnvvnmm0cmTJhQ4SXlr7/+etDQoUPPGgx/DBB9/PHHAXFxcYXLly/3q+r57r777nB/f39renp6YlJS0r6lS5cePnny5BXViqzKc8jKytI/99xzIT/++GPagQMHkrKzs93WrVtnBHjggQfOpKWlJaekpCRPnjw5a9KkSaEA3bt3N584ccJ9//797lcSX31RnY9qj6BdmTcAxxVywAYp5TNoFc/D0YboGqXvDv7GJ4tCyDnmRUhgPs8+/DM+bWLxbdmurkNTlIZJSvj2Wxg1Cr74QltkeNIkeO89rVq50uD07NmzMCMjwx1g0aJFgQkJCQUjRow4B2A0Gu1vvvnm0blz57YEeOmll1pMmTLlROfOnYsBDAYD06ZNO2+ObUZGhuHee+9tu3fvXm+TyRSTlJTksW7dOmN0dHRMVFRUzKhRo8LNZvMFGfncuXMDw8PD4zp27Bi9devWCie+rlu3zu/uu+8+o9PpGDRoUOG5c+cMR44cuaBw4KpVqwLvvPPOXOf9pKQkj6KiIv3MmTMzVq1aFVCV1yUpKclj165dPnPnzs3Q67W3YZPJVHrXXXflVeXxF1OV55CamuoRHh5e0qpVKyvAoEGDzq1evdofICAgoKx7sKCgQC9cPtwMGTIkd8mSJRftAbyWVCeRigZWO753zlPSA0gpT6AlV0/UXGjXjqSsE3y/0cbe71rhZi9l+tif8GsZSED7Lgj1qVhRasfZs1pxzbNnoUsXWLkS/vIX0Dfaz3MNmtVqZePGjcZhw4blAiQlJXl26dKlyLVNbGxsSVFRkS4nJ0eXmprq1aNHj6IKD+bQunVr64IFC44kJCQUpKSkJLdt27b0oYcearty5cqDaWlpyVarFWcPmNORI0fcZs2a1Wrr1q0p27dvT0lLS6vwCoYTJ064hYeHlzrvt2zZsrR8ElJcXCyOHTvm0aFDh7J2S5cu9R8+fHjO4MGDCw4fPux57NixS/Yq/f77754xMTFFrr1aF3Prrbe2M5lMMeVvb7zxRuDlPIeYmJiSQ4cOeaamprpbLBbWr1/vn5mZWdbT9PLLLweHhobGPf/88yHz588/6tzeo0ePwq1btzaIGiTV6fazAYWO751fXV/4dCCyBmK6ppwtKuHrPan8sLgzWG3ce2sS0QkQGNsPnZqboSg1S0rtptNBQAA89ZRWbHPYMDUX6ipY93tGjU/2vOO61pX2mpSUlOhMJlNMdna2W/v27YuHDRt2rqZjcNq9e7dnSEhISadOnUoAxo0bd2b+/PnNcCm5s2nTJp+ePXvmO3tgRowYkZOWluZ5OefLysoyGI3G85ZWW7NmTeCaNWsO6PV6brnllrPLli3zf/bZZ09d7EN5dT+sb9iw4dDlxHoxwcHBtv/85z9HRo0a1U6n09GtW7eCw4cPezj3T58+/dT06dNPvfXWWwHPP/98yzVr1qQDtGzZ0pqdnd0glvaoTiJ1FGgLIKUsEUIcA/oAKxz7u6HNlWpUtqSn8O07ERSd1dO1XRYjb92HX8xwDG4NYuhXUeqPjAz4179gwABtUjnA0Arr4ym15FJJT21wzpHKz8/X9e/fP3LWrFnNZsyYcTImJqZ48+bN5w2rJScnu3t7e9sDAgLsUVFRxdu2bfPu1auX+WrHDNCyZUtLenp62RvBiRMn3MPCwiyubXx8fOylpaVlnwB+/fVXryNHjngMHjw4CsBisYiQkJDSZ5999lRQUJA1Nzf3vO7W3NxcffPmza0BAQG2ffv2eVutVi7VK3Xrrbe2O3jw4AWJ38SJE7MnTpx4prrPAWDMmDF5Y8aMyQOYM2dOkL6CXuHx48fnTJ06tWyOldls1nl6etovaHgNqs5HuE3ArS73VwMPCSHeFUK8DzyAtqhwo5Gdf47PNxRz+Hd/fD3MTB23BWNkd7z9guo6NEVpOOx2+PBD+L//g+3bYflyrRdKaVSMRqN93rx5RxcsWNDcYrHw4IMPntm+fbtx7dq1RtAmnz/66KNtHnvssSyA6dOnZ7322mst9+zZ4wFgs9l45ZVXKl1UMT4+vjgjI8M9MTHRA2Dp0qWBffr0yXdt07dv38Jt27YZs7Ky9CUlJeLTTz+tcJ7P0KFDcz/44INAu93O999/72M0Gm3lk5Dg4GCbzWYTRUVFwnG+gClTpmRmZGTszcjI2Hvy5Mk92dnZbmlpae433HBD4c6dO5scPXrUALBp0ybv0tJSXfv27UtjY2NLOnXqVDh58uRWzqsWU1NT3VesWHFBD+KGDRsOpaSkJJe/lU+iqvocQJtrBnDq1Cn94sWLm02YMOEUwN69e8t6plauXOkbFhZWNtk/OTnZo0OHDnWS5Na06vRIzQV2CyG8pJRm4HkgCvirY/83wDM1HF+9ZbNLvvj9IFtXRCEsNiaM3EZLUxB+4R3rOjRFaTgOHdKWd0lM1O4PHgxTpkAV5oIoDU/v3r3NJpPJvHDhwoBHH300Z82aNQcmTpzYZtKkSW52u51Ro0admT59+kmAHj16mGfPnn1s9OjR7cxms04IwU033VRpj5q3t7d866230keNGtXeZrMRHx9f9NRTT503QT0sLMwybdq0zJ49e0YbjUZbXFxchfOw7rzzzrwNGzb4hoWFxXl5edkXL16cXlG7vn375n3zzTdNhg0blr927dqAzz77bL/r/iFDhpxdsmRJwIsvvpg1e/bsY4MHD4602+3Cx8fHtnz58kPO3p/ly5enT5gwITQsLCzO09NT+vv7W1999dVjFZ2zqip7DiaTKSYlJSUZ4OGHHw5NTk72Bpg2bVqmc2j0tddea7Z58+amBoNB+vr6Wt9///3Dzsf/8MMPTW+77bar3sNZG8SV1rcUQvgCNillQc2EdGUSEhLkjh07av08iZkneea5HA5sbEXXtid4+dnttLh+JO6eqnKyolwxqxXefx8WL9a+b9YMpk+HPn3qOrIGSwixU0qZ4Lpt9+7d6fHx8afrKqbGYMuWLd5z5sxpvnbt2sOXbt0wmM1m0bNnzw47duxIcXO7NqZJ7d69Oyg+Pj68on1XPDtTSpknpSwQmnuu9HjXglKrnVXfHid1Uys8hIUn/roTv9g+KolSlJoiBPzvf1oSNWKEVmRTJVFKA3TDDTcU9e/f/5y1EQ1XHzhwwP3FF1/MuFaSqEu54v5xoV0yMBr4O9pQ37IrPWZ9t+d4Fp+/0xK9RTJmSBJhsX4Ym1VYa01RlKoqLoaSEvD11UoYPP885OVB1651HZmi1KpJkyZdMD+pIevYsWNJx44dLyiOeq26ZI+UEOIGIcQ6IUSyEGKLEOIhl31/AhLRkqdWwOzaC7X+eGtJLucyfGgblMOoOw7jF91P1YtSlCuxcyeMHg0vvvjHtogIlUQpilLvVdojJYToDXwPuPa/9RJC+ACewL+AXOAFYK6UssbK69dXB0+dZMtngbjZbYy//Wf8O/XDw+uKljNSlMaroADmzYM1a7T7Hh6Qnw/GBlGnT1GURuBSQ3vTgBLgz2gJVQSwFG2tPSPwNjBdSplbizHWKws+OIklpw3RgZkk3NIc3+ZqSE9RLsuWLfDSS3DypHYV3v33w7hx0EDmTSiK0jhcKpHqAbwtpfzMcX+PEOIptFIHS6SUj9RqdPXMvuwsvv84EHe7jVE3pxBkGqSqlytKdUmpzX/6wlF2Li5OW3S4nVqXUlGUa8+lsoBAIKncNuf9tTUeTT23dE02BVlNaGk8Tf+RwXj5VLhWpaIolREC/P21YbzJk+Hdd1USpVyUXq/vajKZYiIjI2MHDhwYcfr06bKy2Tt27PDs2bNnVHh4eFxYWFjc1KlTWzoLUgKsWrWqaVxcXHT79u1jo6OjY8aPHx9S/vhms1lcf/31USaTKWbRokUXXUS3e/fuHTZt2uRdfvu8efMCx44de8HQxK5duzyvu+46k7u7e5fnnnuu+cWOa7fb6dmzZ1ROTk7Z+/GyZcv8hBBdd+3aVVaB/PPPPzcOGDDgvNW4R44cGf7ee+/5A5SUlIgJEya0DgsLi4uJiYm+7rrrTKtWrWp6sfNW1fTp01u0adMmLjw8PO6TTz6p8Hjr1683xsTEREdGRsaOGDEi3GLRana++eabAVFRUTFRUVExnTt3Nv38889eoK0xmJCQ0MHZ7lp3qURKB5SW2+a8n08jkpF7jo1rAnCTNkbcsI+AqE5qgrmiVNXJk38U1QR45BGtpMGYMWqNPKVSziVi9u/fn+Tn52d1LiJcUFAghg8fHvH0009npaenJyYmJiZv27atyezZs4MBtm/f7jllypQ2y5YtO3zw4MGkvXv3JkdERFxwpdjWrVu9AVJSUpLHjx9fY/N8mzVrZp07d+7Rhx56KLuydqtWrfKNjY01BwQElGWAK1asCOjSpUvB0qVLA6p6vieffLJVVlaWW0pKSlJycvK+zz777MC5c+euaAXvnTt3eq5ZsyYgNTU16auvvkqbNGlSm/JlGmw2Gw8++GDbFStWHNq/f39SmzZtSt94440ggIiIiJKffvopNS0tLXn69OmZDz30UBiAp6en7Nev37nFixdX+fnVZ1X5C+YjhAhw3gDnEze6bnfZ3yB98n0WJw/74ueez00j3PDxURPMFeWS7HZtIvmoUfD001DoWO/c0xNat67b2JRrTs+ePQszMjLcARYtWhSYkJBQMGLEiHOgLSHz5ptvHp07d25LgJdeeqnFlClTTnTu3LkYwGAwMG3atPOqlGdkZBjuvffetnv37vU2mUwxSUlJHuvWrTNGR0fHREVFxYwaNSrcbDZf8Il57ty5geHh4XEdO3aM3rp1a4VDE61bt7b269evyM3NrdKq1x988EHA8OHDc5338/LydNu3b2/y3nvvpX/66adVek/Nz8/Xffjhh8GLFy8+6uXlJQFCQ0OtDzzwwBUlhh9//LHfiBEjcry8vKTJZCoNCwsr+fHHH89788vOzja4ubnZndXMBw8efG7t2rV+ADfddFNhcHCwDWDAgAGFWVlZZev2/fnPf85dsWJFg8gZqpJIvQWccrmlOLavKbf9FC4rZDckhSWlfPKBHje7naFdd9O8a3c1N0pRLuXYMa3n6aWXtATKZILS8h3cilI1VquVjRs3GocNG5YLkJSU5NmlS5fzlmeJjY0tKSoq0uXk5OhSU1O9evToUeHyLU6tW7e2Lliw4EhCQkJBSkpKctu2bUsfeuihtitXrjyYlpaWbLVacfaAOR05csRt1qxZrbZu3Zqyffv2lLS0tCuqxLxz584mvXv3LnTe//DDD/369++f16lTpxJ/f3/r5s2bLxhOLC85OdmjZcuWpa69Whdz//33h5pMppjyt2effbZF+bYZGRnuoaGhZf9pW7VqVXrs2DF31zYtWrSw2mw24Rz2XLlypf+JEyfcyx/r9ddfDxowYEDZkjDdunUz79mzp0H0SFxqsvmSqxJFPbd5bzZH9gbiL8wMudOA0devrkNSlPrLucjwm29qBTb9/bXeqBtv1OZHKdeuvasvWAT3inUcVel6ayUlJTqTyRSTnZ3t1r59++Jhw4adq/EYHHbv3u0ZEhJS4uxdGTdu3Jn58+c3w6WTYNOmTT49e/bMb9WqlRVgxIgROWlpaZ4XOeQl5eXlGfz9/csSoFWrVgU8/vjjJwFGjhyZs2zZsoA+ffoUCSEq7Nm62PaLeeedd65o/b3ydDodS5cuPfTkk0+GlpaW6gYMGJBXvqPhs88+My5fvjxo69atzo4YDAYDbm5u8uzZszrX538tqjSRklLee7UCqc8++rAEg81Inw6phPbqgnORSEVRKvDMM/DDD9r3t9yiLTLsW/Pvv0oduETSUxucc6Ty8/N1/fv3j5w1a1azGTNmnIyJiSnevHnzecNqycnJ7t7e3vaAgAB7VFRU8bZt27x79eplvtoxV4der5c2mw29Xk92drb+l19+MaampnpNnDgRm80mhBDSbrcfb9asmTUvL++89+yzZ88agoODrTExMSUnTpxwz8nJ0V2qV+r+++8P/emnny4o1DZixIicl156Kct1W+vWrc/rgcrMzDyvh8rpxhtvLNy5c2cqwJo1a5oeOHCgLLHctm2b14QJE8I2bNiwv0WLFjbXx1ksFuHt7X1lC/7WA2p86hJOnivkl++b4mG3MPiWAnz9G8SQrqLUnqFDoXlzmDsXZs5USZRSI4xGo33evHlHFyxY0NxisfDggw+e2b59u3Ht2rVG0CafP/roo20ee+yxLIDp06dnvfbaay337NnjAdqk6FdeeSW4snPEx8cXZ2RkuCcmJnoALF26NLBPnz7nXVjVt2/fwm3bthmzsrL0JSUl4tNPP73olX5V0bZt2+J9+/Z5ACxbtsx/+PDhOZmZmXszMjL2ZmVl7QkJCSn9+uuvm8TFxZVkZ2e7/fbbb54AaWlp7ikpKV49e/Y0G41G+1133XX6wQcfbFNcXCwAMjMzDe++++4Fsb3zzjvHUlJSksvfyidRACNHjsxds2ZNgNlsFikpKe7p6eme/fv3LyzfLiMjwwDaFZCvvvpqi4cffvgUwP79+91HjRrV/t133z3s7OVzysrK0vv5+Vk9PDxUItXQvbMqi9J8d9oFnKDzrVEYDFe8PKGiNCyJifDRR3/cv+EG+PRT6N277mJSGqTevXubTSaTeeHChQFNmjSRa9asOfDSSy+1Cg8Pj4uJiYnt0qVL4fTp008C9OjRwzx79uxjo0ePbteuXbvYqKio2EOHDnlUdnxvb2/51ltvpY8aNap9VFRUjE6n46mnnjpvgnpYWJhl2rRpmT179oxOSEgwRUVFFVd0rKNHjxqaN2/eaeHChc3/85//tGzevHkn1xIHTjfffHPeN998YwRYvXp1wIgRI86bIH7HHXecXb58eYCXl5d87733Dt17773hJpMpZsSIEe3nz59/JDAw0Abw3//+NyMoKMgaFRUVGxkZGTt48OAIX19fW/nzVUdCQkLxsGHDcqKiomIHDx4c9dprrx1xvgf269cvIj093Q1g5syZLdq1axcbHR0dO2TIkNyhQ4fmA8yYMaNlbm6u4bHHHgszmUwxcXFx0c5jf/nll01vvPHGq97DWRuElNd8MniehIQEuWPHjho5ls0u6XvLMc6kevP4yF3c+8INeHld0bxCRWk4zGZtHtRHH2lzn5YsgejoSz9OqZeEEDullAmu23bv3p0eHx9/uq5iagyOHDniNnr06PCtW7fur+tYrqabb765/Zw5c46X76mqr3bv3h0UHx8fXtE+1SNViR+2n+HY/qYY9WYGDDPi6XnZ8wkVpWHZvh3uukubVC4E3HOPKqqpKJchLCzMct99952uqLeqoSouLhZDhw7NvVaSqEtR41SVeOf9fNykHwPi0mjdsasqwKko+fna3Ke1a7X7UVHw97+rnihFuQJXWu/pWuPp6SknTpx4pq7jqCkqkbqI02dL2LnJiCdWBt9WiE8TtRq9ovDf/8K6ddrCwuPHw9ix2oLDiqIojZT6C3gRn35xFkuxJ7EB6cTdFK1KHigKwMMPw6lT8OST0LZtXUejKIpS56o1JiuEMAohnhNCbBFC7BdC9HJsD3JsN9VOmFff199YcbNbuL7raZq2CK3rcBTl6pMSvvgCHn8cbI6Lf4KDYd48lUQpiqI4VLlHSggRDGwB2gEHHF+9AKSUp4UQfwX8gMk1H+bVVWi2snuHJ+52C32HeKpJ5krjk52tLe3y00/a/e+/h5tvrtuYFEVR6qHq9Ej9C2gB9AD6AOVnXq8DBtVQXHXq06/OYDHrCAs4TYc+Heo6HEW5eux2+PhjbZHhn34CoxGefx5uuqmuI1MaKb1e39VkMsVERkbGDhw4MOL06dNl8yx27Njh2bNnz6jw8PC4sLCwuKlTp7a02/8o7L1q1aqmcXFx0e3bt4+Njo6OGT9+fEj545vNZnH99ddHmUymmEWLFl20uGb37t07ONeTczVv3rzAsWPHtim//c033wyIioqKiYqKiuncubPp559/rrB2jt1up2fPnlGuV+0tW7bMTwjRddeuXWWf4j///HPjgAEDIlwfO3LkyPD33nvPH6CkpERMmDChdVhYWFxMTEz0ddddZ1q1alXTiz2fqpo+fXqLNm3axIWHh8d98sknFR5v/fr1xpiYmOjIyMjYESNGhFssFgCWL1/uFxUVFeOsIfX11183Aa1YaJ8+fSKvNLb6ojqJ1G3AAinlb0BFxacOAQ1iDGzDl8W42W1cn5CLj1FVZVYaiaNHtTlQs2ZBUREMHAirV8Ptt6s18pQ641wiZv/+/Ul+fn5W5yLCBQUFYvjw4RFPP/10Vnp6emJiYmLytm3bmsyePTsYYPv27Z5Tpkxps2zZssMHDx5M2rt3b3JERMQFl9tv3brVGyAlJSV5/PjxNXb1XERERMlPP/2UmpaWljx9+vTMhx56KKyidqtWrfKNjY01uy7tsmLFioAuXboULF26tMpLaTz55JOtsrKy3FJSUpKSk5P3ffbZZwfOnTt3RZN7d+7c6blmzZqA1NTUpK+++ipt0qRJbaxW63ltbDYbDz74YNsVK1Yc2r9/f1KbNm1K33jjjSCA22+//Zyzcvo777yT/vDDD4cBtGrVytq8eXPLN9980yAWLa5OIhWENqR3MXbgmh8DKy61s2ubF+62UnoP8cHd/YJFrBWlYfr5Z/jtNwgIgFde0W5BQXUdlaKU6dmzZ2FGRoY7wKJFiwITEhIKRowYcQ60JWTefPPNo3Pnzm0J8NJLL7WYMmXKic6dOxeDtkjutGnTzqtSnpGRYbj33nvb7t2719tkMsUkJSV5rFu3zhgdHR0TFRUVM2rUqHCz2XzBp4i5c+cGhoeHx3Xs2DF669atTcrvB7jpppsKg4ODbQADBgwozMrKqvDN5IMPPggYPnx4rvN+Xl6ebvv27U3ee++99E8//bRKiVR+fr7uww8/DF68ePFRLy8vCRAaGmq90rIKH3/8sd+IESNyvLy8pMlkKg0LCyv58ccfz0t+srOzDW5ubnZnTajBgwefW7t2rR+Ar6+v3bmAcX5+vs61hNCwYcNyly5dGngl8dUX1UmksoD2lezvDBy9snDq3tf/y6M4T0/LgALi+6sJtUoDV+iybNaoUTBhgja0N3Bg3cWkKBWwWq1s3LjROGzYsFyApKQkzy5duhS5tomNjS0pKirS5eTk6FJTU7169OhRVOHBHFq3bm1dsGDBkYSEhIKUlJTktm3blj700ENtV65ceTAtLS3ZarXi7AFzOnLkiNusWbNabd26NWX79u0paWlpl1zu4vXXXw8aMGBAhcuh7Ny5s0nv3r3L/iN++OGHfv3798/r1KlTib+/v3Xz5s0XDCeWl5yc7NGyZcvSSy1YDNqixSaTKab87dlnn21Rvm1GRsZ5ixS3atXqvEWMAVq0aGG12WzCOey5cuVK/xMnTpS1Wbp0qV/btm1jR44cGblw4cJ05/bevXsX/vrrrxUmodea6pQ/+AK4XwjxOnDe6s9CiB7AWOC/NRda3Vi3oRCD3Z1e3fNo0kQlUkoDVVoKixdrSdNHH2mLDOt0cN99dR2ZUo99ceiLGp/rcEu7Wypdb62kpERnMplisrOz3dq3b188bNiwczUdg9Pu3bs9Q0JCSpy9K+PGjTszf/78ZsBJZ5tNmzb59OzZM79Vq1ZWgBEjRuSkpaVddDTms88+My5fvjxo69atKRXtz8vLM/j7+5clQKtWrQp4/PHHTwKMHDkyZ9myZQF9+vQpEkJUuJ7bxbZfzDvvvHOsOu0vRafTsXTp0kNPPvlkaGlpqW7AgAF5zl4ogLFjx+aOHTs298svv2zy3HPPtb7xxhvTQBveO3nyZIMY8qlOIvVPYCiwC1iPNk/qr0KI8cAIIBOYXZ2TCyEGA3MBPbBYSjmrgjZ3Av9wnG+3lHJMdc5RHRarnW1b3HCTNm64UY+bm1ttnUpR6s6ePTBzJqSna3Oftm6F4cPrOirlGnCppKc2OOdI5efn6/r37x85a9asZjNmzDgZExNTvHnz5vN6NJKTk929vb3tAQEB9qioqOJt27Z59+rVy3y1Y3batm2b14QJE8I2bNiwv0WLFhUuIKzX66XNZkOv15Odna3/5ZdfjKmpqV4TJ07EZrMJIYS02+3HmzVrZs3LyzvvPfvs2bOG4OBga0xMTMmJEyfcc3JydJfqlbr//vtDf/rppwsqTI8YMSLnpZdeynLd1rp16/N6oDIzM8/roXK68cYbC3fu3JkKsGbNmqYHDhy4ILEcMmRIwfjx4z1OnDhhaNmypbWoqEh4eHhcsgftWlDloT0pZRbQE9gG3Id21d49wJ3AN0AfKWVOVY8nhNAD84EhQAwwWggRU65NJDAd6C2ljAUmVfX4l+PHX89SeMaN4KaFJAwIVUvCKA1LURHMmQP3368lUWFhsGiRSqKUa4LRaLTPmzfv6IIFC5pbLBYefPDBM9u3bzeuXbvWCNrk80cffbTNY489lgUwffr0rNdee63lnj17PECbFP3KK68EV3aO+Pj44oyMDPfExEQPgKVLlwb26dMn37VN3759C7dt22bMysrSl5SUiE8//bTCK/3279/vPmrUqPbvvvvu4crWlGvbtm3xvn37PACWLVvmP3z48JzMzMy9GRkZe7OysvaEhISUfv31103i4uJKsrOz3X777TdPgLS0NPeUlBSvnj17mo1Go/2uu+46/eCDD7YpLi4WoF0Z9+67714Q2zvvvHPMOQHc9VY+iQIYOXJk7po1awLMZrNISUlxT09P9+zfv39h+XYZGRkG0K6AfPXVV1s8/PDDpwASExM9nFdRbtmyxbu0tFQ0b97c6tjnGRUVVWdJbk2qVmVzKeUx4A4hRFOgA1oydaA6CZSL7o7HHgIQQqwA7gCSXdqMB+ZLKc86zn/ygqPUoB82WdDbDHS/7gxNfBvEBYiKotm9W1sTLzNTG8IbN05b4kVdTKFcQ3r37m02mUzmhQsXBjz66KM5a9asOTBx4sQ2kyZNcrPb7YwaNerM9OnTTwL06NHDPHv27GOjR49uZzabdUIIbrrppkp71Ly9veVbb72VPmrUqPY2m434+Piip5566rwJ6mFhYZZp06Zl9uzZM9poNNri4uIqnIc1Y8aMlrm5uYbHHnssDMBgMMjExMR95dvdfPPNed98840xLi6uZPXq1QFTp049L6G54447zi5fvjxgyJAhBe+9996he++9N7ykpERnMBjk/PnzjwQGBtoA/vvf/2ZMmjSpdVRUVKyHh4f08vKyPf/885nVe4XPl5CQUDxs2LCcqKioWL1ez2uvvXbE4FgSql+/fhFLliw5Eh4ebpk5c2aLb7/91tdut4v77rvv5NChQ/MBPvroI/+VK1cGGgwG6enpaV+2bNkh57Dft99+axw8ePBV7+GsDULKqg2vCiECpZQ1tsigEOLPwGAp5QOO+/cAPaSUE13arAXSgN5ow3//kFJ+VcGxHgQeBGjTpk3XI0eOXFZMN488QfqvOv4x9SijH0tQPVJKw5GWBn/5C0REaHWhOqj6aMr5hBA7pZQJrtt2796dHh8ff7quYmoMjhw54jZ69OjwrVu37q/rWK6mhISEDl9++eUB55WN9d3u3buD4uPjwyvaV52r9jKFEGuEEHcIIa7WGn0GIBLoD4wGFgkh/Mo3klIulFImSCkTgoMr7bm9qJISycEkd/TSRo8/tVJJlHLt27v3j++jouCtt2DpUpVEKUo9EhYWZrnvvvtOuxbkbOgyMzMNTzzxRPa1kkRdSnV+cGuAPzm+nhBCzBNCJFziMZXJ4PwCniGOba6OA+ullBYp5WG03qlaqYb6y84ibEV22obk06pNlWugKUr9c+YMTJsG994LGzf+sb1LFzCodcoVpb554IEHzlaldEFD0apVK+s999yTW9dx1JTqTDYfjbZEzINo85geBbYJIZKEEFOFEK2qee7tQKQQoq0Qwh24C+1qQFdr0XqjEEIEAVFoFdRr3Lf/y0dvtxN/nRUPD4/aOIWi1C4pYcMGrR7U99+Dl9f5daIURVGUGletrkQpZb6U8h0pZT+0RYv/AbihlT04IoS4YP5SJceyAhOBr4F9wCopZZIQYqYQYqij2dfAGSFEMrARmFqT87Rcbd9mRy9tdL7BB9caGIpyTThxAh5/XJv/dO4c9OoFq1bBbbfVdWSKoigN2mX380spjwAvAC8IIUYDbwLVWtlUSvkFWqFP123PuXwvgcmOW60xF9s5lOyGm97G9f3VsJ5yjdm1S0uizGZo2hSmTIFbblHr4ymKolwFl51ICSGaoNWQGgvcgNa7lVhDcV1Vv+wswma2E9XWTLNmKpFSrjEdOoC/P1x/vTY3KkD9DiuKolwt1RrDEprBQogPgWxgMVoxzTeArlLKTrUQY637cXMRwmYnvptQ1cyV+s9q1ZZ1KXKUr/H21q7Gmz1bJVFKg6PX67uaTKaYyMjI2IEDB0acPn1a79y3Y8cOz549e0aFh4fHhYWFxU2dOrWlswAkwKpVq5rGxcVFt2/fPjY6Ojpm/PjxIeWPbzabxfXXXx9lMpliFi1aVGFxTYDu3bt3cK4n52revHmBY8eObVN++/Lly/2ioqJiTCZTTFxcXPTXX39d4bpyBQUFolu3bh2sVmvZtpkzZzbz8PDocubMmbLnWtF5XGPKy8vTjRkzJiw0NDQuNjY2unv37h1++OGH8xYYri673c64ceNC27RpExcVFRWzZcuWCtf9W7RokX9UVFRMRERE7COPPNLauf2VV14Jdr4GXbt27bBz505PgF9//dVr5MiR4VcSW31S5URKCDEH7aq6DWhLwnwJDANaSSknSSl31UqEV8GOLRZ0AhKu91JlD5T6LTUVxo6Ff/8b5s//Y7ufX52FpCi1yblEzP79+5P8/PyszkWECwoKxPDhwyOefvrprPT09MTExMTkbdu2NZk9e3YwwPbt2z2nTJnSZtmyZYcPHjyYtHfv3uSIiIgLKoxv3brVGyAlJSV5/PjxZ2sq7ttvv/2cs2r4O++8k/7www+HVdTu9ddfDxo6dOhZg8sVtR9//HFAXFxc4fLly/2qer6777473N/f35qenp6YlJS0b+nSpYdPnjx5RZfprl692vfQoUOe6enpiW+++eaRCRMmXJAwZmVl6Z977rmQH3/8Me3AgQNJ2dnZbuvWrTMCPPDAA2fS0tKSU1JSkidPnpw1adKkUIDu3bubT5w44b5///4GURG4Oj1Sk4FjwGNASynln6WU6x2Txq9ZZjOkpRjQu+npfUODWIhaaYhKS+GNN+Cee7Timq1aQZ8+dR2VolxVPXv2LMzIyHAHWLRoUWBCQkLBiBEjzoG2hMybb755dO7cuS0BXnrppRZTpkw50blz52IAg8HAtGnTzqtSnpGRYbj33nvb7t2719tkMsUkJSV5rFu3zhgdHR0TFRUVM2rUqHCz2XzBp+u5c+cGhoeHx3Xs2DF669atFb5x+Pr62p0XLuXn5+su9iF91apVgXfeeWeu835SUpJHUVGRfubMmRmrVq2qUhdzUlKSx65du3zmzp2boddrnVgmk6n0rrvuuqLK4evWrfO7++67z+h0OgYNGlR47tw5w5EjR84btklNTfUIDw8vcS7iPGjQoHOrV6/2B3At6VBQUKB3fQ2GDBmSu2TJkov2AF5LqpNIxUgpe0gpFziXbGkItu0sxlZio114CYGBquyBUg/9/jvcdRe8/75W4mD0aFixAnr2rOvIFOWqsVqtbNy40Ths2LBcgKSkJM8uXbqctzxLbGxsSVFRkS4nJ0eXmprq1aNHjwqXb3Fq3bq1dcGCBUcSEhIKUlJSktu2bVv60EMPtV25cuXBtLS0ZKvVirMHzOnIkSNus2bNarV169aU7du3p6SlpXld7PhLly71a9u2bezIkSMjFy5cmF5+f3FxsTh27JhHhw4dSl0e4z98+PCcwYMHFxw+fNjz2LFjl+xV+v333z1jYmKKDFWoE3frrbe2M5lMMeVvb7zxRmD5tidOnHALDw8vi61ly5al5ROpmJiYkkOHDnmmpqa6WywW1q9f75+ZmVnW0/Tyyy8Hh4aGxj3//PMh8+fPP+rc3qNHj8KtW7desHjytajK3X5SypTaDKSu/LS1CGGTdOwmqMovoaJcVQcPamviSQlt22rr5XW6JqciKg1A3uef+9b0MX1vu63SXpOSkhKdyWSKyc7Odmvfvn3xsGHDztV0DE67d+/2DAkJKXEuMjxu3Lgz8+fPbwaUrfO6adMmn549e+Y7e2BGjBiRk5aW5lnR8caOHZs7duzY3C+//LLJc8891/rGG29Mc92flZVlMBqN543qrFmzJnDNmjUH9Ho9t9xyy9lly5b5P/vss6cu1qNV3ekoGzZsqNFajMHBwbb//Oc/R0aNGtVOp9PRrVu3gsOHD5f1SkyfPv3U9OnTT7311lsBzz//fMs1a9akA7Rs2dKanZ3dICYlXzRzEEKMdXy7TEopXe5XSkq5tEYiu0p2/VoK6OjVu0H8PJWGpn17GDJEG8q77z61yLBSpy6V9NQG5xyp/Px8Xf/+/SNnzZrVbMaMGSdjYmKKN2/efN6wWnJysru3t7c9ICDAHhUVVbxt2zbvXr16ma92zOUNGTKkYPz48R4nTpwwtGzZsixx8vHxsZeWlpaNDP36669eR44c8Rg8eHAUgMViESEhIaXPPvvsqaCgIGtubq7e9bi5ubn65s2bWwMCAmz79u3ztlqtl+wQuPXWW9sdPHjwgsRv4sSJ2RMnTjyvTmPLli0t6enpZX90Tpw44R4WFmYp/9gxY8bkjRkzJg9gzpw5Qc7hRVfjx4/PmTp1atkcK7PZrPP09GwQ1dwrG9p7H3gPreCm6/33K7m9V9MB1ia7HVL3CXQGPd26VXgxgqJcXXl58M9/wj6XReL/+U94+GGVRCmNmtFotM+bN+/oggULmlssFh588MEz27dvN65du9YI2uTzRx99tM1jjz2WBTB9+vSs1157reWePXs8AGw2G6+88kqli7HGx8cXZ2RkuCcmJnoALF26NLBPnz75rm369u1buG3bNmNWVpa+pKREfPrppxXO80lMTPRwXkG4ZcsW79LSUtG8efPzep+Cg4NtNptNFBUVCcf5AqZMmZKZkZGxNyMjY+/Jkyf3ZGdnu6WlpbnfcMMNhTt37mxy9OhRA8CmTZu8S0tLde3bty+NjY0t6dSpU+HkyZNbOc+ZmprqvmLFigt6EDds2HDIOQne9VY+iQIYOnRo7gcffBBot9v5/vvvfYxGo62iRCojI8MAcOrUKf3ixYubTZgw4RTA3r17y3qmVq5c6RsWFlY22T85OdmjQ4cOdZ7k1oTKUtcBAFLKUtf7DUl6OhQV2GkeLGjRQs2PUuqQlPDDD1oJg5wc7Zfz3Xe1oprqSlJFAaB3795mk8lkXrhwYcCjjz6as2bNmgMTJ05sM2nSJDe73c6oUaPOTJ8+/SRAjx49zLNnzz42evTodmazWSeE4Kabbqq0R83b21u+9dZb6aNGjWpvs9mIj48veuqpp86boB4WFmaZNm1aZs+ePaONRqMtLi6uwnlYH330kf/KlSsDDQaD9PT0tC9btuxQRatm9O3bN++bb75pMmzYsPy1a9cGfPbZZ/td9w8ZMuTskiVLAl588cWs2bNnHxs8eHCk3W4XPj4+tuXLlx9y9v4sX748fcKECaFhYWFxnp6e0t/f3/rqq68eq94rfL4777wzb8OGDb5hYWFxXl5e9sWLF6c795lMppiUlJRkgIcffjg0OTnZG2DatGmZzqHR1157rdnmzZubGgwG6evra33//fcPOx//ww8/NL2tDno4a4PQioc3HAkJCXLHjh1VartmXSlPTThNvwGC95a3rOXIFOUiTp/WEijnAsOdO2tzodpccKWxotQaIcROKeV5C9Hv3r07PT4+/nRdxdQYbNmyxXvOnDnN165de/jSrRsGs9ksevbs2WHHjh0p10rtxt27dwfFx8eHV7SvOnWk3hVC9Khkf3chxLuXEV+d2fFzLkhJXGc1yVypA1LC+vXaIsMbN2qFNZ95Bt5+WyVRitJI3HDDDUX9+/c/51qQs6E7cOCA+4svvphxrSRRl1Kd8gfjgPaV7G8L/PWKornKdv9mRuj1dOzYMH6YyjXm7FmtsGZ+vra8y+rV8Oc/g1o0W1EalUmTJp1pTFeNd+zYseS2227Lv3TLa0NN/uR8gAsmodVXViscOuyBTq8nPl7Nj1KuEufyFTqdtpzLtGnaHKjBg9VcKEVRlGtQpYmUEKINEO6yySSE6FtB0wDgEeBAzYVWuw4elJQWS0La2AkKUldDKVfBoUPwr3/BTTdpRTUBbrmlbmNSFEVRrsileqTuBZ4HpOP2N8etPAHYHe2vCb/tKgKbJCpWT0U1LxSlxlitsGQJLF4MFgvk5mrzohpRV76iKEpDdam/5GuBdLRE6V1gIfBzuTYSKAC2Symv6FLLq2nXtjykEMTG1nUkSoO2bx/MnAn7HVc0DxsGTzyhkihFUZQGotJZrVLK3VLKJVLK94F/Am847rvelkop11xLSRRA4h4rQqfjung1rKfUAosF5s2Dv/5VS6Jat4YFC2DGDDA2iOWlFOWq0Ov1XU0mU0xkZGTswIEDI06fPl02hLBjxw7Pnj17RoWHh8eFhYXFTZ06taWzICXAqlWrmsbFxUW3b98+Njo6Omb8+PEh5Y9vNpvF9ddfH2UymWIWLVp00UV0u3fv3mHTpk0XVG6eN29e4NixYy96me3//vc/b4PB0PW9996r8NgFBQWiW7duHVyv2ps5c2YzDw+PLmfOnCl7rhWdxzWmvLw83ZgxY8JCQ0PjYmNjo7t3797hhx9+8LlYXFVht9sZN25caJs2beKioqJitmzZUmHl6kWLFvlHRUXFRERExD7yyCOtndtfeeWV4KioqBiTyRTTtWvXDjt37vQErYL7yJEjw68ktvqkypcHSSn/KaVMrM1grpbSUjh8xB29QU+nTiqRUmqBXg+7dmnf3323tshw9+51G5OiXIOcS8Ts378/yc/Pz+pcRLigoEAMHz484umnn85KT09PTExMTN62bVuT2bNnBwNs377dc8qUKW2WLVt2+ODBg0l79+5NjoiIKCl//K1bt3oDpKSkJI8fP/5sTcZutVqZNm1aSO/evS9aePL1118PGjp06FnXq/Y+/vjjgLi4uMLly5f7VfVcd999d7i/v781PT09MSkpad/SpUsPnzx58oq6vlevXu176NAhz/T09MQ333zzyIQJEy5IGLOysvTPPfdcyI8//ph24MCBpOzsbLd169YZAR544IEzaWlpySkpKcmTJ0/OmjRpUihA9+7dzSdOnHDfv39/g3gDvmgiJYTo6zqx3Hn/UrerE/aV2b8fbaJ5qAU/vwbxc1Tqg8JCrSo5aFflPf+8Vp38ySfB66ILxCuKUkU9e/YszMjIcAdYtGhRYEJCQsGIESPOgbaEzJtvvnl07ty5LQFeeumlFlOmTDnRuXPnYgCDwcC0adPOq1KekZFhuPfee9vu3bvX22QyxSQlJXmsW7fOGB0dHRMVFRUzatSocLPZfMHltHPnzg0MDw+P69ixY/TWrVublN/v9NJLLzW74447zgYFBV20SNSqVasC77zzzlzn/aSkJI+ioiL9zJkzM1atWhVQldclKSnJY9euXT5z587NcM75NZlMpXfdddcVVQ5ft26d3913331Gp9MxaNCgwnPnzhmOHDlyXr2g1NRUj/Dw8BLnIs6DBg06t3r1an+AgICAsu7BgoICvesCy0OGDMldsmTJRXsAryWV9Uj9CGwUQri73q/k5txf7/22qxDsdqKi9VRUsl9Rqu2nn+DOO+GFF7RCmwDh4RAXV6dhKUpDYbVa2bhxo3HYsGG5AElJSZ5dunQ5b3mW2NjYkqKiIl1OTo4uNTXVq0ePHhUu3+LUunVr64IFC44kJCQUpKSkJLdt27b0oYcearty5cqDaWlpyVarFWcPmNORI0fcZs2a1Wrr1q0p27dvT0lLS6vwU9Lhw4fdPvvsM/+nn376VEX7AYqLi8WxY8c8OnTo4FyKjaVLl/oPHz48Z/DgwQWHDx/2PHbs2CV7lX7//XfPmJiYoqrUorr11lvbmUymmPK3N954I7B82xMnTriFh4eXxdayZcvS8olUTExMyaFDhzxTU1PdLRYL69ev98/MzCzroXj55ZeDQ0ND455//vmQ+fPnH3Vu79GjR+HWrVsbxDyHyl71+9AmkjtrQ10zV+Rdyq4d+Uh0xHZUSZRyhXJz4bXX4IsvtPuBgVBQoOZBKQ1S2q9ZFyyCe6WiureotNekpKREZzKZYrKzs93at29fPGzYsHM1HYPT7t27PUNCQkqca8WNGzfuzPz585sBJ51tNm3a5NOzZ898Zw/MiBEjctLS0jzLH2vChAmhs2bNOl7ZVeFZWVkGo9F4Xm/VmjVrAtesWXNAr9dzyy23nF22bJn/s88+e0pcpM7cxbZfzIYNGw5V6wGXEBwcbPvPf/5zZNSoUe10Oh3dunUrOHz4cFlxxunTp5+aPn36qbfeeivg+eefb7lmzZp0gJYtW1qzs7MbRDXsiyZSjgnmrveX1Ho0V0niHhtCr6fzdWpYT7lMUsK338Krr2oVyt3dYcIErT6UKqehNFCXSnpqg3OOVH5+vq5///6Rs2bNajZjxoyTMTExxZs3bz5vWC05Odnd29vbHhAQYI+Kiiretm2bd69evcxXO2aAPXv2+IwdO7YdwNmzZw0bN270NRgM8p577sl1tvHx8bGXlpaWfaL/9ddfvY4cOeIxePDgKACLxSJCQkJKn3322VNBQUHW3Nzc8/645Obm6ps3b24NCAiw7du3z9tqtXKpXqlbb7213cGDBy9I/CZOnJg9ceLEM67bWrZsaUlPTy97ozxx4oR7WFjYBYW3x4wZkzdmzJg8gDlz5gRVlDyOHz8+Z+rUqWVzrMxms87T09N+QcNrUKPrkikthSNH3NDrdXTqpCqaK5fBbtcqkj/7rJZEde0KK1fCX/6ikihFqSVGo9E+b968owsWLGhusVh48MEHz2zfvt24du1aI2iTzx999NE2jz32WBbA9OnTs1577bWWe/bs8QCw2Wy88sorwZWdIz4+vjgjI8M9MTHRA2Dp0qWBffr0OW8pk759+xZu27bNmJWVpS8pKRGffvpphfN8MjIy9jpvQ4YMOfvvf//7qGsSBVpvjs1mE0VFRcJxvoApU6ZkOh938uTJPdnZ2W5paWnuN9xwQ+HOnTubHD161ACwadMm79LSUl379u1LY2NjSzp16lQ4efLkVs6rFlNTU91XrFhxQQ/ihg0bDqWkpCSXv5VPogCGDh2a+8EHHwTa7Xa+//57H6PRaKsokcrIyDAAnDp1Sr948eJmEyZMOAWwd+/esjfZlStX+oaFhZVN9k9OTvbo0KFDnSS5Na3KM/qFEN2BeCnlIpdtdwD/QqtsvkRK+WzNh1izjh4Fm8VOy1Z2jEbVI6VcBp0OQkPBx0erCTVsmFofT1Gugt69e5tNJpN54cKFAY8++mjOmjVrDkycOLHNpEmT3Ox2O6NGjTozffr0kwA9evQwz549+9jo0aPbmc1mnRCCm266qdIeNW9vb/nWW2+ljxo1qr3NZiM+Pr7oqaeeOm+OU1hYmGXatGmZPXv2jDYajba4uLhK52FdSt++ffO++eabJsOGDctfu3ZtwGeffbbfdf+QIUPOLlmyJODFF1/Mmj179rHBgwdH2u124ePjY1u+fPkhZ+/P8uXL0ydMmBAaFhYW5+npKf39/a2vvvrqFZUluvPOO/M2bNjgGxYWFufl5WVfvHhxunOfyWSKSUlJSQZ4+OGHQ5OTk70Bpk2blukcGn3ttdeabd68uanBYJC+vr7W999//7Dz8T/88EPT22677ar3cNYGIZ0TYy/VUIgNgF1KebvjfhsgBSgETgEdgAeklO/VUqxVkpCQIHfs2HHR/d9+Z+ehv2ZxfR/B8hUtr2JkyjUtIwNOnYLrrtPul5RAXh40a1anYSlKTRFC7JRSJrhu2717d3p8fPzpuoqpMdiyZYv3nDlzmq9du/bwpVs3DGazWfTs2bPDjh07Utzcro1pUrt37w6Kj48Pr2hfdT5GxwNbXO7fhVbx/DopZQzwDfDg5QZ5taSmFoNdEh6hKksrVWC3w4cfalfkTZ8O+Y5efg8PlUQpinLFbrjhhqL+/fufcy3I2dAdOHDA/cUXX8y4VpKoS6lONhEIZLvc/xOwSUqZ4bi/HnihpgKrLfsS8xA6Qfv2ai6LcgkHD2rLuyQlafe7dPmjtIGiKEoNmTRp0gXzkxqyjh07lnTs2PGC4qjXquokUrlAcwAhhAfQE3jJZb8E6n3VwUMH7QidnoiIhpEJK7XAYoH33tOKaVqtWs/T9OnQp09dR6YoiqLUM9VJpH4HHhBCfAcMBzyBr132t+X8Hqt6R0o4fswNoYfISJVIKRfx9NOwebP2/YgR8Pjj0OSixYsVRVGURqw6idQLaPOgfkWbG/WtlNJ1VvdtwLYajK3GnT4N5iJJU6MkOFglUspF3HknpKdrCwx37VrX0SiKoij1WJUTKSnlViFEF7S5UXnACuc+IUQgWpL1aY1HWIPS0yV2m53QMEll1WaVRmbHDkhOhrFjtfu9esHq1VCF5RYURVGUxq1a7xRSyjQgrYLtZ4Anayqo2rIv1ey4Yk8V4lTQlnKZNw/WrAEhICEBYmK0fSqJUpR64ejRo4YJEya02b17t3fTpk1tQUFBlttvvz13w4YNfhs3bjxQ1/EpSrXfLYQQTYEbgXaOTYfQhvnyL/6o+iE5MRchtLVklUZu0yZ4+WWtNpTBAPffD5GRdR2Voigu7HY7Q4cOjRgzZsyZzz///BDAzz//7LVmzRq/Og5NUcpUqxyzEOIB4BiwGnjFcVsNHBdC3F/dkwshBgshUoUQB4QQz1TSbqQQQgohEi7WpioO7bcidDpV+qAxO3sW/vY3mDxZS6Li4rQ6UePHQwOpaaIoDcXnn39uNBgM8umnny6rLt6rVy9zv379CgoLC/WDBw9u17Zt29ihQ4e2dS6N8tRTT7WMi4uLjoyMjB09enSYc3v37t07PPLII607duwYHR4eHvfVV181AbBarTz44IMhkZGRsVFRUTEvvvhiM4DNmzd7d+vWrUNsbGz0DTfcEHnkyBH1B0KpUHWWiBkKLETrgfo74CiuQyzwGLBQCHFSSvlZFY+nB+YDNwHHge1CiPVSyuRy7YzAE9TARPZjR3Sg0xEZqZaGabRefx2+/ho8PbVFhu+6Sy3voihVFRcXfdF9U6ee4K9/zQVgyRI/Xn314ktHJCbuq8rp9uzZ4xUfH1/hEiz79u3z+v333w+Fh4dbunbtavr222+b/OlPfyqYOnXqyTlz5pwAGDZsWNsVK1b4OhfUtVqtYu/evftWrlzpO3PmzFaDBw9O+/e//x189OhR9+Tk5CQ3Nzeys7P1JSUl4vHHH2+zYcOGA61atbIuWrTI/6mnnmq9evXq9KrErTQu1RnaexrYB/SQUha4bP9eCPEe8AswDahSIgV0Bw5IKQ8BCCFWAHcAyeXavQDMBqZWI9YLFBfD6dNuuLnpCAtT818aFSm1OVAAjz6qzY164glo3bpu41IU5bJ17NixsH379haA2NjYooMHD7oDfPnll8bXXnutRXFxsS43N9cQExNjRrtAilGjRp0FuP766wunTp3qDtqabw8//PApZ5Xt5s2b27Zv3+65f/9+r4EDB0aBNsQYHBx8wWK9igLVS6TigZnlkigApJT5QoglaD1VVdUabZjQ6TjQw7WB4yrBUCnlBiHERRMpIcSDOJanadOmTYVtjh4Fm81OaGs7np4qkWoU7HZYu1brgVqwAPR6CAyEV16p68gU5dpUxZ4k/vrX3LLeqSvQsWNH89q1a/0r2ufh4VG2zIBer8dqtYqioiIxZcqUsG3btiVHRERYJk+e3Kq4uLisy9nT01MCGAwGbDabuNh5pZQiIiLC/Pvvv6dc6XNQGr7qjGlc9JfOoUbXzhBC6IDXgCmXaiulXCilTJBSJgQHB1fY5sAhC9gkYeGgU0M5Dd/Ro/Dww/DSS7BzJ2zcWNcRKYpSTbfffnt+aWmpmDNnTpBz27Zt27z+97//VVght6ioSAfQokULa15enu6zzz6rMAlzNWjQoHNvv/12kMWidThlZ2frO3XqVJyTk2P47rvvfABKSkrEjh07PGvkSSkNTnUyit3AOCGET/kdQogmwDhHm6rKAEJd7oc4tjkZgTjgRyFEOtqSNOsvd8J50r5CBNCmveqNatBsNli2TJv79Ntv4O+vXZ03aFBdR6YoSjXpdDrWr19/8IcffmgaGhoaFxERETtt2rTWLVq0qHCYLSgoyHb33Xefio6Ojh0wYEBUfHx84aXO8eSTT54KCQkpNZlMsR06dIh55513Ajw9PeWKFSsOPvPMMyEdOnSIiY2NjblY8qYoQlZxEVYhxDBgDbAfmMcfc5mck80jgBFSynVVPJ4BrSbVILQEajswRkqZdJH2PwJPlaumfoGEhAS5Y8eFTR58MJPvNwj+/qI748YFViVE5Vpz4IC2yHCy41fzlltgyhTw9a3buBTlGiCE2CmlPO+D6u7du9Pj4+NP11VMilJf7N69Oyg+Pj68on3VqWy+VggxEW3i9+v8MZQngEJgYlWTKMfxrI7jfQ3ogXellElCiJnADinl+qoeqyrSD0mEumKvYdu9W0uimjfXShxcf31dR6QoiqI0cNWtbL5ACPEhWsmCto7NzoKcedU9uZTyC+CLctueu0jb/tU9/h+PhcxjBoReEBGhhvYalLy8P3qchg/XLs8cNgx8LhiBVhRFUZQad8mswjEEdwfa0N1pYJ2UcnVtB1aTTp2CYrPAz99KQIBKpBoEsxnefBPWrYOPPoJWrbR6UHffXdeRKYqiKI1IpVmFEMIf+BFt0rdAG857RQhxs5RyZ+2HVzPS00Ha7bRqVYpBraF27fv1V/jXvyAzU0uedu7UEilFURRFucoulVXMADoCn6PNZYoCHkarcN61dkOrOadO2ZF2SYsWOoS4VBUHpd7Kz4e5c7XaUKCtjff3v/+x0LCiKIqiXGWXSqRuB76SUg51bnCUIpgjhAiRUh6vzeBqSt5ZMyAxGlVv1DVrxw6YMQNOn9bWxBs/HsaO1RYcVhRFUZQ6cqk6UqGUmwyOtgSMAMJqJaJacOZ0PgiBt496071m+flpCw536qQtMnzffSqJUhRFUercpd6JPICcctvOuuy7JuSdKUYIDzy8VUXza4aUWi9UQoK2Tl5EBCxeDLGxapFhRVEUpd64knekGl0Spjbl5ZYgdODlra/rUJSqyMrSFhV+5BH4/vs/tnfsqJIoRWmEhBBd77jjDmfJHSwWC/7+/vEDBgyIqM3z6vX6riaTKSYyMjJ24MCBEadPny57Ezl48KDboEGD2oeFhcWFhobG3XvvvaHFxcVlk3CPHj1quO2229qFhobGxcbGRvfr1y9iz549F3RAFBQUiG7dunWwWq1l25YtW+YnhOi6a9eusmVpUlNT3SMjI2NdHzt58uRWzz33XPPqnK+6Pv7446bh4eFxbdq0iXv22WdbVNTmhRdeaBYZGRkbERERO3PmzGbO7UVFRaJjx47RHTp0iImIiIh98skna+SqoKrEVFmbivYVFxeLhISEDs6lgqqjKu9KU4QQ6503YDlaEvWi63bHrcoFOa+mgrwShNDhqVZKqt/sdli9Gu68E7ZuBaNR26YoSqPm5eVlT01N9SooKBAAn376adPmzZtX/x2vmjw8POwpKSnJ+/fvT/Lz87O++uqrwQB2u51hw4ZFDB06NPfIkSOJhw8fTiwsLNQ98cQTrZ37hw4dGtG3b9/8Y8eOJSYlJe2bNWtWRmZmplv5c7z++utBQ4cOPet6RfmKFSsCunTpUrB06dKAqsRZnfNVh9Vq5cknn2zzxRdfpKWlpSV98sknATt37jzvnXT79u2eS5cuDf7tt9/27du3L+mrr77yS0xM9ABtkegtW7akpqamJiclJSV///33Tb///vuLFvn7/PPPjSNHjgy/0pgqa3OxfZ6enrJfv37nFi9eXKXX3FVVJpl0dtzK61nBtnrZS1VYYEHoBT4+6oq9euvoUXjhBdi1S7s/YABMmwZBQZU/TlGUqyIujujaOG5iIvuq0u7GG2/MW716td+999579qOPPgoYOXJkztatW5sALFiwIODNN99sbrFYRJcuXQqXLl16xGAwcOONN7Y/ceKEe0lJie7hhx/Ofuqpp06npqa6DxkyJLJ79+4FO3bsaNK8efPSr7/++kCTJk0qff/q2bNn4Z49e7wAPvvsM6OHh4f9iSeeOANgMBh46623jrVr167TnDlzMjdu3OhjMBjk008/fcr5+F69epkrOu6qVasCV6xYcch5Py8vT7d9+/Ym3333XerQoUMj//Of/2Re6rX5/PPPjVU9X3X8+OOPPmFhYSUxMTGlACNGjMj5+OOP/bp27ZrlbLN3716vzp07FxiNRjtA796981esWOH3r3/9K1un0+Hr62sHKC0tFVarVVzplfNViamyNpXt+/Of/5z7zDPPtH7kkUfKT2mqVKU9UlJKXTVv9XLszFwEQqfHW82Rqp927NAWGd61CwIC4JVX4NVXVRKlKEqZe+65J2flypX+RUVFYt++fd69evUqBPjtt988P/7444AdO3akpKSkJOt0OvnWW28FAnzwwQfpSUlJ+37//ffkt99+u3lWVpYe4OjRo56PP/74yQMHDiT5+vrali5d6l/Zua1WKxs3bjQOGzYsF7TkIT4+vsi1TUBAgL1ly5alycnJHnv27Llgf0WKi4vFsWPHPDp06FDq3Pbhhx/69e/fP69Tp04l/v7+1s2bN3tf6jhVPR9A165dO5hMppjyt7Vr1xrLtz127Jh769aty2ILCQkpzcjIOG+dteuuu87866+/GrOysvT5+fm6b7/91vfYsWNlbaxWKyaTKaZ58+bx/fr1Ozdw4MALFpLu1KmTyWQyxUyYMCHsu+++83PG9MknnzS9nJgqa1PZvm7dupn37NlT7WUxGsVlT2Yz6HV6vL1Vj1S9FBenrY8XHw+TJ0PTC/7vKIpSx6rac1RbevToYT5+/LjHokWLAm688cayJcm++uorY2Jiond8fHw0QHFxsa5Zs2ZWgNmzZzffsGGDH0BWVpZbUlKSZ0hIiKV169Yl119/vRmgc+fORenp6RXOJSopKdGZTKaY7Oxst/bt2xcPGzbsXE0+p6ysLIPRaLS6blu1alXA448/fhJg5MiROcuWLQvo06dP0cV6cqrbw7Nz587Uy423Il26dCl+4oknsgYNGhTl5eVlj42NLdLr/+hTMRgMpKSkJJ8+fVp/6623tt++fbtnt27dil2PsWfPnhTQetbee++9wE8++SS9JmOsKoPBgJubmzx79qzO39+/yvNKGkUiVVws0OlVIlVvlJbCBx9oc6F8fMDTE5YtgyZN6joyRVHqscGDB+c+//zzod98803qyZMnDQBSSjFq1Kgz8+fPz3Bt+/nnnxv/97//GXfs2JFiNBrt3bt372A2m3UA7u7uZcN4er1eOreX55wjlZ+fr+vfv3/krFmzms2YMeNkXFycee3atef1YuXk5OhOnDjhHhMTU5KVlWUov78iPj4+9tLS0rJzZ2dn63/55Rdjamqq18SJE7HZbEIIIe12+/HmzZtb8/Lyzhv1ycnJ0bdt27akTZs2pVU5H2g9UoWFhReMHs2aNevYsGHD8l23hYaGntfbc/z48fN6c5yefPLJ008++eRpgIkTJ7YOCQm5oE1QUJCtT58++Z999plv+USqOqoSU2VtLvV4i8UivL29qzVNqcGPdVlLSikt1SN0Ory8VCJV5/bsgTFjYP58mDfvj+0qiVIU5RIeeeSR00899VRm9+7dy+b/DB48+Nznn3/un5GRYQAtGUlLS3PPzc3V+/r62oxGo33Xrl2eu3fvvuyVzI1Go33evHlHFyxY0NxisTB06ND84uJi3RtvvBEI2vDVhAkTQkeNGnXaaDTab7/99vzS0lIxZ86csvkJ27Zt8/rqq6/O+0MXHBxss9lsoqioSAAsW7bMf/jw4TmZmZl7MzIy9mZlZe0JCQkp/frrr5v4+vramzVrZlm/fr3R+Tx//PFH34EDBxZU9Xyg9UilpKQkl7+VT6IA+vXrV5ienu6ZkpLiXlxcLNasWRMwcuTI3PLtnK/9/v373Tds2OD3wAMP5ABkZmYanFc6FhQUiI0bNzaNjo6+aBJ122235V+qN6oqMVXWprJ9WVlZej8/P6uHh4dKpFzlF57DYnVH6HSqR6ouFRVp857uv19b/DAsDIYMqeuoFEW5hrRv394yY8aMk67bunbtWjxjxoyMQYMGRUVFRcUMHDgw6tixY24jR47Ms1qtol27drFTp05tHR8ff8HcnOro3bu32WQymRcuXBig0+lYu3btgTVr1viHhYXFtW3bNs7Dw8M+b968DACdTsf69esP/vDDD01DQ0PjIiIiYqdNm9a6devWF1xp2Ldv37xvvvmmCcDq1asDRowYcdZ1/x133HF2+fLlAQBLliw5/OKLL7Y0mUwx/fr16zBt2rTM2NjYkuqcrzrc3Nz497//fXTw4MFRkZGRscOGDctJSEgoBujXr19Eenq6G8DQoUPbt2/fPva2226L+O9//3s0KCjIBnDs2DG3Pn36dIiKiorp3LlzzIABA86NHj06r/x5nHOkyt8qmiNVlZgqa1PZvi+//LKp67BxVQkp6+WFdpctISFB7tixo+z+0cOHGXmLhRK3NnzyCURGqhoIV90vv8CLL8KJE1odqL/+VVvixd390o9VFOWqEELslFImuG7bvXt3enx8/Om6iqkx2LJli/ecOXOar1279nBdx9LY3Xzzze3nzJlzvFOnTiXl9+3evTsoPj4+vKLHNfg5UkUFZix2L0CVP6gT+/fDxIna91FR8Pzz0KFD3cakKIpST9xwww1FO3bsOGe1WjGoZa/qTHFxsRg6dGhuRUnUpTT4n1rRuTwstqboEfj4NPiRzPonMhLuuANCQuCee9T6eIqiKOVMmjTpTF3H0Nh5enrKiRMnXtbPodrvakKIcOBGoDnwgZQyXQjhDrQAsqSUF8zWr0vFBYVYrW7oQSVSV8OZM9pcqHvu0dbFA/j73+s2JkVRFEWpJdVKpIQQs4HJgB6tivnPQDrgCSQDM4D/1miEV6jgXCHodLi7S/R6NbRXa6SEzz+H//wHzp2DkyfhnXe0BYcVRVEUpYGqcheNEOIhYCowH7gZKHuHlFKeA9YDt9d0gFeq6GwxQujw8JDVLlymVFFmJjz2GPzzn1oS1auXNrlcvd6KoihKA1edHqkJwKdSyklCiMAK9u8BJtZMWDWn4JwVodPh6dmwrk6sF5yLDL/xhlY+vmlTmDIFbrlFJVGKoihKo1CdRCoKeLOS/aeAerc4mrnAjhBCJVK1ITcX3npLS6IGDdIWGQ6o9sLZiqIoinLNqk4iVQxUVhk2DMi9omhqQYnZjtAJPFX5qJphtWq9TXq9ljQ9+6z2/cCBdR2ZoiiKolx11bmM7VdgeEU7hBCewD3ATzURVE0qKtYhdHrVI1UTUlNh7FhtnTynm25SSZSiKIrSaFWnR+pV4GshxDLgXce2FkKIPwH/BEKAMTUc3xUrKRQInQ4Pjyov5KyUV1ICixbB0qXavCirFe6+W+uJUhSl0Th8+LC32WyusWJwXl5e1rZt2xbV1PEARo0aFf7999/7BgYGWvfv359U1cedPn1av3jx4oBnnnnmVEX7J0+e3KpJkya2mTNnZlfleNVtr1y7qtwjJaX8DngE+DPwnWPzMuALIB4YL6X8ucYjvELmEp2abH4lfv8dRo+G99/XShw4v1dJlKI0Omaz2eDj42OtqVt1k7LPP//cOHLkyPDK2tx3332n169fv7+6z+3MmTP6d955p1l1H6co1apQKaVcCLQFJqFNPH8beAqIkFK+X9PBXSmr1UapRa8mm1+O0lJ45RV44AE4ehTattXqQk2ZAt7edR2doihKhYYMGVIQHBxsrazNuXPndP3794/o0KFDTGRkZOyiRYv8p0yZEnLs2DEPk8kU89BDD4UATJs2rUV4eHhc165dO+zfv9/jUueurP2CBQsCOnbsGG0ymWLGjBkTZrVamTBhQuuXX3452Nlm8uTJrZ577rnml/vclbpR7S5aKWUW8HotxFLjLCWlWGw6QCVS1WYwaHOi9HoYNw7uv18tMqwoSp3o1KmTqbS0VFdUVKTLy8szmEymGIAXX3zx+MiRI89V93hr1qxp2qJFC8uPP/54ALTeqL59+xbedtttXikpKckAmzdv9v70008D9u7dm2yxWLjuuutiOnfufNFhyMra//bbb54ff/xxwI4dO1I8PDzkX/7ylzZvvfVW4N13350zadKkNtOnTz8FsG7dOv+vv/467XJeI6XuNOiFz8xmM6V2d0Dg4aESqUvKywOLBYKCQKfTFhguLtYWG1YURakje/bsSQFtaO+9994L/OSTT9Kv5HhdunQx/+1vfwt95JFHWt9xxx15gwcPLjh9+vR58xU2btzY5JZbbsk1Go12gJtvvjm3smNW1v6rr74yJiYmesfHx0cDFBcX65o1a2adOHHimTNnzhjS09PdTpw4YfD19bVFRERYruS5KVdflRMpIcQPVWgmpZSDriCeGlVsLsBiMQBS9UhVRkr4/nttKM9kgrlztRIHbdrUdWSKoig1rlOnTiW//fZb8ieffOL797//vfV33313bvz48bW2cLCUUowaNerM/PnzM8rvGzp06Nnly5f7Z2VluY0YMSKntmJQak915ki1Q5sf5XqLBPoC/YE4R5t6w1JSgtWmDUepOlIXcfo0TJ0KzzwDOTlaD1RRjV5EoyiKUiNuu+22/CvtjQJIT093MxqN9gkTJuRMnjw56/fff/f29fW1FRYWlr0nDhw4sOCLL77wKygoEGfPntV9++23fpUds7L2gwcPPvf555/7Z2RkGACys7P1aWlp7gB/+ctfcj755JOAzz//3P+ee+45e6XPTbn6qtwjJaUMr2i7EMIDbSHje4F+NRNWzbCWlGCVWiLl41OtefUNn5Tw2Wfw2mtQUKBNIH/iCRg+XBvWUxRFKcfLy8taWFhYo+UPqtLOOUeq/PaK5kjdfvvtbX/55Rfj2bNnDc2bN+/0zDPPZD755JOnXdvs3LnTa/r06SE6nQ6DwSAXLFhwpEWLFrauXbsWREZGxg4cODDv7bffPj58+PCcuLi42MDAQEunTp0KnY/v169fxJIlS46Eh4eXDcPdcMMNRRdr37Vr1+IZM2ZkDBo0KMput+Pm5ibnzZt3NCoqqjQhIaG4sLBQ17x589KwsDBLZedQ6ichZc0MeTnqSxmklKNr5ICXKSEhQe7YsQOAlN928tz0ElKzEpg2zcyYMb51GVr9YbfDpEmwdat2//rr4W9/g+bqYhFFaayEEDullAmu23bv3p0eHx9/+mKPUZTGYvfu3UHx8fHhFe2rycnmW4CXa/B4V8xSYsZqdQPAy0stoltGp9PmQiUlwVNPweDBapFhRVEURbkMNTmG0xao1vXxQojBQohUIcQBIcQzFeyfLIRIFkLsEUJ8L4QIq87xbRYrFqsWkrd3I08UDh2C7dv/uP/AA7B6NQwZopIoRVEURblM1blq72KXcAUANwKPAz9W43h6YD5wE3Ac2C6EWC+lTHZptgtIkFIWCSEeAV4B/q+q5ygtLqbUrhWqbbQ9UhYLLFmiFdM0GuHjj6FpU60mVEBAXUenKEr9Zrfb7UKn06nLnpVGy263C+Ci68xVZ2gvHbjYfyYBpKIlU1XVHTggpTwEIIRYAdwBlCVSUsqNLu1/Af5SjeNjs5Q07h6p5GR44QXY71gtoV8/NZFcUZTqSDx16lRMcHBwnkqmlMbIbreLU6dO+QKJF2tTnURqJhcmUhLIAdKA76SU1VkZuDVwzOX+caBHJe3vB76saIcQ4kHgQYA2LrWPrFY7pVbtKTZp0ojWhispgbffhuXLtYnlrVvDjBnQrVtdR6YoyjXEarU+kJWVtTgrKyuOmp0KoijXCjuQaLVaH7hYg+qUP/hHTUR0OYQQfwESuEh5BccagAtBu2rPud1mKcbqSKQaVY/UU0/Bzz9rvU933w0PPwxeXnUdlaIo15iuXbueBIbWdRyKUp9VKZESQjQBdgOvSyn/W0PnzgBCXe6HOLaVP/eNwN+AflLKkuqcwG61UVpqAEMjqyN1zz1w8iT8/e8QF1fX0SiKoihKg1Wl7EJKWQAEAgU1eO7tQKQQoq0Qwh24C1jv2kAI0Rl4GxgqpTxZ3RPYbJISR/kDb+8GnEht2QKLFv1xv3t3+OgjlUQpiqIoSi2rzhypX9CG1xbXxImllFYhxETga0APvCulTBJCzAR2SCnXA68CTYDVQrtE/6iUssrdzCUlNux2HUKAh0cDTKRyc+Hf/4YvHVPHeveGmBjtezWpXFEURVFqXXUSqWeAH4QQ24D3ZQ2URJdSfgF8UW7bcy7f33glxzebtRA9PSU6XQOaIyUlfPuttshwbi54eMAjj2hFNhVFURRFuWoqTaQctaNOSSnNwGvAWbQeqVeEEAeB8qvbSinloFqJ9DIUFUuE0BKpBuPkSZg1CzZt0u537apdkRcaWvnjFEVRFEWpcZfqkTqMVrvpI6AdWrmDo4599X5httJSPSAaViL19ttaEuXjo62XN2yYqkyuKIqiKHXkUomUcNyQUobXejQ1zFKiB9EAEim7/Y85TxMnatXKJ06EZs3qNi5FURRFaeQa9IzkkhIdgmt4aM9uhw8+gPvv15InAH9/mDlTJVGKoiiKUg9UZ7L5NaekROtQ8/C4BhOpgwe1hCkpSbu/eTMMHFi3MSmKoiiKcp6qJFJ9hBDVqYC+9AriqVHFJXq41iabWyzw3nvw7rtgtWo9T9OnQ58+dR2ZoiiKoijlVCVBKlvH7hIE2mT0epNIXXOTzZOT4Z//1HqjAEaMgMcfhyZN6jYuRVEURVEqVJVEaiFaMc5rjsWiLVR8zSRSaWlaEhUSoi3v0rVrXUekKIqiKEolqpJIbZZSfljrkdSCkhKDY2ivriOpxOnTEBSkfX/HHdpw3m231fOgFUVRFEWBBn7VXmmpjno72bygAF58UasDdfy4tk0I+POfVRKlKIqiKNeIBp1IlZRqT6/eDe1t2gSjRsGnn2o9UImJdR2RoiiKoiiXoWGXP7AYEELg41NPKn+fPQuvvgrffKPdj4uD556Ddu3qNi5FURRFUS5LpYmUlPKa7rGylBpACLy86kEi9csv8Le/QV6eNnQ3YQLcddcfFcsVRVEURbnmNOgeKYtVK39QLxKpZs2gqAi6d9cSqtat6zoiRVEURVGuUINOpEotBoQAb+86SKTsdvjpJ7jhBm0Sebt2sGQJREaqRYYVRVEUpYFo0ONKpRY36qRH6uhRePhhePJJ+PrrP7ZHRakkSlEURVEakIbdI2XV6kj5+FylfNFm0xYZfustKC3VFhhWpQwURVEUpcFq2ImUxQ2hF1dnaG//fnjhBW2ZF4BbboEpU8DXt/bPrSiKoihKnWiwiZSUEovVAPqrMEdq2zZtTTybDZo31yaTX3997Z5TURRFUZQ612ATKZvNqk0297gKdaSuu05bH697d5g4EXx8avd8iqIoiqLUCw02kbJbbVisbngI8Pau4TlSZjO8/z785S9gNIKHhzY3Ss2HUhRFUZRGpcEmUlarlVKrAQ9qOJH69Vf4178gMxNycrRhPFBJlKIoiqI0Qg02kSop1r66u0sMhhpIpPLz4b//hXXrtPtRUTBixJUfV1EURVGUa1aDTaQKC+1ADS1Y/OOPMGsWnD4Nbm4wfjyMHQuGBvvyKYqiKIpSBQ02Eyg2awnUFSdSaWnw1FPa9506aYsMh4df2TEVRVEURWkQGmwiVWTWrtS74kQqKgruvBPCwmDUKLXIsKIoiqIoZRpsVlBs1ob2PDyq+cDsbG1plz17/tj29NPwf/+nkihFURRFUc7TgHuktJ4oD48q9kjZ7fDJJ/D661BUBHl58O67tRihoiiKoijXuoabSBVVY47U0aPa8i67dmn3Bw6EadNqMTpFURRFURqCBptImc3a10oTKZsNli+Ht9/WFhkOCIBnntESKUVRFEVRlEtosIlUcbGWQHl5VZJInTsHS5ZoSdRtt8HkydC06VWKUFEURVGUa12DTaTMZVftldtRWqpNGjcYwN8f/v53rVGvXlc/SEVRFEVRrmkN9jI0c1EFk8337IExY2DZsj+2DRigkihFURRFUS5Lw02kil3qSBUVwauvwv33Q3o6fPutNj9KURRFURTlCtRpIiWEGCyESBVCHBBCPFPBfg8hxErH/m1CiPCqHrvYMdm8SXa6VgNq5UoQAu67D95/H/T6GnoWiqIoiqI0VnWWSAkh9MB8YAgQA4wWQsSUa3Y/cFZKGQH8B5hd1eObC6x45ecT8PEyOHECOnTQrtCbMAHc3WvqaSiKoiiK0ojVZY9Ud+CAlPKQlLIUWAHcUa7NHcASx/cfA4OEEKIqBy+26NHZbXi522DiRO3qvKioGgteURRFURSlLq/aaw0cc7l/HOhxsTZSSqsQIg8IBE67NhJCPAg8CNCmTRsAunYzUnSqgFZjnoShYbXyBBRFURRFadwaRPkDKeVCYCFAQkKCBPjLWHf+MlYlUIqiKIqi1J66TKQygFCX+yGObRW1OS6EMAC+wJnKDrpz587TQogjjrtBlOu9aqTU66BRr4N6DZzU66BxfR3UJ09FuQx1mUhtByKFEG3REqa7gDHl2qwH/gr8DPwZ+EFKWenieVLKYOf3QogdUsqEGo36GqReB416HdRr4KReB416HRTlytVZIuWY8zQR+BrQA+9KKZOEEDOBHVLK9cA7wDIhxAEgBy3ZUhRFURRFqRfqdI6UlPIL4Ity255z+b4Y/r+9u4+WqyrvOP79KYIGAyyMduFboy3RKOICsUVtS3yBSmgTRRQUhLDA14WKupDlSxeXSlGIaEVF3hYNQUHAF0i1KgoEDCUoBo0RBaIJAaEFESMYQiA8/ePZQ06GuZlz5849w53+PmudNTPn7DnnmX3PvfPcs/fZmzc3HZeZmZlZHUM7snlx5qADeJxwPSTXg+ugxfWQXA9m46QuXY7MzMzMbBTDfkXKzMzMbMI4kTIzMzPr0VAkUhM5+fFkUqMePiTpRknLJV0uaejGjelWB5Vyb5IUkoby1u869SDpLeV8+KWk85uOsQk1fieeK+lKSTeU34vZg4hzIkk6R9JdklaMsl2STi11tFzS7k3HaDaZTfpEaqInP54satbDDcAeEbErOXfhyc1GObFq1gGSpgIfAK5rNsJm1KkHSTsDHwVeFREvBo5uOs6JVvN8+ARwUUTsRg6vclqzUTZiAfD6LWzfF9i5LO8EvtxATGZDY9InUkzw5MeTSNd6iIgrI2JdebmUHE1+mNQ5FwA+SSbT65sMrkF16uEdwJci4l6AiLir4RibUKceAtiuPN8euKPB+BoREVeT4/CNZi6wMNJSYAdJOzUTndnkNwyJVKfJj581WpmIeBhoTX48TOrUQ9URwHcnNKLmda2D0mzxnIj4TpOBNazOuTADmCHpGklLJW3pisVkVaceRoBDJN1Ojmn3vmZCe1wZ698OM6sYikmLbWwkHQLsAew16FiaJOkJwGeBeQMO5fFgK7IpZxZ5ZfJqSS+JiD8OMqgBeCuwICJOkfQKciaFXSLikUEHZmaTwzBckRrL5MfUnfx4EqpTD0h6HfBxYE5EPNhQbE3pVgdTgV2AxZJWA3sCi4aww3mdc+F2YFFEPBQRq4CbycRqmNSphyOAiwAi4lrgyeREvv+f1PrbYWadDUMi9ejkx5K2JjuMLmor05r8GGpOfjwJda0HSbsBZ5BJ1DD2idliHUTE2oiYFhHTI2I62U9sTkRcP5hwJ0yd34lLyKtRSJpGNvX9tsEYm1CnHtYArwWQNJNMpO5uNMrBWwQcWu7e2xNYGxF3Djoos8li0jftefLjVLMe5gNPBS4ufe3XRMScgQXdZzXrYOjVrIfvA/tIuhHYCBwTEUN1lbZmPXwYOEvSB8mO5/OG7Z8sSReQSfO00hfsOOBJABFxOtk3bDawElgHHD6YSM0mJ08RY2ZmZtajYWjaMzMzMxsIJ1JmZmZmPXIiZWZmZtYjJ1JmZmZmPXIiZWZmZtYjJ1LWOEkjkkLS9EHH0qSxfm5J80r5WRMamJmZ9cyJlHUlaVb5Qh9t2XPQMdYlaXqH+NdJWiHpOElPaTieWSXB2qHJ49YlaXFbXT0k6Q5JF0raZZz7foOkkT6FamY2EJN+QE5r1AXk4H3tVjYdSB/8AFhYnj8dOJCcwPaVwD9O0DFPAD4NVKfmmUUOkLgA+GNb+fOArwEbJiieuh4EjizPnwK8jBy0cbakPSLiph73+wZyxoGR8QZoZjYoTqRsLJZFxFcGHUSf3Fz9LJK+QE4pso+kl0fET/p9wIh4GHh4DOU3kqOOD9rDbT/3s8qI6J8HjgLeN5iwzMwGz0171heS/kbSAkk3l6ay+yRdI+mNNd+/o6TPSfqNpPWS7pH0U0nHdCh7oKQl5RjrJF0n6YDxxF+SnMvLy7+uHOtIScskPSBpraTLJP1dh5j2k3SVpN+XsmskfVPSjEqZzfpISVpAXo0CWFVpPhsp2zfrIyVp3/L6/Z0+g6RrJd0t6UmVdTtLOk/SnZI2SFotab6kbXuurNSqq80mOq57HkhaTJn/sq3pcF6lzE6SvlzqckNpUjxT0jPGGbuZWd/4ipSNxRTlBLdVD0bEfcAbgRcCFwG3Ak8jvyi/KengiDi/y74vBv4BOB1YTjYhzSSbvua3Ckk6Afg48D3gX4BHyrEvlnRURHxpHJ+vlRT8vhzrJOAjwI+BjwFTgXcCV0qaGxH/VcrtRU78ugL4FNlE90zgdWRSdvMoxzsD2K7E/8HWccvn7+Qy4H+AQ4FTqxsk7QzsCZwaEQ+VdS8DrijxnAH8Dngp8H7gVZL2apXtwV+Vxz+0ra97Hvwb+Y/c3wNvr7z/v0vszwWuBbYm58r8DVmX7wFeXZoU1/YYu5lZ/0SEFy9bXMhkJkZZvlbKbNvhfVOAm4Ab29aPlPdOL6+3L69P6xLH7qXciR22XQL8CZjaZR/Tyz7OBqaVZSbZfymAVcA2wAvIJG0JsHXl/c8kE5PVwBPLus+W9z6jy7E3+9yjratsm1e2zaqsm1/Wvait7CfL+t0r634O/Lq9TshkpzVBb7ef/WLg/kpdPYfs27S67GN2W/mxnAcL8k9Qx+NeCtwFPLtt/R5k8+jIoH8vvHjx4iUi3LRnY3ImsHfbcgJARPy5VUjSFElPI79ArwBmStpuC/t9gOzQ/Lfa8tAAB5Nf3udKmlZdyCtCU4FX1PwsRwB3l+VG8irX1cA+EfEgMBcQcHJEPNrZOyLuAP4D+Etgt7K6dWXkTZIm+irvueXx0NYKSQIOAVZExLKy7iXArsD5wDZtdbUE+DOwT81jbsumuloDfIu8UnRYlKtyLeM8D1rv2x74J/Jnur4t9tXkzQ11Yzczm1Bu2rOxuCUifthpQ+m3cgKZgHTqw7IDecXoMSJig6Sjyc7Lq0pH5iuASyLi8krRmWRy8+stxPgXXT5Dy6XAF8nEbD2wMiL+t7L9eeXxlx3e21r3fOD6sp+5wGnASZKWkE2PF0TE3TXjqSUiVkhaBhws6WMR8QjZJDqdbIZsmVkejy9LJ3Xraj3wz+X5jmQStzcd+liO5zyoeEHZ9xFl6eS33YI2M2uCEykbt3JF5DLyy/vzZHKxlrzj7HDgbXS5sSEiTpd0KbAfsBdwAHCUpAsj4qDWocjEZ19Gv5utU+LTye2jJYVjFRH3SHo52d9nbzKx+RxwvKTZEXFtP45TsRD4d+A1wA/JxGYjUL2zTuXxFDKp6+TemsfbWK0rSV8Hvg2cKWlZRCwv68d9HrTF/hU2XYFr90DN2M3MJpQTKeuHXclOzP8aEcdVN0g6svNbHisi7iT7Lp0t6YnkOEpvlXRK5HAEtwCvB9ZExK/6Fn1nrSseLyY7Ole9qK0MkUMVLC4LknYFfgp8gkwORxM9xHY+2VfqUEnXkEnnD0r9tdxSHjf2K2FsiYhHJH2AbBL9DJua2cZ6Hoz22VeWbVv3O3Yzs35zHynrh9bVIVVXKke+7jr8QelLM6W6riQmrbvXdiyP55XHE0ui1b6fuk1VdSwiv8yPaRtOYCfy6sqtwA1lXfudjJDNjw+wKfbR3F8eu5V7VGku/C6wP9lvbDsee+XmBvIuwndLen77PiRtJan2MTvEcAuZ0O1dGQ5irOfB/WX7ZnFExD3kwK/7q8Oo+UpP7zV2M7N+8hUp64dfkU1qHykJ0U3ADOBdwC/IkbC3ZAZwlaRvkV/+95LNQ+8h76L7EUBE/KSMsTQC/EzSxcAdwE7lGLPJTtDjFhE3SZpP9ju6WtKFbBr+4KnAwSXZgxyg8tlks9at5NANB5byCx+z880tLY8nSfoq2R9pRUSs6PK+c4E5ZNPdWvKuxWr8IentZF+z5ZLOIX9GU8hhBPYHPkreOderE8lO7scDr2Xs58FSckDP0yR9B3gIuC4iVpE/+yVk3S8kE8MnkP3S5pL1OjKO2M3M+sKJlI1bRGyUtB/ZzHMYeZfXivL8pXRPpG4DzgFeTd5avw055tFZwEkRsa5yrOMlXU+OhXR0OdZd5XgdB6rsVUQcK2kl8F5yapcNwHXA2yLiR5Wi55FDFRxGTjfzJ7LZ64CI+EaXY1wj6Vjg3eTn3YpMTLolUt8mx3DaETg7ItZ32PfPJO1GJkxzyjHuI+98W8CmQTV7UpLNi4CDyphUV43xPLiAvPPxIODNZKJ0OLAqIm4r42AdSyZOh5BJ5m3Af5LjVJmZDZwieumiYWZmZmbuI2VmZmbWIydSZmZmZj1yImVmZmbWIydSZmZmZj1yImVmZmbWIydSZmZmZj1yImVmZmbWIydSZmZmZj1yImVmZmbWo/8DGY3sOmAa6a0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d5.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d5)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 51\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d5 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    decoding_d5, time_mwpm = do_new_decoding(x_test_d5, 5, 0)\n",
    "    decoding_d5['combine'] = decoding_d5[[0, 1]].values.tolist()\n",
    "    decoding_d5['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d5 = np.array(decoding_d5[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb.transform(decoding_d5)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d5 = lookup_decoder(5)\n",
    "    \n",
    "    lookup_d5 = train_plut(lookup_d5, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d5 = test_plut(lookup_d5, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d5)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d5, mlb)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d5, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d5 = compile_FFNN_cv_model_DepthFive(5)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    history = model_d5.fit(\n",
    "    inputs_train,\n",
    "    targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs = 800)\n",
    "    \n",
    "   # Generate generalization metrics\n",
    "    scores = model_d5.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d5 = model_d5.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d5.copy() #change here\n",
    "    pred[pred>=.4]=1 \n",
    "    pred[pred<.4]=0\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d5.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d5[:, i]) \n",
    "        aucs_classes[mlb.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"##############################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "    \n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 5 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7 Training, Testing, and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data sets from CSV\n",
    "\n",
    "#### Data sets for D7:\n",
    "* Original:\n",
    "    - \"depth7_all_combos.csv\"\n",
    "* Randomly-sampled, with replacement (version 2):\n",
    "    - \"v2samples-d7-1000.csv\"\n",
    "    - \"v2samples-d7-10000.csv\"\n",
    "    - \"v2samples-d7-500000.csv\"\n",
    "* Randomly-sampled, without replacement (version 3):\n",
    "    - \"v3samples-d7-1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported and formatted in 779.2631597518921 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainData_d7 = pd.read_csv(\"SAMPLES/v2samples-d7-100000.csv\")\n",
    "trainData_d7 = trainData_d7.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "#These four lines remove duplicates\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].astype(str)\n",
    "trainData_d7 = trainData_d7.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "testData_d7_MWPM = graph_with_errs_d7(trainData_d7)\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb_d7 = MultiLabelBinarizer()\n",
    "mlb_d7.fit(trainData_d7['Labels'])\n",
    "df = pd.DataFrame(mlb_d7.transform(trainData_d7['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d7 = trainData_d7.drop(['Labels'], axis=1)\n",
    "trainData_d7 = pd.concat([df[\"Labels\"],testData_d7_MWPM, trainData_d7], axis=1, ignore_index=True)\n",
    "trainData_d7.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\", \"Z1\", \"X2\", \"X3\", \"Z4\", \"X5\", \"X6\", \"Z7\", \"X8\", \"Z9\", \"Z10\", \"X11\", \"Z12\", \"X13\", \"Z14\", \"X15\", \"Z16\", \"Z17\", \"X18\", \"Z19\",\"X20\", \"Z21\", \"X22\", \"Z23\", \"Z24\", \"X25\", \"Z26\", \"X27\", \"Z28\", \"X29\", \"Z30\", \"Z31\", \"X32\", \"Z33\", \"X34\", \"Z35\", \"X36\", \"Z37\", \"Z38\", \"X39\", \"X40\", \"Z41\", \"X42\", \"X43\", \"Z44\", \"X45\", \"X46\", \"Z47\"]\n",
    "\n",
    "y_d7 = trainData_d7[\"Labels\"]\n",
    "x_d7 = trainData_d7.drop([\"Labels\"], axis=1)\n",
    "\n",
    "x_d7 = x_d7.replace([-1], 0)\n",
    "print(\"Data imported and formatted in %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 562 samples, validate on 188 samples\n",
      "Epoch 1/150\n",
      "562/562 [==============================] - 0s 304us/step - loss: 0.6764 - accuracy: 0.6586 - val_loss: 0.6628 - val_accuracy: 0.7352\n",
      "Epoch 2/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.6487 - accuracy: 0.7747 - val_loss: 0.6266 - val_accuracy: 0.8182\n",
      "Epoch 3/150\n",
      "562/562 [==============================] - 0s 107us/step - loss: 0.5978 - accuracy: 0.8478 - val_loss: 0.5493 - val_accuracy: 0.8808\n",
      "Epoch 4/150\n",
      "562/562 [==============================] - 0s 107us/step - loss: 0.4816 - accuracy: 0.9032 - val_loss: 0.3786 - val_accuracy: 0.9355\n",
      "Epoch 5/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.2973 - accuracy: 0.9493 - val_loss: 0.2233 - val_accuracy: 0.9543\n",
      "Epoch 6/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.2050 - accuracy: 0.9543 - val_loss: 0.1916 - val_accuracy: 0.9543\n",
      "Epoch 7/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1899 - accuracy: 0.9543 - val_loss: 0.1871 - val_accuracy: 0.9543\n",
      "Epoch 8/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1868 - accuracy: 0.9543 - val_loss: 0.1855 - val_accuracy: 0.9543\n",
      "Epoch 9/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1855 - accuracy: 0.9543 - val_loss: 0.1847 - val_accuracy: 0.9543\n",
      "Epoch 10/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1847 - accuracy: 0.9543 - val_loss: 0.1842 - val_accuracy: 0.9543\n",
      "Epoch 11/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1842 - accuracy: 0.9543 - val_loss: 0.1839 - val_accuracy: 0.9543\n",
      "Epoch 12/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1839 - accuracy: 0.9543 - val_loss: 0.1838 - val_accuracy: 0.9543\n",
      "Epoch 13/150\n",
      "562/562 [==============================] - 0s 123us/step - loss: 0.1837 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 14/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1836 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 15/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1835 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 16/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1834 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 17/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1834 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 18/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1833 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 19/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1833 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 20/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1833 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 21/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1833 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 22/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1833 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 23/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 24/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 25/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 26/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 27/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 28/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 29/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1837 - val_accuracy: 0.9543\n",
      "Epoch 30/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1831 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 31/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1832 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 32/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1831 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 33/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1831 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 34/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1831 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 35/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1831 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 36/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1831 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 37/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 38/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1836 - val_accuracy: 0.9543\n",
      "Epoch 39/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 40/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 41/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 42/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 43/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 44/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 45/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 46/150\n",
      "562/562 [==============================] - 0s 125us/step - loss: 0.1830 - accuracy: 0.9543 - val_loss: 0.1835 - val_accuracy: 0.9543\n",
      "Epoch 47/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 48/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 49/150\n",
      "562/562 [==============================] - 0s 132us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 50/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 51/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 52/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 53/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 54/150\n",
      "562/562 [==============================] - 0s 126us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1834 - val_accuracy: 0.9543\n",
      "Epoch 55/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1829 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 56/150\n",
      "562/562 [==============================] - 0s 123us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 58/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 59/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 60/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 61/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 62/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 63/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1828 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 64/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 65/150\n",
      "562/562 [==============================] - 0s 148us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1833 - val_accuracy: 0.9543\n",
      "Epoch 66/150\n",
      "562/562 [==============================] - 0s 169us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 67/150\n",
      "562/562 [==============================] - 0s 135us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 68/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 69/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 70/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 71/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 72/150\n",
      "562/562 [==============================] - 0s 125us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 73/150\n",
      "562/562 [==============================] - 0s 126us/step - loss: 0.1827 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 74/150\n",
      "562/562 [==============================] - 0s 126us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 75/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 76/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 77/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 78/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 79/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 80/150\n",
      "562/562 [==============================] - 0s 121us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1832 - val_accuracy: 0.9543\n",
      "Epoch 81/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 82/150\n",
      "562/562 [==============================] - 0s 125us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 83/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1826 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 84/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 85/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 86/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 87/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 88/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 89/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 90/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 91/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1825 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 92/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 93/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1831 - val_accuracy: 0.9543\n",
      "Epoch 94/150\n",
      "562/562 [==============================] - 0s 128us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 95/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 96/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 97/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 98/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 99/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 100/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1830 - val_accuracy: 0.9543\n",
      "Epoch 101/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1824 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 102/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 103/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 104/150\n",
      "562/562 [==============================] - 0s 123us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 105/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 106/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 107/150\n",
      "562/562 [==============================] - 0s 116us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 108/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 109/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 110/150\n",
      "562/562 [==============================] - 0s 123us/step - loss: 0.1823 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 111/150\n",
      "562/562 [==============================] - 0s 123us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 112/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - 0s 119us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 114/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 115/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 116/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 117/150\n",
      "562/562 [==============================] - 0s 119us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 118/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 119/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9543\n",
      "Epoch 120/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1822 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 121/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 122/150\n",
      "562/562 [==============================] - 0s 117us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 123/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 124/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 125/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 126/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 127/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1828 - val_accuracy: 0.9543\n",
      "Epoch 128/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 129/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1821 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 130/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 131/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 132/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 133/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 134/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 135/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 136/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 137/150\n",
      "562/562 [==============================] - 0s 123us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 138/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 139/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 140/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1820 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 141/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 142/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 143/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1827 - val_accuracy: 0.9543\n",
      "Epoch 144/150\n",
      "562/562 [==============================] - 0s 112us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 145/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 146/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 147/150\n",
      "562/562 [==============================] - 0s 110us/step - loss: 0.1819 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 148/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1818 - accuracy: 0.9543 - val_loss: 0.1825 - val_accuracy: 0.9543\n",
      "Epoch 149/150\n",
      "562/562 [==============================] - 0s 114us/step - loss: 0.1818 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Epoch 150/150\n",
      "562/562 [==============================] - 0s 109us/step - loss: 0.1818 - accuracy: 0.9543 - val_loss: 0.1826 - val_accuracy: 0.9543\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 400)               19600     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 99)                39699     \n",
      "=================================================================\n",
      "Total params: 542,851\n",
      "Trainable params: 542,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model trained in 10.68740177154541 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "inputs = x_d7.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d7.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "train_input_d7 = inputs[:,2:]\n",
    "train_output_d7 = targets\n",
    "\n",
    "x_train_d7, x_test_d7, Y_train_d7, Y_test_d7 = train_test_split(train_input_d7, train_output_d7, train_size=0.75, shuffle=True)\n",
    "\n",
    "model_d7 = compile_FFNN_cv_model_DepthSeven(7)\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d7.fit(\n",
    "    x=x_train_d7,\n",
    "    y=Y_train_d7,\n",
    "    validation_split=.25,\n",
    "    epochs=150\n",
    ")\n",
    "model_d7.summary()\n",
    "print(\"Model trained in %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:   0\n",
      "False positive:  0\n",
      "True negative:   23348\n",
      "False negative:  1152\n"
     ]
    }
   ],
   "source": [
    "predictions_d7 = model_d7.predict(x_test_d7)\n",
    "\n",
    "y_pred = predictions_d7\n",
    "y_test = Y_test_d7\n",
    "\n",
    "y_pred[y_pred>=.5]=1 \n",
    "y_pred[y_pred<.5]=0\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_pred)-1):\n",
    "    for j in range(len(y_test[0])-1):\n",
    "        if y_test[i][j] == y_pred[i][j] and y_test[i][j] == 1:\n",
    "            TP += 1\n",
    "        if y_pred[i][j] == 1 and y_test[i][j] != y_pred[i][j]:\n",
    "            FP += 1\n",
    "        if y_test[i][j] == y_pred[i][j] and y_test[i][j] == 0:\n",
    "            TN += 1\n",
    "        if y_pred[i][j] == 0 and y_test[i][j] != y_pred[i][j]:\n",
    "            FN += 1\n",
    "\n",
    "        \n",
    "print(\"True positive:   \" + str(TP) + \"\\nFalse positive:  \" + str(FP) + \"\\nTrue negative:   \" + str(TN) + \"\\nFalse negative:  \" + str(FN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Epoch vs. Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss (MSE)')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 7 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Epoch vs. Accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 7 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d7.save(\"model_d7_v2-500k.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 45342 samples, validate on 15114 samples\n",
      "Epoch 1/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.2161 - accuracy: 0.9426 - val_loss: 0.1937 - val_accuracy: 0.9511\n",
      "Epoch 2/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.1901 - accuracy: 0.9521 - val_loss: 0.1930 - val_accuracy: 0.9511\n",
      "Epoch 3/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.1895 - accuracy: 0.9521 - val_loss: 0.1923 - val_accuracy: 0.9511\n",
      "Epoch 4/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1887 - accuracy: 0.9521 - val_loss: 0.1915 - val_accuracy: 0.9511\n",
      "Epoch 5/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1876 - accuracy: 0.9521 - val_loss: 0.1902 - val_accuracy: 0.9511\n",
      "Epoch 6/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.1862 - accuracy: 0.9521 - val_loss: 0.1884 - val_accuracy: 0.9511\n",
      "Epoch 7/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.1840 - accuracy: 0.9521 - val_loss: 0.1859 - val_accuracy: 0.9511\n",
      "Epoch 8/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1809 - accuracy: 0.9521 - val_loss: 0.1821 - val_accuracy: 0.9511\n",
      "Epoch 9/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1767 - accuracy: 0.9521 - val_loss: 0.1775 - val_accuracy: 0.9511\n",
      "Epoch 10/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.1718 - accuracy: 0.9522 - val_loss: 0.1724 - val_accuracy: 0.9511\n",
      "Epoch 11/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1666 - accuracy: 0.9522 - val_loss: 0.1672 - val_accuracy: 0.9512\n",
      "Epoch 12/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1615 - accuracy: 0.9522 - val_loss: 0.1623 - val_accuracy: 0.9512\n",
      "Epoch 13/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1567 - accuracy: 0.9523 - val_loss: 0.1576 - val_accuracy: 0.9513\n",
      "Epoch 14/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1523 - accuracy: 0.9525 - val_loss: 0.1535 - val_accuracy: 0.9515\n",
      "Epoch 15/150\n",
      "45342/45342 [==============================] - 6s 121us/step - loss: 0.1483 - accuracy: 0.9527 - val_loss: 0.1497 - val_accuracy: 0.9517\n",
      "Epoch 16/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1446 - accuracy: 0.9529 - val_loss: 0.1462 - val_accuracy: 0.9520\n",
      "Epoch 17/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1413 - accuracy: 0.9533 - val_loss: 0.1431 - val_accuracy: 0.9524\n",
      "Epoch 18/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.1385 - accuracy: 0.9537 - val_loss: 0.1406 - val_accuracy: 0.9529\n",
      "Epoch 19/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1359 - accuracy: 0.9542 - val_loss: 0.1381 - val_accuracy: 0.9532\n",
      "Epoch 20/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1335 - accuracy: 0.9547 - val_loss: 0.1356 - val_accuracy: 0.9538\n",
      "Epoch 21/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1311 - accuracy: 0.9551 - val_loss: 0.1331 - val_accuracy: 0.9542\n",
      "Epoch 22/150\n",
      "45342/45342 [==============================] - 5s 104us/step - loss: 0.1289 - accuracy: 0.9556 - val_loss: 0.1311 - val_accuracy: 0.9546\n",
      "Epoch 23/150\n",
      "45342/45342 [==============================] - 5s 102us/step - loss: 0.1269 - accuracy: 0.9561 - val_loss: 0.1291 - val_accuracy: 0.9551\n",
      "Epoch 24/150\n",
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.1250 - accuracy: 0.9565 - val_loss: 0.1274 - val_accuracy: 0.9557\n",
      "Epoch 25/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.1234 - accuracy: 0.9569 - val_loss: 0.1259 - val_accuracy: 0.9560\n",
      "Epoch 26/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.1220 - accuracy: 0.9574 - val_loss: 0.1244 - val_accuracy: 0.9564\n",
      "Epoch 27/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1207 - accuracy: 0.9578 - val_loss: 0.1233 - val_accuracy: 0.9568\n",
      "Epoch 28/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1194 - accuracy: 0.9581 - val_loss: 0.1219 - val_accuracy: 0.9572\n",
      "Epoch 29/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1182 - accuracy: 0.9585 - val_loss: 0.1208 - val_accuracy: 0.9575\n",
      "Epoch 30/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.1169 - accuracy: 0.9589 - val_loss: 0.1194 - val_accuracy: 0.9580\n",
      "Epoch 31/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1157 - accuracy: 0.9593 - val_loss: 0.1184 - val_accuracy: 0.9583\n",
      "Epoch 32/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1146 - accuracy: 0.9597 - val_loss: 0.1172 - val_accuracy: 0.9587\n",
      "Epoch 33/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1135 - accuracy: 0.9601 - val_loss: 0.1164 - val_accuracy: 0.9590\n",
      "Epoch 34/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1125 - accuracy: 0.9604 - val_loss: 0.1153 - val_accuracy: 0.9595\n",
      "Epoch 35/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.1116 - accuracy: 0.9607 - val_loss: 0.1144 - val_accuracy: 0.9596\n",
      "Epoch 36/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1106 - accuracy: 0.9610 - val_loss: 0.1133 - val_accuracy: 0.9601\n",
      "Epoch 37/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.1097 - accuracy: 0.9614 - val_loss: 0.1126 - val_accuracy: 0.9601\n",
      "Epoch 38/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 0.1117 - val_accuracy: 0.9605\n",
      "Epoch 39/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1079 - accuracy: 0.9620 - val_loss: 0.1108 - val_accuracy: 0.9610\n",
      "Epoch 40/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1070 - accuracy: 0.9623 - val_loss: 0.1099 - val_accuracy: 0.9614\n",
      "Epoch 41/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1062 - accuracy: 0.9626 - val_loss: 0.1090 - val_accuracy: 0.9614\n",
      "Epoch 42/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1054 - accuracy: 0.9629 - val_loss: 0.1083 - val_accuracy: 0.9617\n",
      "Epoch 43/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1047 - accuracy: 0.9632 - val_loss: 0.1078 - val_accuracy: 0.9619\n",
      "Epoch 44/150\n",
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.1040 - accuracy: 0.9634 - val_loss: 0.1071 - val_accuracy: 0.9623\n",
      "Epoch 45/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.1034 - accuracy: 0.9636 - val_loss: 0.1066 - val_accuracy: 0.9623\n",
      "Epoch 46/150\n",
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.1029 - accuracy: 0.9639 - val_loss: 0.1060 - val_accuracy: 0.9626\n",
      "Epoch 47/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1024 - accuracy: 0.9640 - val_loss: 0.1054 - val_accuracy: 0.9630\n",
      "Epoch 48/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1019 - accuracy: 0.9642 - val_loss: 0.1050 - val_accuracy: 0.9631\n",
      "Epoch 49/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1015 - accuracy: 0.9644 - val_loss: 0.1047 - val_accuracy: 0.9631\n",
      "Epoch 50/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1010 - accuracy: 0.9646 - val_loss: 0.1042 - val_accuracy: 0.9634\n",
      "Epoch 51/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1006 - accuracy: 0.9648 - val_loss: 0.1038 - val_accuracy: 0.9636\n",
      "Epoch 52/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1001 - accuracy: 0.9649 - val_loss: 0.1036 - val_accuracy: 0.9636\n",
      "Epoch 53/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0997 - accuracy: 0.9651 - val_loss: 0.1030 - val_accuracy: 0.9638\n",
      "Epoch 54/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0993 - accuracy: 0.9652 - val_loss: 0.1026 - val_accuracy: 0.9638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150\n",
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.0989 - accuracy: 0.9654 - val_loss: 0.1023 - val_accuracy: 0.9640\n",
      "Epoch 56/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0985 - accuracy: 0.9655 - val_loss: 0.1020 - val_accuracy: 0.9642\n",
      "Epoch 57/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0981 - accuracy: 0.9656 - val_loss: 0.1015 - val_accuracy: 0.9643\n",
      "Epoch 58/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0978 - accuracy: 0.9658 - val_loss: 0.1012 - val_accuracy: 0.9645\n",
      "Epoch 59/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0974 - accuracy: 0.9659 - val_loss: 0.1008 - val_accuracy: 0.9646\n",
      "Epoch 60/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0971 - accuracy: 0.9661 - val_loss: 0.1006 - val_accuracy: 0.9646\n",
      "Epoch 61/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0968 - accuracy: 0.9662 - val_loss: 0.1002 - val_accuracy: 0.9649\n",
      "Epoch 62/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0964 - accuracy: 0.9663 - val_loss: 0.0999 - val_accuracy: 0.9649\n",
      "Epoch 63/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0961 - accuracy: 0.9664 - val_loss: 0.0997 - val_accuracy: 0.9649\n",
      "Epoch 64/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0958 - accuracy: 0.9665 - val_loss: 0.0994 - val_accuracy: 0.9652\n",
      "Epoch 65/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0955 - accuracy: 0.9666 - val_loss: 0.0992 - val_accuracy: 0.9652\n",
      "Epoch 66/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0952 - accuracy: 0.9667 - val_loss: 0.0989 - val_accuracy: 0.9652\n",
      "Epoch 67/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0949 - accuracy: 0.9668 - val_loss: 0.0984 - val_accuracy: 0.9656\n",
      "Epoch 68/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0946 - accuracy: 0.9669 - val_loss: 0.0982 - val_accuracy: 0.9654\n",
      "Epoch 69/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0943 - accuracy: 0.9669 - val_loss: 0.0979 - val_accuracy: 0.9656\n",
      "Epoch 70/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0940 - accuracy: 0.9671 - val_loss: 0.0977 - val_accuracy: 0.9657\n",
      "Epoch 71/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0937 - accuracy: 0.9672 - val_loss: 0.0974 - val_accuracy: 0.9658\n",
      "Epoch 72/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0934 - accuracy: 0.9673 - val_loss: 0.0971 - val_accuracy: 0.9658\n",
      "Epoch 73/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0931 - accuracy: 0.9674 - val_loss: 0.0967 - val_accuracy: 0.9661\n",
      "Epoch 74/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0928 - accuracy: 0.9675 - val_loss: 0.0966 - val_accuracy: 0.9661\n",
      "Epoch 75/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0925 - accuracy: 0.9675 - val_loss: 0.0962 - val_accuracy: 0.9662\n",
      "Epoch 76/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0922 - accuracy: 0.9677 - val_loss: 0.0961 - val_accuracy: 0.9663\n",
      "Epoch 77/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0920 - accuracy: 0.9677 - val_loss: 0.0957 - val_accuracy: 0.9663\n",
      "Epoch 78/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0917 - accuracy: 0.9678 - val_loss: 0.0956 - val_accuracy: 0.9663\n",
      "Epoch 79/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.0914 - accuracy: 0.9679 - val_loss: 0.0951 - val_accuracy: 0.9665\n",
      "Epoch 80/150\n",
      "45342/45342 [==============================] - 6s 125us/step - loss: 0.0911 - accuracy: 0.9680 - val_loss: 0.0950 - val_accuracy: 0.9666\n",
      "Epoch 81/150\n",
      "45342/45342 [==============================] - 6s 124us/step - loss: 0.0909 - accuracy: 0.9681 - val_loss: 0.0949 - val_accuracy: 0.9666\n",
      "Epoch 82/150\n",
      "45342/45342 [==============================] - 6s 124us/step - loss: 0.0906 - accuracy: 0.9681 - val_loss: 0.0947 - val_accuracy: 0.9668\n",
      "Epoch 83/150\n",
      "45342/45342 [==============================] - 6s 124us/step - loss: 0.0903 - accuracy: 0.9682 - val_loss: 0.0944 - val_accuracy: 0.9668\n",
      "Epoch 84/150\n",
      "45342/45342 [==============================] - 6s 122us/step - loss: 0.0901 - accuracy: 0.9683 - val_loss: 0.0940 - val_accuracy: 0.9669\n",
      "Epoch 85/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0898 - accuracy: 0.9683 - val_loss: 0.0940 - val_accuracy: 0.9667\n",
      "Epoch 86/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0896 - accuracy: 0.9684 - val_loss: 0.0937 - val_accuracy: 0.9671\n",
      "Epoch 87/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0893 - accuracy: 0.9685 - val_loss: 0.0932 - val_accuracy: 0.9672\n",
      "Epoch 88/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0891 - accuracy: 0.9686 - val_loss: 0.0933 - val_accuracy: 0.9671\n",
      "Epoch 89/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0889 - accuracy: 0.9686 - val_loss: 0.0929 - val_accuracy: 0.9672\n",
      "Epoch 90/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0886 - accuracy: 0.9687 - val_loss: 0.0926 - val_accuracy: 0.9674\n",
      "Epoch 91/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.0884 - accuracy: 0.9688 - val_loss: 0.0924 - val_accuracy: 0.9674\n",
      "Epoch 92/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0882 - accuracy: 0.9688 - val_loss: 0.0923 - val_accuracy: 0.9674\n",
      "Epoch 93/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0879 - accuracy: 0.9689 - val_loss: 0.0921 - val_accuracy: 0.9675\n",
      "Epoch 94/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0877 - accuracy: 0.9690 - val_loss: 0.0919 - val_accuracy: 0.9676\n",
      "Epoch 95/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.0875 - accuracy: 0.9690 - val_loss: 0.0916 - val_accuracy: 0.9677\n",
      "Epoch 96/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0872 - accuracy: 0.9691 - val_loss: 0.0914 - val_accuracy: 0.9676\n",
      "Epoch 97/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0870 - accuracy: 0.9691 - val_loss: 0.0916 - val_accuracy: 0.9677\n",
      "Epoch 98/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0868 - accuracy: 0.9692 - val_loss: 0.0911 - val_accuracy: 0.9677\n",
      "Epoch 99/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0866 - accuracy: 0.9692 - val_loss: 0.0909 - val_accuracy: 0.9679\n",
      "Epoch 100/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0864 - accuracy: 0.9693 - val_loss: 0.0908 - val_accuracy: 0.9680\n",
      "Epoch 101/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0862 - accuracy: 0.9694 - val_loss: 0.0906 - val_accuracy: 0.9679\n",
      "Epoch 102/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0860 - accuracy: 0.9694 - val_loss: 0.0906 - val_accuracy: 0.9680\n",
      "Epoch 103/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0858 - accuracy: 0.9695 - val_loss: 0.0901 - val_accuracy: 0.9680\n",
      "Epoch 104/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0856 - accuracy: 0.9695 - val_loss: 0.0899 - val_accuracy: 0.9680\n",
      "Epoch 105/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0854 - accuracy: 0.9695 - val_loss: 0.0898 - val_accuracy: 0.9681\n",
      "Epoch 106/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0852 - accuracy: 0.9696 - val_loss: 0.0896 - val_accuracy: 0.9682\n",
      "Epoch 107/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0850 - accuracy: 0.9696 - val_loss: 0.0894 - val_accuracy: 0.9682\n",
      "Epoch 108/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0848 - accuracy: 0.9697 - val_loss: 0.0894 - val_accuracy: 0.9683\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0846 - accuracy: 0.9698 - val_loss: 0.0891 - val_accuracy: 0.9682\n",
      "Epoch 110/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0845 - accuracy: 0.9698 - val_loss: 0.0890 - val_accuracy: 0.9682\n",
      "Epoch 111/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0843 - accuracy: 0.9698 - val_loss: 0.0888 - val_accuracy: 0.9684\n",
      "Epoch 112/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0841 - accuracy: 0.9699 - val_loss: 0.0888 - val_accuracy: 0.9683\n",
      "Epoch 113/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0839 - accuracy: 0.9699 - val_loss: 0.0886 - val_accuracy: 0.9684\n",
      "Epoch 114/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0837 - accuracy: 0.9700 - val_loss: 0.0885 - val_accuracy: 0.9684\n",
      "Epoch 115/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0836 - accuracy: 0.9700 - val_loss: 0.0883 - val_accuracy: 0.9685\n",
      "Epoch 116/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0834 - accuracy: 0.9700 - val_loss: 0.0881 - val_accuracy: 0.9686\n",
      "Epoch 117/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0833 - accuracy: 0.9701 - val_loss: 0.0878 - val_accuracy: 0.9687\n",
      "Epoch 118/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0831 - accuracy: 0.9701 - val_loss: 0.0879 - val_accuracy: 0.9685\n",
      "Epoch 119/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0829 - accuracy: 0.9701 - val_loss: 0.0877 - val_accuracy: 0.9686\n",
      "Epoch 120/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0828 - accuracy: 0.9702 - val_loss: 0.0875 - val_accuracy: 0.9688\n",
      "Epoch 121/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0826 - accuracy: 0.9702 - val_loss: 0.0876 - val_accuracy: 0.9687\n",
      "Epoch 122/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.0825 - accuracy: 0.9703 - val_loss: 0.0872 - val_accuracy: 0.9688\n",
      "Epoch 123/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0823 - accuracy: 0.9703 - val_loss: 0.0871 - val_accuracy: 0.9688\n",
      "Epoch 124/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0822 - accuracy: 0.9703 - val_loss: 0.0869 - val_accuracy: 0.9689\n",
      "Epoch 125/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0820 - accuracy: 0.9704 - val_loss: 0.0870 - val_accuracy: 0.9688\n",
      "Epoch 126/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0819 - accuracy: 0.9704 - val_loss: 0.0869 - val_accuracy: 0.9688\n",
      "Epoch 127/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0818 - accuracy: 0.9704 - val_loss: 0.0868 - val_accuracy: 0.9688\n",
      "Epoch 128/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0816 - accuracy: 0.9705 - val_loss: 0.0865 - val_accuracy: 0.9690\n",
      "Epoch 129/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0815 - accuracy: 0.9705 - val_loss: 0.0863 - val_accuracy: 0.9689\n",
      "Epoch 130/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0813 - accuracy: 0.9705 - val_loss: 0.0863 - val_accuracy: 0.9690\n",
      "Epoch 131/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0812 - accuracy: 0.9705 - val_loss: 0.0863 - val_accuracy: 0.9689\n",
      "Epoch 132/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0811 - accuracy: 0.9706 - val_loss: 0.0861 - val_accuracy: 0.9690\n",
      "Epoch 133/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0809 - accuracy: 0.9706 - val_loss: 0.0861 - val_accuracy: 0.9690\n",
      "Epoch 134/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0808 - accuracy: 0.9706 - val_loss: 0.0862 - val_accuracy: 0.9689\n",
      "Epoch 135/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0807 - accuracy: 0.9707 - val_loss: 0.0858 - val_accuracy: 0.9690\n",
      "Epoch 136/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0805 - accuracy: 0.9707 - val_loss: 0.0857 - val_accuracy: 0.9691\n",
      "Epoch 137/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0804 - accuracy: 0.9707 - val_loss: 0.0855 - val_accuracy: 0.9691\n",
      "Epoch 138/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0803 - accuracy: 0.9707 - val_loss: 0.0856 - val_accuracy: 0.9691\n",
      "Epoch 139/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0802 - accuracy: 0.9708 - val_loss: 0.0854 - val_accuracy: 0.9690\n",
      "Epoch 140/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0800 - accuracy: 0.9708 - val_loss: 0.0853 - val_accuracy: 0.9691\n",
      "Epoch 141/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0799 - accuracy: 0.9709 - val_loss: 0.0851 - val_accuracy: 0.9692\n",
      "Epoch 142/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0798 - accuracy: 0.9708 - val_loss: 0.0850 - val_accuracy: 0.9692\n",
      "Epoch 143/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0797 - accuracy: 0.9709 - val_loss: 0.0849 - val_accuracy: 0.9692\n",
      "Epoch 144/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0796 - accuracy: 0.9709 - val_loss: 0.0851 - val_accuracy: 0.9692\n",
      "Epoch 145/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0795 - accuracy: 0.9709 - val_loss: 0.0846 - val_accuracy: 0.9693\n",
      "Epoch 146/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0793 - accuracy: 0.9710 - val_loss: 0.0846 - val_accuracy: 0.9693\n",
      "Epoch 147/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0792 - accuracy: 0.9710 - val_loss: 0.0846 - val_accuracy: 0.9692\n",
      "Epoch 148/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0791 - accuracy: 0.9710 - val_loss: 0.0846 - val_accuracy: 0.9692\n",
      "Epoch 149/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0790 - accuracy: 0.9711 - val_loss: 0.0845 - val_accuracy: 0.9693\n",
      "Epoch 150/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0789 - accuracy: 0.9711 - val_loss: 0.0844 - val_accuracy: 0.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:136: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 45342 samples, validate on 15115 samples\n",
      "Epoch 1/150\n",
      "45342/45342 [==============================] - 5s 120us/step - loss: 0.2163 - accuracy: 0.9404 - val_loss: 0.1936 - val_accuracy: 0.9511\n",
      "Epoch 2/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1897 - accuracy: 0.9522 - val_loss: 0.1929 - val_accuracy: 0.9511\n",
      "Epoch 3/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1889 - accuracy: 0.9522 - val_loss: 0.1920 - val_accuracy: 0.9511\n",
      "Epoch 4/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1879 - accuracy: 0.9522 - val_loss: 0.1908 - val_accuracy: 0.9511\n",
      "Epoch 5/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1864 - accuracy: 0.9522 - val_loss: 0.1890 - val_accuracy: 0.9511\n",
      "Epoch 6/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1843 - accuracy: 0.9522 - val_loss: 0.1864 - val_accuracy: 0.9511\n",
      "Epoch 7/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1812 - accuracy: 0.9522 - val_loss: 0.1827 - val_accuracy: 0.9511\n",
      "Epoch 8/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1771 - accuracy: 0.9522 - val_loss: 0.1783 - val_accuracy: 0.9511\n",
      "Epoch 9/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1725 - accuracy: 0.9522 - val_loss: 0.1734 - val_accuracy: 0.9511\n",
      "Epoch 10/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1672 - accuracy: 0.9522 - val_loss: 0.1679 - val_accuracy: 0.9511\n",
      "Epoch 11/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1617 - accuracy: 0.9523 - val_loss: 0.1626 - val_accuracy: 0.9512\n",
      "Epoch 12/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1567 - accuracy: 0.9524 - val_loss: 0.1578 - val_accuracy: 0.9513\n",
      "Epoch 13/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.1520 - accuracy: 0.9525 - val_loss: 0.1532 - val_accuracy: 0.9515\n",
      "Epoch 14/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.1475 - accuracy: 0.9528 - val_loss: 0.1488 - val_accuracy: 0.9518\n",
      "Epoch 15/150\n",
      "45342/45342 [==============================] - 5s 121us/step - loss: 0.1433 - accuracy: 0.9531 - val_loss: 0.1449 - val_accuracy: 0.9521\n",
      "Epoch 16/150\n",
      "45342/45342 [==============================] - 5s 121us/step - loss: 0.1395 - accuracy: 0.9535 - val_loss: 0.1413 - val_accuracy: 0.9525\n",
      "Epoch 17/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1361 - accuracy: 0.9539 - val_loss: 0.1382 - val_accuracy: 0.9530\n",
      "Epoch 18/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.1331 - accuracy: 0.9544 - val_loss: 0.1353 - val_accuracy: 0.9535\n",
      "Epoch 19/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.1302 - accuracy: 0.9550 - val_loss: 0.1325 - val_accuracy: 0.9541\n",
      "Epoch 20/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.1275 - accuracy: 0.9556 - val_loss: 0.1301 - val_accuracy: 0.9547\n",
      "Epoch 21/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1251 - accuracy: 0.9562 - val_loss: 0.1277 - val_accuracy: 0.9553\n",
      "Epoch 22/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1229 - accuracy: 0.9568 - val_loss: 0.1257 - val_accuracy: 0.9557\n",
      "Epoch 23/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1209 - accuracy: 0.9574 - val_loss: 0.1238 - val_accuracy: 0.9564\n",
      "Epoch 24/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1192 - accuracy: 0.9579 - val_loss: 0.1223 - val_accuracy: 0.9568\n",
      "Epoch 25/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1176 - accuracy: 0.9584 - val_loss: 0.1206 - val_accuracy: 0.9573\n",
      "Epoch 26/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1161 - accuracy: 0.9589 - val_loss: 0.1192 - val_accuracy: 0.9579\n",
      "Epoch 27/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1147 - accuracy: 0.9594 - val_loss: 0.1180 - val_accuracy: 0.9583\n",
      "Epoch 28/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1134 - accuracy: 0.9599 - val_loss: 0.1169 - val_accuracy: 0.9587\n",
      "Epoch 29/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1124 - accuracy: 0.9603 - val_loss: 0.1158 - val_accuracy: 0.9592\n",
      "Epoch 30/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1114 - accuracy: 0.9607 - val_loss: 0.1149 - val_accuracy: 0.9594\n",
      "Epoch 31/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.1105 - accuracy: 0.9610 - val_loss: 0.1139 - val_accuracy: 0.9596\n",
      "Epoch 32/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1097 - accuracy: 0.9613 - val_loss: 0.1132 - val_accuracy: 0.9602\n",
      "Epoch 33/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1088 - accuracy: 0.9616 - val_loss: 0.1124 - val_accuracy: 0.9601\n",
      "Epoch 34/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1081 - accuracy: 0.9619 - val_loss: 0.1116 - val_accuracy: 0.9605\n",
      "Epoch 35/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1074 - accuracy: 0.9621 - val_loss: 0.1110 - val_accuracy: 0.9608\n",
      "Epoch 36/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1067 - accuracy: 0.9624 - val_loss: 0.1103 - val_accuracy: 0.9610\n",
      "Epoch 37/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1060 - accuracy: 0.9626 - val_loss: 0.1097 - val_accuracy: 0.9614\n",
      "Epoch 38/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1053 - accuracy: 0.9629 - val_loss: 0.1090 - val_accuracy: 0.9615\n",
      "Epoch 39/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1046 - accuracy: 0.9632 - val_loss: 0.1084 - val_accuracy: 0.9618\n",
      "Epoch 40/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1039 - accuracy: 0.9634 - val_loss: 0.1078 - val_accuracy: 0.9620\n",
      "Epoch 41/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1032 - accuracy: 0.9637 - val_loss: 0.1070 - val_accuracy: 0.9622\n",
      "Epoch 42/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1025 - accuracy: 0.9639 - val_loss: 0.1062 - val_accuracy: 0.9626\n",
      "Epoch 43/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.1018 - accuracy: 0.9642 - val_loss: 0.1056 - val_accuracy: 0.9628\n",
      "Epoch 44/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1012 - accuracy: 0.9644 - val_loss: 0.1050 - val_accuracy: 0.9631\n",
      "Epoch 45/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1006 - accuracy: 0.9646 - val_loss: 0.1044 - val_accuracy: 0.9633\n",
      "Epoch 46/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1001 - accuracy: 0.9648 - val_loss: 0.1040 - val_accuracy: 0.9636\n",
      "Epoch 47/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0996 - accuracy: 0.9650 - val_loss: 0.1034 - val_accuracy: 0.9638\n",
      "Epoch 48/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0991 - accuracy: 0.9652 - val_loss: 0.1028 - val_accuracy: 0.9638\n",
      "Epoch 49/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0986 - accuracy: 0.9654 - val_loss: 0.1023 - val_accuracy: 0.9640\n",
      "Epoch 50/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0981 - accuracy: 0.9656 - val_loss: 0.1020 - val_accuracy: 0.9643\n",
      "Epoch 51/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0976 - accuracy: 0.9658 - val_loss: 0.1016 - val_accuracy: 0.9645\n",
      "Epoch 52/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0972 - accuracy: 0.9660 - val_loss: 0.1012 - val_accuracy: 0.9645\n",
      "Epoch 53/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0967 - accuracy: 0.9661 - val_loss: 0.1005 - val_accuracy: 0.9648\n",
      "Epoch 54/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0963 - accuracy: 0.9663 - val_loss: 0.1001 - val_accuracy: 0.9649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0959 - accuracy: 0.9664 - val_loss: 0.0998 - val_accuracy: 0.9651\n",
      "Epoch 56/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0955 - accuracy: 0.9666 - val_loss: 0.0994 - val_accuracy: 0.9653\n",
      "Epoch 57/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0951 - accuracy: 0.9667 - val_loss: 0.0989 - val_accuracy: 0.9653\n",
      "Epoch 58/150\n",
      "45342/45342 [==============================] - 5s 120us/step - loss: 0.0947 - accuracy: 0.9669 - val_loss: 0.0987 - val_accuracy: 0.9655\n",
      "Epoch 59/150\n",
      "45342/45342 [==============================] - 6s 131us/step - loss: 0.0944 - accuracy: 0.9670 - val_loss: 0.0982 - val_accuracy: 0.9656\n",
      "Epoch 60/150\n",
      "45342/45342 [==============================] - 6s 127us/step - loss: 0.0940 - accuracy: 0.9671 - val_loss: 0.0981 - val_accuracy: 0.9655\n",
      "Epoch 61/150\n",
      "45342/45342 [==============================] - 5s 120us/step - loss: 0.0937 - accuracy: 0.9673 - val_loss: 0.0976 - val_accuracy: 0.9659\n",
      "Epoch 62/150\n",
      "45342/45342 [==============================] - 6s 139us/step - loss: 0.0934 - accuracy: 0.9674 - val_loss: 0.0975 - val_accuracy: 0.9660\n",
      "Epoch 63/150\n",
      "45342/45342 [==============================] - 6s 129us/step - loss: 0.0931 - accuracy: 0.9675 - val_loss: 0.0970 - val_accuracy: 0.9660\n",
      "Epoch 64/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0928 - accuracy: 0.9676 - val_loss: 0.0971 - val_accuracy: 0.9660\n",
      "Epoch 65/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0925 - accuracy: 0.9677 - val_loss: 0.0965 - val_accuracy: 0.9662\n",
      "Epoch 66/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0922 - accuracy: 0.9678 - val_loss: 0.0962 - val_accuracy: 0.9664\n",
      "Epoch 67/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0919 - accuracy: 0.9679 - val_loss: 0.0960 - val_accuracy: 0.9665\n",
      "Epoch 68/150\n",
      "45342/45342 [==============================] - 5s 121us/step - loss: 0.0916 - accuracy: 0.9680 - val_loss: 0.0960 - val_accuracy: 0.9664\n",
      "Epoch 69/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0914 - accuracy: 0.9680 - val_loss: 0.0956 - val_accuracy: 0.9666\n",
      "Epoch 70/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.0911 - accuracy: 0.9681 - val_loss: 0.0952 - val_accuracy: 0.9666\n",
      "Epoch 71/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0908 - accuracy: 0.9682 - val_loss: 0.0952 - val_accuracy: 0.9667\n",
      "Epoch 72/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0906 - accuracy: 0.9683 - val_loss: 0.0948 - val_accuracy: 0.9670\n",
      "Epoch 73/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.0903 - accuracy: 0.9684 - val_loss: 0.0946 - val_accuracy: 0.9670\n",
      "Epoch 74/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.0901 - accuracy: 0.9684 - val_loss: 0.0943 - val_accuracy: 0.9671\n",
      "Epoch 75/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0898 - accuracy: 0.9685 - val_loss: 0.0940 - val_accuracy: 0.9671\n",
      "Epoch 76/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0895 - accuracy: 0.9686 - val_loss: 0.0939 - val_accuracy: 0.9671\n",
      "Epoch 77/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0893 - accuracy: 0.9687 - val_loss: 0.0938 - val_accuracy: 0.9673\n",
      "Epoch 78/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0891 - accuracy: 0.9687 - val_loss: 0.0933 - val_accuracy: 0.9673\n",
      "Epoch 79/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.0888 - accuracy: 0.9688 - val_loss: 0.0931 - val_accuracy: 0.9674\n",
      "Epoch 80/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0886 - accuracy: 0.9689 - val_loss: 0.0929 - val_accuracy: 0.9674\n",
      "Epoch 81/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0884 - accuracy: 0.9689 - val_loss: 0.0928 - val_accuracy: 0.9674\n",
      "Epoch 82/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0881 - accuracy: 0.9690 - val_loss: 0.0924 - val_accuracy: 0.9675\n",
      "Epoch 83/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0879 - accuracy: 0.9691 - val_loss: 0.0924 - val_accuracy: 0.9675\n",
      "Epoch 84/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0877 - accuracy: 0.9691 - val_loss: 0.0922 - val_accuracy: 0.9675\n",
      "Epoch 85/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0874 - accuracy: 0.9692 - val_loss: 0.0919 - val_accuracy: 0.9676\n",
      "Epoch 86/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0872 - accuracy: 0.9692 - val_loss: 0.0918 - val_accuracy: 0.9678\n",
      "Epoch 87/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0870 - accuracy: 0.9693 - val_loss: 0.0915 - val_accuracy: 0.9678\n",
      "Epoch 88/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0868 - accuracy: 0.9693 - val_loss: 0.0912 - val_accuracy: 0.9679\n",
      "Epoch 89/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0866 - accuracy: 0.9694 - val_loss: 0.0914 - val_accuracy: 0.9678\n",
      "Epoch 90/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0864 - accuracy: 0.9695 - val_loss: 0.0908 - val_accuracy: 0.9680\n",
      "Epoch 91/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.0862 - accuracy: 0.9695 - val_loss: 0.0907 - val_accuracy: 0.9680\n",
      "Epoch 92/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0859 - accuracy: 0.9695 - val_loss: 0.0905 - val_accuracy: 0.9680\n",
      "Epoch 93/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0857 - accuracy: 0.9696 - val_loss: 0.0905 - val_accuracy: 0.9682\n",
      "Epoch 94/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0855 - accuracy: 0.9696 - val_loss: 0.0901 - val_accuracy: 0.9681\n",
      "Epoch 95/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0853 - accuracy: 0.9697 - val_loss: 0.0900 - val_accuracy: 0.9683\n",
      "Epoch 96/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0851 - accuracy: 0.9697 - val_loss: 0.0898 - val_accuracy: 0.9681\n",
      "Epoch 97/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0849 - accuracy: 0.9698 - val_loss: 0.0896 - val_accuracy: 0.9682\n",
      "Epoch 98/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0847 - accuracy: 0.9698 - val_loss: 0.0897 - val_accuracy: 0.9684\n",
      "Epoch 99/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0846 - accuracy: 0.9699 - val_loss: 0.0893 - val_accuracy: 0.9683\n",
      "Epoch 100/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0843 - accuracy: 0.9699 - val_loss: 0.0891 - val_accuracy: 0.9684\n",
      "Epoch 101/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0842 - accuracy: 0.9699 - val_loss: 0.0890 - val_accuracy: 0.9683\n",
      "Epoch 102/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0840 - accuracy: 0.9700 - val_loss: 0.0888 - val_accuracy: 0.9684\n",
      "Epoch 103/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0838 - accuracy: 0.9701 - val_loss: 0.0887 - val_accuracy: 0.9685\n",
      "Epoch 104/150\n",
      "45342/45342 [==============================] - 5s 103us/step - loss: 0.0836 - accuracy: 0.9701 - val_loss: 0.0885 - val_accuracy: 0.9685\n",
      "Epoch 105/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0834 - accuracy: 0.9701 - val_loss: 0.0883 - val_accuracy: 0.9686\n",
      "Epoch 106/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0832 - accuracy: 0.9702 - val_loss: 0.0883 - val_accuracy: 0.9685\n",
      "Epoch 107/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0830 - accuracy: 0.9702 - val_loss: 0.0881 - val_accuracy: 0.9685\n",
      "Epoch 108/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0829 - accuracy: 0.9703 - val_loss: 0.0878 - val_accuracy: 0.9687\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45342/45342 [==============================] - 5s 103us/step - loss: 0.0827 - accuracy: 0.9703 - val_loss: 0.0878 - val_accuracy: 0.9686\n",
      "Epoch 110/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0825 - accuracy: 0.9704 - val_loss: 0.0878 - val_accuracy: 0.9687\n",
      "Epoch 111/150\n",
      "45342/45342 [==============================] - 6s 122us/step - loss: 0.0824 - accuracy: 0.9704 - val_loss: 0.0876 - val_accuracy: 0.9687\n",
      "Epoch 112/150\n",
      "45342/45342 [==============================] - 6s 126us/step - loss: 0.0822 - accuracy: 0.9704 - val_loss: 0.0873 - val_accuracy: 0.9688\n",
      "Epoch 113/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0820 - accuracy: 0.9705 - val_loss: 0.0874 - val_accuracy: 0.9688\n",
      "Epoch 114/150\n",
      "45342/45342 [==============================] - 5s 118us/step - loss: 0.0818 - accuracy: 0.9705 - val_loss: 0.0871 - val_accuracy: 0.9688\n",
      "Epoch 115/150\n",
      "45342/45342 [==============================] - 5s 112us/step - loss: 0.0817 - accuracy: 0.9705 - val_loss: 0.0869 - val_accuracy: 0.9688\n",
      "Epoch 116/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0815 - accuracy: 0.9706 - val_loss: 0.0867 - val_accuracy: 0.9689\n",
      "Epoch 117/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0814 - accuracy: 0.9706 - val_loss: 0.0867 - val_accuracy: 0.9689\n",
      "Epoch 118/150\n",
      "45342/45342 [==============================] - 5s 117us/step - loss: 0.0812 - accuracy: 0.9707 - val_loss: 0.0863 - val_accuracy: 0.9690\n",
      "Epoch 119/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0811 - accuracy: 0.9707 - val_loss: 0.0863 - val_accuracy: 0.9690\n",
      "Epoch 120/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0809 - accuracy: 0.9707 - val_loss: 0.0861 - val_accuracy: 0.9690\n",
      "Epoch 121/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0807 - accuracy: 0.9707 - val_loss: 0.0860 - val_accuracy: 0.9691\n",
      "Epoch 122/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0806 - accuracy: 0.9707 - val_loss: 0.0861 - val_accuracy: 0.9690\n",
      "Epoch 123/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0804 - accuracy: 0.9708 - val_loss: 0.0861 - val_accuracy: 0.9689\n",
      "Epoch 124/150\n",
      "45342/45342 [==============================] - 5s 121us/step - loss: 0.0803 - accuracy: 0.9708 - val_loss: 0.0858 - val_accuracy: 0.9690\n",
      "Epoch 125/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0802 - accuracy: 0.9708 - val_loss: 0.0858 - val_accuracy: 0.9691\n",
      "Epoch 126/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0800 - accuracy: 0.9709 - val_loss: 0.0855 - val_accuracy: 0.9692\n",
      "Epoch 127/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0798 - accuracy: 0.9709 - val_loss: 0.0853 - val_accuracy: 0.9692\n",
      "Epoch 128/150\n",
      "45342/45342 [==============================] - 6s 122us/step - loss: 0.0797 - accuracy: 0.9710 - val_loss: 0.0852 - val_accuracy: 0.9692\n",
      "Epoch 129/150\n",
      "45342/45342 [==============================] - 5s 113us/step - loss: 0.0796 - accuracy: 0.9710 - val_loss: 0.0851 - val_accuracy: 0.9693\n",
      "Epoch 130/150\n",
      "45342/45342 [==============================] - 5s 120us/step - loss: 0.0794 - accuracy: 0.9710 - val_loss: 0.0850 - val_accuracy: 0.9692\n",
      "Epoch 131/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0793 - accuracy: 0.9711 - val_loss: 0.0848 - val_accuracy: 0.9692\n",
      "Epoch 132/150\n",
      "45342/45342 [==============================] - 6s 125us/step - loss: 0.0791 - accuracy: 0.9711 - val_loss: 0.0847 - val_accuracy: 0.9692\n",
      "Epoch 133/150\n",
      "45342/45342 [==============================] - 6s 123us/step - loss: 0.0790 - accuracy: 0.9710 - val_loss: 0.0845 - val_accuracy: 0.9693\n",
      "Epoch 134/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0789 - accuracy: 0.9711 - val_loss: 0.0846 - val_accuracy: 0.9692\n",
      "Epoch 135/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0787 - accuracy: 0.9711 - val_loss: 0.0846 - val_accuracy: 0.9691\n",
      "Epoch 136/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0786 - accuracy: 0.9711 - val_loss: 0.0843 - val_accuracy: 0.9694\n",
      "Epoch 137/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0785 - accuracy: 0.9712 - val_loss: 0.0844 - val_accuracy: 0.9693\n",
      "Epoch 138/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0783 - accuracy: 0.9712 - val_loss: 0.0839 - val_accuracy: 0.9695\n",
      "Epoch 139/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0782 - accuracy: 0.9713 - val_loss: 0.0841 - val_accuracy: 0.9692\n",
      "Epoch 140/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0781 - accuracy: 0.9713 - val_loss: 0.0839 - val_accuracy: 0.9694\n",
      "Epoch 141/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0780 - accuracy: 0.9713 - val_loss: 0.0839 - val_accuracy: 0.9693\n",
      "Epoch 142/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.0778 - accuracy: 0.9713 - val_loss: 0.0840 - val_accuracy: 0.9694\n",
      "Epoch 143/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0777 - accuracy: 0.9713 - val_loss: 0.0838 - val_accuracy: 0.9693\n",
      "Epoch 144/150\n",
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.0776 - accuracy: 0.9714 - val_loss: 0.0834 - val_accuracy: 0.9695\n",
      "Epoch 145/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0774 - accuracy: 0.9714 - val_loss: 0.0836 - val_accuracy: 0.9695\n",
      "Epoch 146/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0773 - accuracy: 0.9714 - val_loss: 0.0832 - val_accuracy: 0.9695\n",
      "Epoch 147/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0772 - accuracy: 0.9714 - val_loss: 0.0832 - val_accuracy: 0.9695\n",
      "Epoch 148/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0771 - accuracy: 0.9715 - val_loss: 0.0831 - val_accuracy: 0.9695\n",
      "Epoch 149/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0770 - accuracy: 0.9715 - val_loss: 0.0829 - val_accuracy: 0.9696\n",
      "Epoch 150/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0769 - accuracy: 0.9715 - val_loss: 0.0828 - val_accuracy: 0.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:136: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 45342 samples, validate on 15115 samples\n",
      "Epoch 1/150\n",
      "45342/45342 [==============================] - 5s 103us/step - loss: 0.2106 - accuracy: 0.9448 - val_loss: 0.1935 - val_accuracy: 0.9511\n",
      "Epoch 2/150\n",
      "45342/45342 [==============================] - 5s 102us/step - loss: 0.1896 - accuracy: 0.9522 - val_loss: 0.1927 - val_accuracy: 0.9511\n",
      "Epoch 3/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.1888 - accuracy: 0.9522 - val_loss: 0.1918 - val_accuracy: 0.9511\n",
      "Epoch 4/150\n",
      "45342/45342 [==============================] - 5s 104us/step - loss: 0.1877 - accuracy: 0.9522 - val_loss: 0.1905 - val_accuracy: 0.9511\n",
      "Epoch 5/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1862 - accuracy: 0.9522 - val_loss: 0.1886 - val_accuracy: 0.9511\n",
      "Epoch 6/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1839 - accuracy: 0.9522 - val_loss: 0.1859 - val_accuracy: 0.9511\n",
      "Epoch 7/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.1806 - accuracy: 0.9522 - val_loss: 0.1819 - val_accuracy: 0.9511\n",
      "Epoch 8/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1761 - accuracy: 0.9522 - val_loss: 0.1771 - val_accuracy: 0.9511\n",
      "Epoch 9/150\n",
      "45342/45342 [==============================] - 5s 102us/step - loss: 0.1713 - accuracy: 0.9522 - val_loss: 0.1723 - val_accuracy: 0.9511\n",
      "Epoch 10/150\n",
      "45342/45342 [==============================] - 5s 102us/step - loss: 0.1662 - accuracy: 0.9522 - val_loss: 0.1668 - val_accuracy: 0.9511\n",
      "Epoch 11/150\n",
      "45342/45342 [==============================] - 5s 104us/step - loss: 0.1605 - accuracy: 0.9522 - val_loss: 0.1609 - val_accuracy: 0.9511\n",
      "Epoch 12/150\n",
      "45342/45342 [==============================] - 5s 116us/step - loss: 0.1546 - accuracy: 0.9523 - val_loss: 0.1552 - val_accuracy: 0.9513\n",
      "Epoch 13/150\n",
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.1492 - accuracy: 0.9526 - val_loss: 0.1501 - val_accuracy: 0.9516\n",
      "Epoch 14/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.1443 - accuracy: 0.9529 - val_loss: 0.1455 - val_accuracy: 0.9520\n",
      "Epoch 15/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.1399 - accuracy: 0.9534 - val_loss: 0.1414 - val_accuracy: 0.9525\n",
      "Epoch 16/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1359 - accuracy: 0.9540 - val_loss: 0.1375 - val_accuracy: 0.9532\n",
      "Epoch 17/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.1322 - accuracy: 0.9547 - val_loss: 0.1340 - val_accuracy: 0.9539\n",
      "Epoch 18/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1289 - accuracy: 0.9555 - val_loss: 0.1309 - val_accuracy: 0.9546\n",
      "Epoch 19/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1261 - accuracy: 0.9562 - val_loss: 0.1284 - val_accuracy: 0.9552\n",
      "Epoch 20/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1237 - accuracy: 0.9569 - val_loss: 0.1262 - val_accuracy: 0.9559\n",
      "Epoch 21/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.1216 - accuracy: 0.9575 - val_loss: 0.1242 - val_accuracy: 0.9563\n",
      "Epoch 22/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.1196 - accuracy: 0.9581 - val_loss: 0.1224 - val_accuracy: 0.9570\n",
      "Epoch 23/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.1177 - accuracy: 0.9587 - val_loss: 0.1204 - val_accuracy: 0.9577\n",
      "Epoch 24/150\n",
      "45342/45342 [==============================] - 5s 105us/step - loss: 0.1159 - accuracy: 0.9593 - val_loss: 0.1187 - val_accuracy: 0.9582\n",
      "Epoch 25/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.1143 - accuracy: 0.9599 - val_loss: 0.1173 - val_accuracy: 0.9587\n",
      "Epoch 26/150\n",
      "45342/45342 [==============================] - 5s 103us/step - loss: 0.1128 - accuracy: 0.9604 - val_loss: 0.1159 - val_accuracy: 0.9592\n",
      "Epoch 27/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.1115 - accuracy: 0.9609 - val_loss: 0.1146 - val_accuracy: 0.9597\n",
      "Epoch 28/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.1103 - accuracy: 0.9613 - val_loss: 0.1137 - val_accuracy: 0.9601\n",
      "Epoch 29/150\n",
      "45342/45342 [==============================] - 5s 104us/step - loss: 0.1092 - accuracy: 0.9616 - val_loss: 0.1126 - val_accuracy: 0.9605\n",
      "Epoch 30/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.1082 - accuracy: 0.9620 - val_loss: 0.1115 - val_accuracy: 0.9609\n",
      "Epoch 31/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1073 - accuracy: 0.9624 - val_loss: 0.1104 - val_accuracy: 0.9614\n",
      "Epoch 32/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.1064 - accuracy: 0.9627 - val_loss: 0.1096 - val_accuracy: 0.9615\n",
      "Epoch 33/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.1056 - accuracy: 0.9630 - val_loss: 0.1090 - val_accuracy: 0.9619\n",
      "Epoch 34/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.1049 - accuracy: 0.9633 - val_loss: 0.1084 - val_accuracy: 0.9621\n",
      "Epoch 35/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.1042 - accuracy: 0.9635 - val_loss: 0.1078 - val_accuracy: 0.9624\n",
      "Epoch 36/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.1036 - accuracy: 0.9638 - val_loss: 0.1073 - val_accuracy: 0.9626\n",
      "Epoch 37/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.1031 - accuracy: 0.9640 - val_loss: 0.1066 - val_accuracy: 0.9628\n",
      "Epoch 38/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1025 - accuracy: 0.9642 - val_loss: 0.1061 - val_accuracy: 0.9631\n",
      "Epoch 39/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1019 - accuracy: 0.9644 - val_loss: 0.1057 - val_accuracy: 0.9633\n",
      "Epoch 40/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1014 - accuracy: 0.9646 - val_loss: 0.1048 - val_accuracy: 0.9635\n",
      "Epoch 41/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1009 - accuracy: 0.9648 - val_loss: 0.1045 - val_accuracy: 0.9636\n",
      "Epoch 42/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.1004 - accuracy: 0.9651 - val_loss: 0.1040 - val_accuracy: 0.9640\n",
      "Epoch 43/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0998 - accuracy: 0.9652 - val_loss: 0.1035 - val_accuracy: 0.9640\n",
      "Epoch 44/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0993 - accuracy: 0.9654 - val_loss: 0.1029 - val_accuracy: 0.9643\n",
      "Epoch 45/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.0989 - accuracy: 0.9656 - val_loss: 0.1027 - val_accuracy: 0.9644\n",
      "Epoch 46/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0984 - accuracy: 0.9658 - val_loss: 0.1020 - val_accuracy: 0.9646\n",
      "Epoch 47/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0979 - accuracy: 0.9660 - val_loss: 0.1017 - val_accuracy: 0.9647\n",
      "Epoch 48/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0975 - accuracy: 0.9661 - val_loss: 0.1012 - val_accuracy: 0.9649\n",
      "Epoch 49/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0971 - accuracy: 0.9662 - val_loss: 0.1011 - val_accuracy: 0.9650\n",
      "Epoch 50/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0967 - accuracy: 0.9664 - val_loss: 0.1005 - val_accuracy: 0.9651\n",
      "Epoch 51/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0964 - accuracy: 0.9665 - val_loss: 0.1000 - val_accuracy: 0.9654\n",
      "Epoch 52/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0960 - accuracy: 0.9667 - val_loss: 0.0997 - val_accuracy: 0.9655\n",
      "Epoch 53/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0956 - accuracy: 0.9668 - val_loss: 0.0994 - val_accuracy: 0.9655\n",
      "Epoch 54/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0953 - accuracy: 0.9670 - val_loss: 0.0992 - val_accuracy: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0949 - accuracy: 0.9671 - val_loss: 0.0988 - val_accuracy: 0.9658\n",
      "Epoch 56/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0946 - accuracy: 0.9672 - val_loss: 0.0986 - val_accuracy: 0.9659\n",
      "Epoch 57/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0942 - accuracy: 0.9673 - val_loss: 0.0981 - val_accuracy: 0.9662\n",
      "Epoch 58/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0939 - accuracy: 0.9674 - val_loss: 0.0979 - val_accuracy: 0.9660\n",
      "Epoch 59/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0936 - accuracy: 0.9675 - val_loss: 0.0974 - val_accuracy: 0.9662\n",
      "Epoch 60/150\n",
      "45342/45342 [==============================] - 5s 103us/step - loss: 0.0933 - accuracy: 0.9676 - val_loss: 0.0974 - val_accuracy: 0.9664\n",
      "Epoch 61/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0930 - accuracy: 0.9677 - val_loss: 0.0969 - val_accuracy: 0.9663\n",
      "Epoch 62/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0927 - accuracy: 0.9678 - val_loss: 0.0966 - val_accuracy: 0.9666\n",
      "Epoch 63/150\n",
      "45342/45342 [==============================] - 5s 119us/step - loss: 0.0923 - accuracy: 0.9679 - val_loss: 0.0964 - val_accuracy: 0.9667\n",
      "Epoch 64/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0920 - accuracy: 0.9680 - val_loss: 0.0962 - val_accuracy: 0.9667\n",
      "Epoch 65/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0917 - accuracy: 0.9681 - val_loss: 0.0959 - val_accuracy: 0.9668\n",
      "Epoch 66/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0915 - accuracy: 0.9681 - val_loss: 0.0957 - val_accuracy: 0.9669\n",
      "Epoch 67/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0912 - accuracy: 0.9682 - val_loss: 0.0953 - val_accuracy: 0.9669\n",
      "Epoch 68/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0909 - accuracy: 0.9683 - val_loss: 0.0950 - val_accuracy: 0.9671\n",
      "Epoch 69/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0906 - accuracy: 0.9684 - val_loss: 0.0946 - val_accuracy: 0.9673\n",
      "Epoch 70/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0904 - accuracy: 0.9685 - val_loss: 0.0946 - val_accuracy: 0.9673\n",
      "Epoch 71/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0901 - accuracy: 0.9686 - val_loss: 0.0943 - val_accuracy: 0.9673\n",
      "Epoch 72/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0898 - accuracy: 0.9686 - val_loss: 0.0940 - val_accuracy: 0.9674\n",
      "Epoch 73/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0896 - accuracy: 0.9687 - val_loss: 0.0937 - val_accuracy: 0.9674\n",
      "Epoch 74/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0894 - accuracy: 0.9688 - val_loss: 0.0935 - val_accuracy: 0.9675\n",
      "Epoch 75/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0891 - accuracy: 0.9688 - val_loss: 0.0933 - val_accuracy: 0.9676\n",
      "Epoch 76/150\n",
      "45342/45342 [==============================] - 5s 104us/step - loss: 0.0889 - accuracy: 0.9689 - val_loss: 0.0931 - val_accuracy: 0.9677\n",
      "Epoch 77/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0886 - accuracy: 0.9690 - val_loss: 0.0930 - val_accuracy: 0.9676\n",
      "Epoch 78/150\n",
      "45342/45342 [==============================] - 5s 111us/step - loss: 0.0884 - accuracy: 0.9690 - val_loss: 0.0927 - val_accuracy: 0.9677\n",
      "Epoch 79/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0882 - accuracy: 0.9691 - val_loss: 0.0925 - val_accuracy: 0.9678\n",
      "Epoch 80/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0880 - accuracy: 0.9692 - val_loss: 0.0924 - val_accuracy: 0.9679\n",
      "Epoch 81/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0877 - accuracy: 0.9692 - val_loss: 0.0921 - val_accuracy: 0.9679\n",
      "Epoch 82/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0875 - accuracy: 0.9693 - val_loss: 0.0919 - val_accuracy: 0.9679\n",
      "Epoch 83/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0873 - accuracy: 0.9693 - val_loss: 0.0919 - val_accuracy: 0.9680\n",
      "Epoch 84/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0871 - accuracy: 0.9694 - val_loss: 0.0916 - val_accuracy: 0.9681\n",
      "Epoch 85/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0868 - accuracy: 0.9695 - val_loss: 0.0913 - val_accuracy: 0.9681\n",
      "Epoch 86/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0866 - accuracy: 0.9695 - val_loss: 0.0912 - val_accuracy: 0.9681\n",
      "Epoch 87/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0864 - accuracy: 0.9696 - val_loss: 0.0910 - val_accuracy: 0.9683\n",
      "Epoch 88/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0862 - accuracy: 0.9696 - val_loss: 0.0906 - val_accuracy: 0.9684\n",
      "Epoch 89/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0860 - accuracy: 0.9697 - val_loss: 0.0904 - val_accuracy: 0.9684\n",
      "Epoch 90/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0858 - accuracy: 0.9698 - val_loss: 0.0905 - val_accuracy: 0.9684\n",
      "Epoch 91/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0856 - accuracy: 0.9698 - val_loss: 0.0902 - val_accuracy: 0.9683\n",
      "Epoch 92/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0854 - accuracy: 0.9699 - val_loss: 0.0902 - val_accuracy: 0.9683\n",
      "Epoch 93/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0852 - accuracy: 0.9699 - val_loss: 0.0898 - val_accuracy: 0.9686\n",
      "Epoch 94/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0850 - accuracy: 0.9700 - val_loss: 0.0896 - val_accuracy: 0.9686\n",
      "Epoch 95/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0848 - accuracy: 0.9700 - val_loss: 0.0895 - val_accuracy: 0.9686\n",
      "Epoch 96/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0846 - accuracy: 0.9701 - val_loss: 0.0894 - val_accuracy: 0.9687\n",
      "Epoch 97/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0844 - accuracy: 0.9701 - val_loss: 0.0891 - val_accuracy: 0.9687\n",
      "Epoch 98/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0842 - accuracy: 0.9702 - val_loss: 0.0890 - val_accuracy: 0.9688\n",
      "Epoch 99/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0840 - accuracy: 0.9702 - val_loss: 0.0887 - val_accuracy: 0.9687\n",
      "Epoch 100/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0838 - accuracy: 0.9702 - val_loss: 0.0884 - val_accuracy: 0.9689\n",
      "Epoch 101/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0836 - accuracy: 0.9703 - val_loss: 0.0884 - val_accuracy: 0.9688\n",
      "Epoch 102/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0834 - accuracy: 0.9703 - val_loss: 0.0881 - val_accuracy: 0.9690\n",
      "Epoch 103/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0833 - accuracy: 0.9704 - val_loss: 0.0883 - val_accuracy: 0.9688\n",
      "Epoch 104/150\n",
      "45342/45342 [==============================] - 5s 110us/step - loss: 0.0831 - accuracy: 0.9704 - val_loss: 0.0880 - val_accuracy: 0.9689\n",
      "Epoch 105/150\n",
      "45342/45342 [==============================] - 5s 115us/step - loss: 0.0829 - accuracy: 0.9704 - val_loss: 0.0879 - val_accuracy: 0.9688\n",
      "Epoch 106/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0828 - accuracy: 0.9704 - val_loss: 0.0877 - val_accuracy: 0.9690\n",
      "Epoch 107/150\n",
      "45342/45342 [==============================] - 5s 105us/step - loss: 0.0826 - accuracy: 0.9705 - val_loss: 0.0875 - val_accuracy: 0.9691\n",
      "Epoch 108/150\n",
      "45342/45342 [==============================] - 5s 105us/step - loss: 0.0824 - accuracy: 0.9705 - val_loss: 0.0877 - val_accuracy: 0.9691\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45342/45342 [==============================] - 5s 106us/step - loss: 0.0823 - accuracy: 0.9705 - val_loss: 0.0874 - val_accuracy: 0.9690\n",
      "Epoch 110/150\n",
      "45342/45342 [==============================] - 5s 105us/step - loss: 0.0821 - accuracy: 0.9706 - val_loss: 0.0875 - val_accuracy: 0.9691\n",
      "Epoch 111/150\n",
      "45342/45342 [==============================] - 5s 107us/step - loss: 0.0820 - accuracy: 0.9706 - val_loss: 0.0870 - val_accuracy: 0.9691\n",
      "Epoch 112/150\n",
      "45342/45342 [==============================] - 5s 105us/step - loss: 0.0818 - accuracy: 0.9706 - val_loss: 0.0870 - val_accuracy: 0.9692\n",
      "Epoch 113/150\n",
      "45342/45342 [==============================] - 5s 109us/step - loss: 0.0817 - accuracy: 0.9707 - val_loss: 0.0868 - val_accuracy: 0.9693\n",
      "Epoch 114/150\n",
      "45342/45342 [==============================] - 5s 103us/step - loss: 0.0815 - accuracy: 0.9707 - val_loss: 0.0867 - val_accuracy: 0.9693\n",
      "Epoch 115/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0813 - accuracy: 0.9707 - val_loss: 0.0868 - val_accuracy: 0.9691\n",
      "Epoch 116/150\n",
      "45342/45342 [==============================] - 5s 114us/step - loss: 0.0812 - accuracy: 0.9708 - val_loss: 0.0863 - val_accuracy: 0.9692\n",
      "Epoch 117/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0811 - accuracy: 0.9708 - val_loss: 0.0864 - val_accuracy: 0.9692\n",
      "Epoch 118/150\n",
      "45342/45342 [==============================] - 5s 108us/step - loss: 0.0809 - accuracy: 0.9708 - val_loss: 0.0861 - val_accuracy: 0.9693\n",
      "Epoch 119/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0808 - accuracy: 0.9708 - val_loss: 0.0860 - val_accuracy: 0.9692\n",
      "Epoch 120/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0806 - accuracy: 0.9709 - val_loss: 0.0859 - val_accuracy: 0.9693\n",
      "Epoch 121/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0805 - accuracy: 0.9709 - val_loss: 0.0860 - val_accuracy: 0.9692\n",
      "Epoch 122/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0804 - accuracy: 0.9709 - val_loss: 0.0857 - val_accuracy: 0.9694\n",
      "Epoch 123/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.0802 - accuracy: 0.9710 - val_loss: 0.0857 - val_accuracy: 0.9695\n",
      "Epoch 124/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0801 - accuracy: 0.9710 - val_loss: 0.0855 - val_accuracy: 0.9693\n",
      "Epoch 125/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0800 - accuracy: 0.9710 - val_loss: 0.0852 - val_accuracy: 0.9695\n",
      "Epoch 126/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0799 - accuracy: 0.9710 - val_loss: 0.0852 - val_accuracy: 0.9695\n",
      "Epoch 127/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0797 - accuracy: 0.9711 - val_loss: 0.0852 - val_accuracy: 0.9694\n",
      "Epoch 128/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0796 - accuracy: 0.9710 - val_loss: 0.0851 - val_accuracy: 0.9694\n",
      "Epoch 129/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0794 - accuracy: 0.9711 - val_loss: 0.0853 - val_accuracy: 0.9693\n",
      "Epoch 130/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.0793 - accuracy: 0.9711 - val_loss: 0.0847 - val_accuracy: 0.9694\n",
      "Epoch 131/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0792 - accuracy: 0.9711 - val_loss: 0.0850 - val_accuracy: 0.9695\n",
      "Epoch 132/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0791 - accuracy: 0.9712 - val_loss: 0.0845 - val_accuracy: 0.9696\n",
      "Epoch 133/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0790 - accuracy: 0.9712 - val_loss: 0.0846 - val_accuracy: 0.9694\n",
      "Epoch 134/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0789 - accuracy: 0.9712 - val_loss: 0.0846 - val_accuracy: 0.9695\n",
      "Epoch 135/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0787 - accuracy: 0.9713 - val_loss: 0.0843 - val_accuracy: 0.9694\n",
      "Epoch 136/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.0786 - accuracy: 0.9713 - val_loss: 0.0843 - val_accuracy: 0.9696\n",
      "Epoch 137/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0785 - accuracy: 0.9713 - val_loss: 0.0843 - val_accuracy: 0.9693\n",
      "Epoch 138/150\n",
      "45342/45342 [==============================] - 5s 99us/step - loss: 0.0784 - accuracy: 0.9713 - val_loss: 0.0840 - val_accuracy: 0.9696\n",
      "Epoch 139/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0783 - accuracy: 0.9713 - val_loss: 0.0842 - val_accuracy: 0.9696\n",
      "Epoch 140/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0782 - accuracy: 0.9714 - val_loss: 0.0840 - val_accuracy: 0.9694\n",
      "Epoch 141/150\n",
      "45342/45342 [==============================] - 4s 98us/step - loss: 0.0780 - accuracy: 0.9714 - val_loss: 0.0839 - val_accuracy: 0.9696\n",
      "Epoch 142/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0779 - accuracy: 0.9714 - val_loss: 0.0839 - val_accuracy: 0.9695\n",
      "Epoch 143/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0778 - accuracy: 0.9714 - val_loss: 0.0840 - val_accuracy: 0.9696\n",
      "Epoch 144/150\n",
      "45342/45342 [==============================] - 5s 100us/step - loss: 0.0777 - accuracy: 0.9714 - val_loss: 0.0835 - val_accuracy: 0.9697\n",
      "Epoch 145/150\n",
      "45342/45342 [==============================] - 5s 101us/step - loss: 0.0776 - accuracy: 0.9715 - val_loss: 0.0836 - val_accuracy: 0.9697\n",
      "Epoch 146/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0775 - accuracy: 0.9715 - val_loss: 0.0836 - val_accuracy: 0.9698\n",
      "Epoch 147/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0774 - accuracy: 0.9716 - val_loss: 0.0833 - val_accuracy: 0.9698\n",
      "Epoch 148/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0773 - accuracy: 0.9716 - val_loss: 0.0833 - val_accuracy: 0.9697\n",
      "Epoch 149/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0772 - accuracy: 0.9715 - val_loss: 0.0835 - val_accuracy: 0.9696\n",
      "Epoch 150/150\n",
      "45342/45342 [==============================] - 4s 99us/step - loss: 0.0771 - accuracy: 0.9716 - val_loss: 0.0831 - val_accuracy: 0.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:136: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\User\\anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[200, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[186, 6]\n",
      "[6, 2]\n",
      "[192, 0]\n",
      "[0, 8]\n",
      "both b and c are zero\n",
      "[181, 5]\n",
      "[10, 4]\n",
      "[181, 5]\n",
      "[5, 9]\n",
      "[192, 2]\n",
      "[5, 1]\n",
      "[189, 5]\n",
      "[2, 4]\n",
      "[194, 2]\n",
      "[3, 1]\n",
      "[188, 8]\n",
      "[2, 2]\n",
      "[197, 1]\n",
      "[1, 1]\n",
      "[188, 10]\n",
      "[1, 1]\n",
      "[196, 1]\n",
      "[2, 1]\n",
      "[189, 8]\n",
      "[2, 1]\n",
      "[176, 16]\n",
      "[6, 2]\n",
      "[189, 3]\n",
      "[1, 7]\n",
      "[177, 8]\n",
      "[8, 7]\n",
      "[185, 0]\n",
      "[1, 14]\n",
      "[183, 7]\n",
      "[6, 4]\n",
      "[186, 4]\n",
      "[2, 8]\n",
      "[192, 3]\n",
      "[4, 1]\n",
      "[184, 11]\n",
      "[3, 2]\n",
      "[194, 3]\n",
      "[1, 2]\n",
      "[188, 9]\n",
      "[2, 1]\n",
      "[192, 3]\n",
      "[4, 1]\n",
      "[190, 5]\n",
      "[2, 3]\n",
      "[192, 4]\n",
      "[3, 1]\n",
      "[189, 7]\n",
      "[0, 4]\n",
      "[188, 0]\n",
      "[1, 11]\n",
      "[188, 0]\n",
      "[1, 11]\n",
      "[184, 1]\n",
      "[2, 13]\n",
      "[184, 1]\n",
      "[2, 13]\n",
      "[189, 5]\n",
      "[2, 4]\n",
      "[190, 4]\n",
      "[1, 5]\n",
      "[187, 4]\n",
      "[4, 5]\n",
      "[178, 13]\n",
      "[3, 6]\n",
      "[190, 3]\n",
      "[4, 3]\n",
      "[188, 5]\n",
      "[3, 4]\n",
      "[192, 1]\n",
      "[7, 0]\n",
      "[187, 6]\n",
      "[1, 6]\n",
      "[194, 3]\n",
      "[0, 3]\n",
      "[193, 4]\n",
      "[2, 1]\n",
      "[185, 6]\n",
      "[5, 4]\n",
      "[188, 3]\n",
      "[3, 6]\n",
      "[178, 8]\n",
      "[5, 9]\n",
      "[185, 1]\n",
      "[2, 12]\n",
      "[182, 8]\n",
      "[6, 4]\n",
      "[184, 6]\n",
      "[5, 5]\n",
      "[191, 3]\n",
      "[1, 5]\n",
      "[186, 8]\n",
      "[1, 5]\n",
      "[193, 2]\n",
      "[3, 2]\n",
      "[189, 6]\n",
      "[2, 3]\n",
      "[194, 0]\n",
      "[5, 1]\n",
      "[186, 8]\n",
      "[1, 5]\n",
      "[193, 2]\n",
      "[2, 3]\n",
      "[189, 6]\n",
      "[0, 5]\n",
      "[191, 0]\n",
      "[2, 7]\n",
      "[191, 0]\n",
      "[2, 7]\n",
      "[190, 1]\n",
      "[1, 8]\n",
      "[190, 1]\n",
      "[1, 8]\n",
      "[191, 5]\n",
      "[1, 3]\n",
      "[185, 11]\n",
      "[2, 2]\n",
      "[193, 2]\n",
      "[4, 1]\n",
      "[189, 6]\n",
      "[1, 4]\n",
      "[196, 1]\n",
      "[3, 0]\n",
      "[194, 3]\n",
      "[1, 2]\n",
      "[194, 3]\n",
      "[2, 1]\n",
      "[193, 4]\n",
      "[2, 1]\n",
      "[192, 1]\n",
      "[4, 3]\n",
      "[189, 4]\n",
      "[3, 4]\n",
      "[176, 12]\n",
      "[6, 6]\n",
      "[185, 3]\n",
      "[4, 8]\n",
      "[184, 8]\n",
      "[6, 2]\n",
      "[190, 2]\n",
      "[2, 6]\n",
      "[194, 2]\n",
      "[3, 1]\n",
      "[189, 7]\n",
      "[2, 2]\n",
      "[193, 5]\n",
      "[2, 0]\n",
      "[192, 6]\n",
      "[0, 2]\n",
      "[195, 2]\n",
      "[2, 1]\n",
      "[193, 4]\n",
      "[0, 3]\n",
      "[198, 1]\n",
      "[1, 0]\n",
      "[194, 5]\n",
      "[1, 0]\n",
      "[191, 4]\n",
      "[3, 2]\n",
      "[182, 13]\n",
      "[1, 4]\n",
      "[187, 0]\n",
      "[1, 12]\n",
      "[187, 0]\n",
      "[1, 12]\n",
      "[193, 0]\n",
      "[0, 7]\n",
      "both b and c are zero\n",
      "[193, 0]\n",
      "[1, 6]\n",
      "[195, 1]\n",
      "[3, 1]\n",
      "[183, 13]\n",
      "[2, 2]\n",
      "[196, 0]\n",
      "[3, 1]\n",
      "[187, 9]\n",
      "[1, 3]\n",
      "[196, 2]\n",
      "[2, 0]\n",
      "[193, 5]\n",
      "[0, 2]\n",
      "[196, 1]\n",
      "[2, 1]\n",
      "[192, 5]\n",
      "[1, 2]\n",
      "[197, 1]\n",
      "[0, 2]\n",
      "[188, 10]\n",
      "[0, 2]\n",
      "[197, 2]\n",
      "[0, 1]\n",
      "[195, 4]\n",
      "[0, 1]\n",
      "[177, 18]\n",
      "[1, 4]\n",
      "[194, 1]\n",
      "[0, 5]\n",
      "[187, 1]\n",
      "[0, 12]\n",
      "[187, 1]\n",
      "[0, 12]\n",
      "[175, 14]\n",
      "[7, 4]\n",
      "[189, 0]\n",
      "[0, 11]\n",
      "both b and c are zero\n",
      "[188, 1]\n",
      "[0, 11]\n",
      "[188, 1]\n",
      "[0, 11]\n",
      "[173, 12]\n",
      "[11, 4]\n",
      "[185, 0]\n",
      "[1, 14]\n",
      "[190, 2]\n",
      "[1, 7]\n",
      "[190, 2]\n",
      "[1, 7]\n",
      "[188, 3]\n",
      "[4, 5]\n",
      "[185, 6]\n",
      "[2, 7]\n",
      "[194, 3]\n",
      "[3, 0]\n",
      "[187, 10]\n",
      "[1, 2]\n",
      "[187, 7]\n",
      "[4, 2]\n",
      "[188, 6]\n",
      "[2, 4]\n",
      "[192, 3]\n",
      "[4, 1]\n",
      "[188, 7]\n",
      "[3, 2]\n",
      "[189, 2]\n",
      "[8, 1]\n",
      "[186, 5]\n",
      "[6, 3]\n",
      "[192, 3]\n",
      "[3, 2]\n",
      "[188, 7]\n",
      "[0, 5]\n",
      "[190, 6]\n",
      "[3, 1]\n",
      "[186, 10]\n",
      "[1, 3]\n",
      "[192, 5]\n",
      "[1, 2]\n",
      "[188, 9]\n",
      "[1, 2]\n",
      "[193, 1]\n",
      "[4, 2]\n",
      "[187, 7]\n",
      "[1, 5]\n",
      "[190, 4]\n",
      "[4, 2]\n",
      "[187, 7]\n",
      "[3, 3]\n",
      "[183, 8]\n",
      "[6, 3]\n",
      "[183, 8]\n",
      "[5, 4]\n",
      "[188, 6]\n",
      "[4, 2]\n",
      "[192, 2]\n",
      "[1, 5]\n",
      "[190, 3]\n",
      "[5, 2]\n",
      "[189, 4]\n",
      "[0, 7]\n",
      "[195, 3]\n",
      "[2, 0]\n",
      "[196, 2]\n",
      "[2, 0]\n",
      "[192, 3]\n",
      "[3, 2]\n",
      "[185, 10]\n",
      "[2, 3]\n",
      "[193, 2]\n",
      "[2, 3]\n",
      "[184, 11]\n",
      "[1, 4]\n",
      "[192, 3]\n",
      "[3, 2]\n",
      "[187, 8]\n",
      "[0, 5]\n",
      "[195, 1]\n",
      "[1, 3]\n",
      "[191, 5]\n",
      "[0, 4]\n",
      "[191, 4]\n",
      "[4, 1]\n",
      "[189, 6]\n",
      "[4, 1]\n",
      "[190, 4]\n",
      "[2, 4]\n",
      "[186, 8]\n",
      "[3, 3]\n",
      "[192, 5]\n",
      "[1, 2]\n",
      "[190, 7]\n",
      "[1, 2]\n",
      "[197, 0]\n",
      "[1, 2]\n",
      "[189, 8]\n",
      "[1, 2]\n",
      "[194, 4]\n",
      "[1, 1]\n",
      "[190, 8]\n",
      "[1, 1]\n",
      "[192, 0]\n",
      "[5, 3]\n",
      "[189, 3]\n",
      "[6, 2]\n",
      "[192, 2]\n",
      "[2, 4]\n",
      "[187, 7]\n",
      "[0, 6]\n",
      "[193, 3]\n",
      "[2, 2]\n",
      "[192, 4]\n",
      "[0, 4]\n",
      "[197, 0]\n",
      "[0, 3]\n",
      "both b and c are zero\n",
      "[191, 6]\n",
      "[0, 3]\n",
      "[191, 3]\n",
      "[2, 4]\n",
      "[184, 10]\n",
      "[1, 5]\n",
      "[195, 0]\n",
      "[3, 2]\n",
      "[188, 7]\n",
      "[2, 3]\n",
      "[187, 9]\n",
      "[3, 1]\n",
      "[187, 9]\n",
      "[0, 4]\n",
      "[195, 3]\n",
      "[1, 1]\n",
      "[192, 6]\n",
      "[1, 1]\n",
      "[195, 3]\n",
      "[1, 1]\n",
      "[194, 4]\n",
      "[0, 2]\n",
      "[192, 3]\n",
      "[4, 1]\n",
      "[189, 6]\n",
      "[3, 2]\n",
      "[195, 2]\n",
      "[2, 1]\n",
      "[189, 8]\n",
      "[2, 1]\n",
      "[195, 3]\n",
      "[0, 2]\n",
      "[194, 4]\n",
      "[1, 1]\n",
      "[196, 1]\n",
      "[0, 3]\n",
      "[191, 6]\n",
      "[2, 1]\n",
      "[184, 9]\n",
      "[4, 3]\n",
      "[186, 7]\n",
      "[4, 3]\n",
      "[186, 5]\n",
      "[5, 4]\n",
      "[190, 1]\n",
      "[2, 7]\n",
      "[193, 1]\n",
      "[2, 4]\n",
      "[193, 1]\n",
      "[2, 4]\n",
      "[183, 9]\n",
      "[5, 3]\n",
      "[189, 3]\n",
      "[2, 6]\n",
      "[190, 1]\n",
      "[0, 9]\n",
      "[189, 2]\n",
      "[0, 9]\n",
      "[181, 9]\n",
      "[6, 4]\n",
      "[190, 0]\n",
      "[2, 8]\n",
      "[190, 1]\n",
      "[5, 4]\n",
      "[190, 1]\n",
      "[5, 4]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9679966329966323 (+- 0.0016036623843918975)\n",
      "> F1: 0.6852312625002055(+- 0.012758620916091536)\n",
      "> Time: 39.84020633333333 (+- 0.1807748784395843)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9516137626072884 (+- 0.00039116042567577955)\n",
      "> F1: 0.015218855418916231(+- 0.002112282331427626)\n",
      "> Time: 0.8885291999999999 (+- 0.12251680182018569)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9699314139828535 (+- 0.00037805002388502937)\n",
      "> F1: 0.6332610439270566(+- 0.003626045734107938)\n",
      "> Time: 0.7458337666666667 (+- 0.01791711945827851)\n",
      "> AUC for class : 0.5647412994574567 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.5524971267878835 (+- 0.006925401879040701)\n",
      "X^2 for MWPM and NN: 0.08333333333333333\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X01: 0.9260282431877805 (+- 0.007986889410948054)\n",
      "X^2 for MWPM and NN: 2.4\n",
      "X^2 for PLUT and NN: 0.1\n",
      "> AUC for class X02: 0.9887095284919439 (+- 0.002555027542038277)\n",
      "X^2 for MWPM and NN: 2.2857142857142856\n",
      "X^2 for PLUT and NN: 0.5714285714285714\n",
      "> AUC for class X03: 0.9893533741465581 (+- 0.0019175743998960122)\n",
      "X^2 for MWPM and NN: 0.8\n",
      "X^2 for PLUT and NN: 2.5\n",
      "> AUC for class X04: 0.9897254205136145 (+- 0.0012507241340893936)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 5.818181818181818\n",
      "> AUC for class X05: 0.986494242859849 (+- 0.0015719488937013856)\n",
      "X^2 for MWPM and NN: 1.3333333333333333\n",
      "X^2 for PLUT and NN: 2.5\n",
      "> AUC for class X06: 0.901568891774145 (+- 0.017033182787536228)\n",
      "X^2 for MWPM and NN: 3.6818181818181817\n",
      "X^2 for PLUT and NN: 0.25\n",
      "> AUC for class X10: 0.908688780507712 (+- 0.0023561876565275108)\n",
      "X^2 for MWPM and NN: 0.0625\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X11: 0.9676240616423111 (+- 0.0023665775894891024)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.16666666666666666\n",
      "> AUC for class X12: 0.9746981030547697 (+- 0.0007853895483425409)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 3.5\n",
      "> AUC for class X13: 0.9785024799104186 (+- 0.001085433027769724)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 3.272727272727273\n",
      "> AUC for class X14: 0.9771345832646458 (+- 0.0031955963315426672)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 0.5714285714285714\n",
      "> AUC for class X15: 0.969432538000087 (+- 0.004221430930133705)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 5.142857142857143\n",
      "> AUC for class X16: 0.9034974530678546 (+- 0.011652429581170087)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X20: 0.9087830960908182 (+- 0.0037084154944393762)\n",
      "X^2 for MWPM and NN: 1.3333333333333333\n",
      "X^2 for PLUT and NN: 1.3333333333333333\n",
      "> AUC for class X21: 0.9697288288786247 (+- 0.0009631029431624867)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 0.8\n",
      "> AUC for class X22: 0.9757418786611053 (+- 0.0023358524247670404)\n",
      "X^2 for MWPM and NN: 0.125\n",
      "X^2 for PLUT and NN: 5.0625\n",
      "> AUC for class X23: 0.9745199426862685 (+- 0.0004623292425177233)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 0.125\n",
      "> AUC for class X24: 0.9759521414042278 (+- 0.0019775645636206144)\n",
      "X^2 for MWPM and NN: 6.125\n",
      "X^2 for PLUT and NN: 2.2857142857142856\n",
      "> AUC for class X25: 0.9657873675991485 (+- 0.0008258240316784903)\n",
      "X^2 for MWPM and NN: 1.3333333333333333\n",
      "X^2 for PLUT and NN: 0.16666666666666666\n",
      "> AUC for class X26: 0.9041849243932919 (+- 0.006447627832500706)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.16666666666666666\n",
      "> AUC for class X30: 0.9049361806809438 (+- 0.009805759781556279)\n",
      "X^2 for MWPM and NN: 0.3076923076923077\n",
      "X^2 for PLUT and NN: 1.3333333333333333\n",
      "> AUC for class X31: 0.9685563374782126 (+- 0.0011646144817795967)\n",
      "X^2 for MWPM and NN: 0.07142857142857142\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X32: 0.9758330185384615 (+- 0.0012925986211553074)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X33: 0.9751770667588994 (+- 0.0011207005793053627)\n",
      "X^2 for MWPM and NN: 0.8\n",
      "X^2 for PLUT and NN: 1.125\n",
      "> AUC for class X34: 0.9772715823925692 (+- 0.0016631467814811614)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X35: 0.9701131262424246 (+- 0.0014714948025513785)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 4.166666666666667\n",
      "> AUC for class X36: 0.8991977939314758 (+- 0.005917366482932014)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class X40: 0.9020455738798864 (+- 0.0089064636067539)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class X41: 0.9689502628115797 (+- 0.003216380305600828)\n",
      "X^2 for MWPM and NN: 1.5\n",
      "X^2 for PLUT and NN: 4.923076923076923\n",
      "> AUC for class X42: 0.9769412562580227 (+- 0.0019899502909160095)\n",
      "X^2 for MWPM and NN: 1.5\n",
      "X^2 for PLUT and NN: 2.2857142857142856\n",
      "> AUC for class X43: 0.9772344493041266 (+- 0.001934245440777846)\n",
      "X^2 for MWPM and NN: 2.25\n",
      "X^2 for PLUT and NN: 0.25\n",
      "> AUC for class X44: 0.9764987116653536 (+- 0.0009173823712133778)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.16666666666666666\n",
      "> AUC for class X45: 0.9686072052252724 (+- 0.0029817972519468033)\n",
      "X^2 for MWPM and NN: 3.2\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X46: 0.8969653836628234 (+- 0.0006109871286092643)\n",
      "X^2 for MWPM and NN: 1.3888888888888888\n",
      "X^2 for PLUT and NN: 0.5714285714285714\n",
      "> AUC for class X50: 0.9106124303480035 (+- 0.006606679375650321)\n",
      "X^2 for MWPM and NN: 0.07142857142857142\n",
      "X^2 for PLUT and NN: 0.25\n",
      "> AUC for class X51: 0.9713927039259653 (+- 0.003550796991665076)\n",
      "X^2 for MWPM and NN: 0.8\n",
      "X^2 for PLUT and NN: 1.7777777777777777\n",
      "> AUC for class X52: 0.979319767605583 (+- 0.0018806615604787529)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 4.166666666666667\n",
      "> AUC for class X53: 0.9792069839735302 (+- 0.0033655955073838974)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 2.25\n",
      "> AUC for class X54: 0.978094304228228 (+- 0.002771579450177339)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 1.5\n",
      "> AUC for class X55: 0.9740994884181582 (+- 0.0013623857712755608)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 8.642857142857142\n",
      "> AUC for class X56: 0.896746570839806 (+- 0.0058385425542435325)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X60: 0.9094610898792324 (+- 0.009003439690557783)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X61: 0.9882715007634167 (+- 0.002506174652615005)\n",
      "X^2 for MWPM and NN: 2.25\n",
      "X^2 for PLUT and NN: 6.666666666666667\n",
      "> AUC for class X62: 0.9890015607601187 (+- 0.0009024287285553527)\n",
      "X^2 for MWPM and NN: 5.333333333333333\n",
      "X^2 for PLUT and NN: 4.9\n",
      "> AUC for class X63: 0.9912879250256995 (+- 0.0006680089687703802)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 3.2\n",
      "> AUC for class X64: 0.9886823953097611 (+- 0.0028610430608523164)\n",
      "X^2 for MWPM and NN: 1.3333333333333333\n",
      "X^2 for PLUT and NN: 1.5\n",
      "> AUC for class X65: 0.9878012729230309 (+- 0.0024862975504749128)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 8.1\n",
      "> AUC for class X66: 0.9617857390877371 (+- 0.006740126903159147)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 2.25\n",
      "> AUC for class Z00: 0.9028427831002231 (+- 0.004010114060794126)\n",
      "X^2 for MWPM and NN: 13.473684210526315\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z01: 0.9064319637812263 (+- 0.004803437528394833)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z02: 0.8990991199147739 (+- 0.01107824235342766)\n",
      "X^2 for MWPM and NN: 1.7142857142857142\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z03: 0.8932860417082642 (+- 0.009497898584176287)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z04: 0.9025530818827789 (+- 0.0035692226974445864)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z05: 0.8999121493016888 (+- 0.0046771002465955825)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z06: 0.9647114741210835 (+- 0.011955794084281938)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 1.125\n",
      "> AUC for class Z10: 0.9861190623423964 (+- 0.0011201165371920365)\n",
      "X^2 for MWPM and NN: 0.16666666666666666\n",
      "X^2 for PLUT and NN: 5.818181818181818\n",
      "> AUC for class Z11: 0.9695911284269405 (+- 0.0013695578302415723)\n",
      "X^2 for MWPM and NN: 0.36363636363636365\n",
      "X^2 for PLUT and NN: 1.125\n",
      "> AUC for class Z12: 0.969886948301368 (+- 0.0030402293485152286)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 0.9\n",
      "> AUC for class Z13: 0.9701077785156317 (+- 0.0016789436254602635)\n",
      "X^2 for MWPM and NN: 4.9\n",
      "X^2 for PLUT and NN: 0.36363636363636365\n",
      "> AUC for class Z14: 0.9673238951997217 (+- 0.0012048426189518442)\n",
      "X^2 for MWPM and NN: 0.16666666666666666\n",
      "X^2 for PLUT and NN: 5.142857142857143\n",
      "> AUC for class Z15: 0.9702194690286681 (+- 0.0032893169341684484)\n",
      "X^2 for MWPM and NN: 0.4444444444444444\n",
      "X^2 for PLUT and NN: 5.818181818181818\n",
      "> AUC for class Z16: 0.9858076386230717 (+- 0.0010226721137362278)\n",
      "X^2 for MWPM and NN: 1.5\n",
      "X^2 for PLUT and NN: 4.9\n",
      "> AUC for class Z20: 0.989389685605151 (+- 0.0009218008552838866)\n",
      "X^2 for MWPM and NN: 3.2\n",
      "X^2 for PLUT and NN: 3.125\n",
      "> AUC for class Z21: 0.9796766743442502 (+- 0.0021828503746729694)\n",
      "X^2 for MWPM and NN: 0.125\n",
      "X^2 for PLUT and NN: 0.9\n",
      "> AUC for class Z22: 0.9759090478340283 (+- 0.0019634034674761323)\n",
      "X^2 for MWPM and NN: 0.07142857142857142\n",
      "X^2 for PLUT and NN: 0.3076923076923077\n",
      "> AUC for class Z23: 0.9737145726890216 (+- 0.0022440066071154913)\n",
      "X^2 for MWPM and NN: 0.1\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z24: 0.9728025127784939 (+- 0.003242289603388679)\n",
      "X^2 for MWPM and NN: 1.125\n",
      "X^2 for PLUT and NN: 2.25\n",
      "> AUC for class Z25: 0.9787359768683727 (+- 0.004559990899709115)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.25\n",
      "> AUC for class Z26: 0.9893517526863032 (+- 0.0027125195748287432)\n",
      "X^2 for MWPM and NN: 0.16666666666666666\n",
      "X^2 for PLUT and NN: 4.083333333333333\n",
      "> AUC for class Z30: 0.989555927249239 (+- 0.0024556020568832775)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 6.75\n",
      "> AUC for class Z31: 0.9788481510039602 (+- 0.0030590411308337364)\n",
      "X^2 for MWPM and NN: 0.16666666666666666\n",
      "X^2 for PLUT and NN: 6.125\n",
      "> AUC for class Z32: 0.9756349118107138 (+- 0.0019538344705250102)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 3.2\n",
      "> AUC for class Z33: 0.9734780999710425 (+- 0.002123743116519361)\n",
      "X^2 for MWPM and NN: 0.125\n",
      "X^2 for PLUT and NN: 0.1\n",
      "> AUC for class Z34: 0.9764536582786588 (+- 0.006266061965595093)\n",
      "X^2 for MWPM and NN: 0.16666666666666666\n",
      "X^2 for PLUT and NN: 1.4545454545454546\n",
      "> AUC for class Z35: 0.9800018920978452 (+- 0.003840577604082322)\n",
      "X^2 for MWPM and NN: 1.5\n",
      "X^2 for PLUT and NN: 3.125\n",
      "> AUC for class Z36: 0.9899185224004667 (+- 0.0007560175737414089)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z40: 0.9891974334723154 (+- 0.0023963752566787894)\n",
      "X^2 for MWPM and NN: 0.8\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z41: 0.9775392800638417 (+- 0.0030094680815290283)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 1.7777777777777777\n",
      "> AUC for class Z42: 0.9757763492908257 (+- 0.0012617720311347046)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 5.142857142857143\n",
      "> AUC for class Z43: 0.9734969302242331 (+- 0.002888590444972813)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 2.25\n",
      "> AUC for class Z44: 0.9745252645587303 (+- 0.003567689647098368)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 4.166666666666667\n",
      "> AUC for class Z45: 0.9764428528536714 (+- 0.004180583779652963)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 5.818181818181818\n",
      "> AUC for class Z46: 0.9907126415672206 (+- 0.002309921422533825)\n",
      "X^2 for MWPM and NN: 5.333333333333333\n",
      "X^2 for PLUT and NN: 1.7777777777777777\n",
      "> AUC for class Z50: 0.9876093451502909 (+- 0.001386878302820845)\n",
      "X^2 for MWPM and NN: 2.0833333333333335\n",
      "X^2 for PLUT and NN: 7.111111111111111\n",
      "> AUC for class Z51: 0.9730309162448352 (+- 0.0038726248866485374)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 2.2857142857142856\n",
      "> AUC for class Z52: 0.9684404901072187 (+- 0.0036310759330099485)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 2.25\n",
      "> AUC for class Z53: 0.9704060802731137 (+- 0.002710509079397813)\n",
      "X^2 for MWPM and NN: 0.5714285714285714\n",
      "X^2 for PLUT and NN: 0.4444444444444444\n",
      "> AUC for class Z54: 0.9666288054961978 (+- 0.002943280058431517)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 2.5\n",
      "> AUC for class Z55: 0.9674899327455618 (+- 0.002239411246333635)\n",
      "X^2 for MWPM and NN: 1.3333333333333333\n",
      "X^2 for PLUT and NN: 0.8\n",
      "> AUC for class Z56: 0.9872469693472207 (+- 0.001160240927736263)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 1.125\n",
      "> AUC for class Z60: 0.9605579626027102 (+- 0.0066774586059966615)\n",
      "X^2 for MWPM and NN: 1.2307692307692308\n",
      "X^2 for PLUT and NN: 0.36363636363636365\n",
      "> AUC for class Z61: 0.9046717522464548 (+- 0.006163106337214191)\n",
      "X^2 for MWPM and NN: 0.1\n",
      "X^2 for PLUT and NN: 1.3333333333333333\n",
      "> AUC for class Z62: 0.9067232567356012 (+- 0.005677830129841125)\n",
      "X^2 for MWPM and NN: 1.3333333333333333\n",
      "X^2 for PLUT and NN: 1.3333333333333333\n",
      "> AUC for class Z63: 0.8973820284654167 (+- 0.006850065560510757)\n",
      "X^2 for MWPM and NN: 0.6428571428571429\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z64: 0.8943096600004807 (+- 0.013247417520513755)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class Z65: 0.9059942017204529 (+- 0.011506403710564933)\n",
      "X^2 for MWPM and NN: 0.26666666666666666\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class Z66: 0.908336907996398 (+- 0.003898360340095556)\n",
      "X^2 for MWPM and NN: 4.166666666666667\n",
      "X^2 for PLUT and NN: 4.166666666666667\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.6281790165364407, 0.6363954038109725, 0.6352087114337568]\n",
      "TOTAL F1 PLUT: [0.016653005464480872, 0.01677114489012715, 0.012232415902140671]\n",
      "TOTAL F1 MWPM: [0.7032745591939547, 0.6762661370407149, 0.6761530912659469]\n",
      "TOTAL ACC NN: [0.9698039889335632, 0.9704447984695435, 0.9695454545454538]\n",
      "TOTAL ACC PLUT: [0.9518951465094444, 0.9518855352518155, 0.9510606060606049]\n",
      "TOTAL ACC MWPM: [0.9702525252525246, 0.9670707070707064, 0.966666666666666]\n",
      "TOTAL TIME NN: [0.7711724, 0.7331641, 0.7331648]\n",
      "TOTAL TIME PLUT: [0.7901663, 1.0612384, 0.8141829]\n",
      "TOTAL TIME MWPM: [39.63113729999998, 40.07216510000001, 39.81731659999999]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "inputs = x_d7.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d7)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d7.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    t_test = targets[test].copy()\n",
    "    x_test_d7 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    indices = np.random.choice(inputs[test].shape[0], 200, replace=False)#20000, replace=False)\n",
    "    x_test_d7 = x_test_d7[indices]\n",
    "    inputs_test_2 = inputs_test[indices]\n",
    "    targets_test_2 = targets[test][indices]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    \n",
    "    decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "    decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "    decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d7 = np.array(decoding_d7[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d7 = lookup_decoder(7)\n",
    "    \n",
    "    lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "        f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "    else:\n",
    "        pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "        f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1)\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d7 = compile_FFNN_cv_model_DepthSeven(7)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model_d7.fit(\n",
    "        x=inputs_train ,\n",
    "        y=targets[train],\n",
    "        validation_split=.25,\n",
    "        epochs= 150)\n",
    "    \n",
    "   # Generate generalization metrics\n",
    "    scores = model_d7.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d7 = model_d7.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d7.copy() #change here\n",
    "    pred[pred>=.5]=1 \n",
    "    pred[pred<.5]=0\n",
    "     \n",
    "    if fold_no < 3:\n",
    "        acc = scores[1]\n",
    "        f1 = f1_score(targets[test], pred, average='micro', zero_division=0)\n",
    "    else:\n",
    "        pred = model_d7.predict(inputs_test_2)\n",
    "        pred[pred>=.5]=1 \n",
    "        pred[pred<.5]=0\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "        f1 = f1_score(targets_test_2, pred, average='micro', zero_division=0)\n",
    " \n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "        aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
